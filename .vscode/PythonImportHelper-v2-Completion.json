[
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "linalg",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "linspace",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "time,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time.",
        "description": "time.",
        "detail": "time.",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "svgfig",
        "description": "svgfig",
        "isExtraImport": true,
        "detail": "svgfig",
        "documentation": {}
    },
    {
        "label": "re,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re.",
        "description": "re.",
        "detail": "re.",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pformat",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pformat",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "PrettyPrinter",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "html_functions",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html_functions",
        "description": "html_functions",
        "detail": "html_functions",
        "documentation": {}
    },
    {
        "label": "doxygen_scan",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "doxygen_scan",
        "description": "doxygen_scan",
        "detail": "doxygen_scan",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "unittest",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "unittest",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "unittest",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "isPointInRect",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "isPointInRect",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "unittest",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "importPath": "tests_common",
        "description": "tests_common",
        "isExtraImport": true,
        "detail": "tests_common",
        "documentation": {}
    },
    {
        "label": "sys,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys.",
        "description": "sys.",
        "detail": "sys.",
        "documentation": {}
    },
    {
        "label": "common",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "common",
        "description": "common",
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "remove_comments",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getTokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "postProcessParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "remove_comments",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getTokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "postProcessParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "remove_comments",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getTokens",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "postProcessParameters",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "findFile",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "findFile",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "splitfn",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "nothing",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "splitfn",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "clock",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "mosaic",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "mosaic",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "make_cmap",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "clock",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getsize",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_keypoints",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "anorm",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getsize",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "Sketcher",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "nothing",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "getsize",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "anorm2",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "RectSelector",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "clock",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "StatValue",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "Sketcher",
        "importPath": "common",
        "description": "common",
        "isExtraImport": true,
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "fnmatch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fnmatch",
        "description": "fnmatch",
        "detail": "fnmatch",
        "documentation": {}
    },
    {
        "label": "fnmatch",
        "importPath": "fnmatch",
        "description": "fnmatch",
        "isExtraImport": true,
        "detail": "fnmatch",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "atan2",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "cos",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sin",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "pi",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "COCO",
        "importPath": "pycocotools.coco",
        "description": "pycocotools.coco",
        "isExtraImport": true,
        "detail": "pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "COCO",
        "importPath": "pycocotools.coco",
        "description": "pycocotools.coco",
        "isExtraImport": true,
        "detail": "pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "importPath": "pycocotools.cocoeval",
        "description": "pycocotools.cocoeval",
        "isExtraImport": true,
        "detail": "pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "importPath": "pycocotools.cocoeval",
        "description": "pycocotools.cocoeval",
        "isExtraImport": true,
        "detail": "pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "optimize_for_inference_lib",
        "importPath": "tensorflow.python.tools",
        "description": "tensorflow.python.tools",
        "isExtraImport": true,
        "detail": "tensorflow.python.tools",
        "documentation": {}
    },
    {
        "label": "TransformGraph",
        "importPath": "tensorflow.tools.graph_transforms",
        "description": "tensorflow.tools.graph_transforms",
        "isExtraImport": true,
        "detail": "tensorflow.tools.graph_transforms",
        "documentation": {}
    },
    {
        "label": "NodeDef",
        "importPath": "tensorflow.core.framework.node_def_pb2",
        "description": "tensorflow.core.framework.node_def_pb2",
        "isExtraImport": true,
        "detail": "tensorflow.core.framework.node_def_pb2",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "load_lua",
        "importPath": "torch.utils.serialization",
        "description": "torch.utils.serialization",
        "isExtraImport": true,
        "detail": "torch.utils.serialization",
        "documentation": {}
    },
    {
        "label": "eval_segm_result",
        "importPath": "pascal_semsegm_test_fcn",
        "description": "pascal_semsegm_test_fcn",
        "isExtraImport": true,
        "detail": "pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "get_conf_mat",
        "importPath": "pascal_semsegm_test_fcn",
        "description": "pascal_semsegm_test_fcn",
        "isExtraImport": true,
        "detail": "pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "get_metrics",
        "importPath": "pascal_semsegm_test_fcn",
        "description": "pascal_semsegm_test_fcn",
        "isExtraImport": true,
        "detail": "pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "DatasetImageFetch",
        "importPath": "pascal_semsegm_test_fcn",
        "description": "pascal_semsegm_test_fcn",
        "isExtraImport": true,
        "detail": "pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "SemSegmEvaluation",
        "importPath": "pascal_semsegm_test_fcn",
        "description": "pascal_semsegm_test_fcn",
        "isExtraImport": true,
        "detail": "pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "Framework",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "DnnCaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "MeanChannelsFetch",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "CaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "DnnCaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "ClsAccEvaluation",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "MeanValueFetch",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "DnnCaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "Framework",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "ClsAccEvaluation",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "CaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "DnnCaffeModel",
        "importPath": "imagenet_cls_test_alexnet",
        "description": "imagenet_cls_test_alexnet",
        "isExtraImport": true,
        "detail": "imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "gfile",
        "importPath": "tensorflow.python.platform",
        "description": "tensorflow.python.platform",
        "isExtraImport": true,
        "detail": "tensorflow.python.platform",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "importPath": "tst_scene_render",
        "description": "tst_scene_render",
        "isExtraImport": true,
        "detail": "tst_scene_render",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "importPath": "tst_scene_render",
        "description": "tst_scene_render",
        "isExtraImport": true,
        "detail": "tst_scene_render",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "importPath": "tst_scene_render",
        "description": "tst_scene_render",
        "isExtraImport": true,
        "detail": "tst_scene_render",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "importPath": "tst_scene_render",
        "description": "tst_scene_render",
        "isExtraImport": true,
        "detail": "tst_scene_render",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "importPath": "tst_scene_render",
        "description": "tst_scene_render",
        "isExtraImport": true,
        "detail": "tst_scene_render",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "PIPE",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "STDOUT",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "STDOUT",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "CalledProcessError",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "codecs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "codecs",
        "description": "codecs",
        "detail": "codecs",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "Template",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "Template",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "Template",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "Template",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "templates",
        "description": "templates",
        "isExtraImport": true,
        "detail": "templates",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "setuptools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setuptools",
        "description": "setuptools",
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "hdr_parser,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hdr_parser.",
        "description": "hdr_parser.",
        "detail": "hdr_parser.",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "randint",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "ifilter",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "count",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "testlog_parser,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "testlog_parser.",
        "description": "testlog_parser.",
        "detail": "testlog_parser.",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "table_formatter",
        "description": "table_formatter",
        "isExtraImport": true,
        "detail": "table_formatter",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "table_formatter",
        "description": "table_formatter",
        "isExtraImport": true,
        "detail": "table_formatter",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "table_formatter",
        "description": "table_formatter",
        "isExtraImport": true,
        "detail": "table_formatter",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "table_formatter",
        "description": "table_formatter",
        "isExtraImport": true,
        "detail": "table_formatter",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "OptionParser",
        "importPath": "optparse",
        "description": "optparse",
        "isExtraImport": true,
        "detail": "optparse",
        "documentation": {}
    },
    {
        "label": "math,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math.",
        "description": "math.",
        "detail": "math.",
        "documentation": {}
    },
    {
        "label": "glob,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob.",
        "description": "glob.",
        "detail": "glob.",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "attrgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "getSetName",
        "importPath": "summary",
        "description": "summary",
        "isExtraImport": true,
        "detail": "summary",
        "documentation": {}
    },
    {
        "label": "alphanum_keyselector",
        "importPath": "summary",
        "description": "summary",
        "isExtraImport": true,
        "detail": "summary",
        "documentation": {}
    },
    {
        "label": "Err",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "CMakeCache",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "log",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "execute",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "Err",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "log",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "execute",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "isColorEnabled",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "hostos",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "Err",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "log",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "execute",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "getPlatformVersion",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "isColorEnabled",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "TempEnvDir",
        "importPath": "run_utils",
        "description": "run_utils",
        "isExtraImport": true,
        "detail": "run_utils",
        "documentation": {}
    },
    {
        "label": "TestSuite",
        "importPath": "run_suite",
        "description": "run_suite",
        "isExtraImport": true,
        "detail": "run_suite",
        "documentation": {}
    },
    {
        "label": "TestSuite",
        "importPath": "run_suite",
        "description": "run_suite",
        "isExtraImport": true,
        "detail": "run_suite",
        "documentation": {}
    },
    {
        "label": "AndroidTestSuite",
        "importPath": "run_android",
        "description": "run_android",
        "isExtraImport": true,
        "detail": "run_android",
        "documentation": {}
    },
    {
        "label": "getpass",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "getpass",
        "description": "getpass",
        "detail": "getpass",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "LONG_TESTS_DEBUG_VALGRIND",
        "importPath": "run_long",
        "description": "run_long",
        "isExtraImport": true,
        "detail": "run_long",
        "documentation": {}
    },
    {
        "label": "longTestFilter",
        "importPath": "run_long",
        "description": "run_long",
        "isExtraImport": true,
        "detail": "run_long",
        "documentation": {}
    },
    {
        "label": "getColorizer",
        "importPath": "color",
        "description": "color",
        "isExtraImport": true,
        "detail": "color",
        "documentation": {}
    },
    {
        "label": "dummyColorizer",
        "importPath": "color",
        "description": "color",
        "isExtraImport": true,
        "detail": "color",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "xml.dom.minidom",
        "description": "xml.dom.minidom",
        "isExtraImport": true,
        "detail": "xml.dom.minidom",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "errno",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "errno",
        "description": "errno",
        "detail": "errno",
        "documentation": {}
    },
    {
        "label": "numbers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numbers",
        "description": "numbers",
        "detail": "numbers",
        "documentation": {}
    },
    {
        "label": "xlwt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xlwt",
        "description": "xlwt",
        "detail": "xlwt",
        "documentation": {}
    },
    {
        "label": "parseLogFile",
        "importPath": "testlog_parser",
        "description": "testlog_parser",
        "isExtraImport": true,
        "detail": "testlog_parser",
        "documentation": {}
    },
    {
        "label": "execute",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_error",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_header",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_version",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_cmake_version",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "execute",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_error",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_major",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_setting",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_version",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_cmake_version",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_error",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_cmake_version",
        "importPath": "cv_build_utils",
        "description": "cv_build_utils",
        "isExtraImport": true,
        "detail": "cv_build_utils",
        "documentation": {}
    },
    {
        "label": "DocBuilder",
        "importPath": "build_docs",
        "description": "build_docs",
        "isExtraImport": true,
        "detail": "build_docs",
        "documentation": {}
    },
    {
        "label": "Builder",
        "importPath": "build_framework",
        "description": "build_framework",
        "isExtraImport": true,
        "detail": "build_framework",
        "documentation": {}
    },
    {
        "label": "TestRunner",
        "importPath": "run_tests",
        "description": "run_tests",
        "isExtraImport": true,
        "detail": "run_tests",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "paddlehub.vision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddlehub.vision.transforms",
        "description": "paddlehub.vision.transforms",
        "detail": "paddlehub.vision.transforms",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "paddlehub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddlehub",
        "description": "paddlehub",
        "detail": "paddlehub",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "torch.onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.onnx",
        "description": "torch.onnx",
        "detail": "torch.onnx",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "VGG16",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "vgg16",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "VGG19",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "vgg19",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "ResNet50",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "resnet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "ResNet101",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "ResNet152",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "DenseNet121",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "densenet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "DenseNet169",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "DenseNet201",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "InceptionResNetV2",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "inception_resnet_v2",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "InceptionV3",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "inception_v3",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "MobileNet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "mobilenet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "MobileNetV2",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "mobilenet_v2",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "NASNetLarge",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "nasnet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "NASNetMobile",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "Xception",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "xception",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "MobileNet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "convert_variables_to_constants_v2",
        "importPath": "tensorflow.python.framework.convert_to_constants",
        "description": "tensorflow.python.framework.convert_to_constants",
        "isExtraImport": true,
        "detail": "tensorflow.python.framework.convert_to_constants",
        "documentation": {}
    },
    {
        "label": "convert_variables_to_constants_v2",
        "importPath": "tensorflow.python.framework.convert_to_constants",
        "description": "tensorflow.python.framework.convert_to_constants",
        "isExtraImport": true,
        "detail": "tensorflow.python.framework.convert_to_constants",
        "documentation": {}
    },
    {
        "label": "tarfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tarfile",
        "description": "tarfile",
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "skimage.io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "skimage.io",
        "description": "skimage.io",
        "detail": "skimage.io",
        "documentation": {}
    },
    {
        "label": "pylab",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pylab",
        "description": "pylab",
        "detail": "pylab",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "readTextMessage",
        "importPath": "tf_text_graph_common",
        "description": "tf_text_graph_common",
        "isExtraImport": true,
        "detail": "tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "tf_text_graph_common",
        "description": "tf_text_graph_common",
        "isExtraImport": true,
        "detail": "tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "tf_text_graph_common",
        "description": "tf_text_graph_common",
        "isExtraImport": true,
        "detail": "tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "tf_text_graph_common",
        "description": "tf_text_graph_common",
        "isExtraImport": true,
        "detail": "tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "tf_text_graph_common",
        "description": "tf_text_graph_common",
        "isExtraImport": true,
        "detail": "tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "createSSDGraph",
        "importPath": "tf_text_graph_ssd",
        "description": "tf_text_graph_ssd",
        "isExtraImport": true,
        "detail": "tf_text_graph_ssd",
        "documentation": {}
    },
    {
        "label": "createFasterRCNNGraph",
        "importPath": "tf_text_graph_faster_rcnn",
        "description": "tf_text_graph_faster_rcnn",
        "isExtraImport": true,
        "detail": "tf_text_graph_faster_rcnn",
        "documentation": {}
    },
    {
        "label": "parse_human",
        "importPath": "human_parsing",
        "description": "human_parsing",
        "isExtraImport": true,
        "detail": "human_parsing",
        "documentation": {}
    },
    {
        "label": "gdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gdb",
        "description": "gdb",
        "detail": "gdb",
        "documentation": {}
    },
    {
        "label": "input",
        "importPath": "builtins",
        "description": "builtins",
        "isExtraImport": true,
        "detail": "builtins",
        "documentation": {}
    },
    {
        "label": "init_feature",
        "importPath": "find_obj",
        "description": "find_obj",
        "isExtraImport": true,
        "detail": "find_obj",
        "documentation": {}
    },
    {
        "label": "filter_matches",
        "importPath": "find_obj",
        "description": "find_obj",
        "isExtraImport": true,
        "detail": "find_obj",
        "documentation": {}
    },
    {
        "label": "explore_match",
        "importPath": "find_obj",
        "description": "find_obj",
        "isExtraImport": true,
        "detail": "find_obj",
        "documentation": {}
    },
    {
        "label": "video",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "video",
        "description": "video",
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "create_capture",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "create_capture",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "presets",
        "importPath": "video",
        "description": "video",
        "isExtraImport": true,
        "detail": "video",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "webbrowser",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "webbrowser",
        "description": "webbrowser",
        "detail": "webbrowser",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "digits",
        "description": "digits",
        "isExtraImport": true,
        "detail": "digits",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "digits",
        "description": "digits",
        "isExtraImport": true,
        "detail": "digits",
        "documentation": {}
    },
    {
        "label": "axes3d",
        "importPath": "mpl_toolkits.mplot3d",
        "description": "mpl_toolkits.mplot3d",
        "isExtraImport": true,
        "detail": "mpl_toolkits.mplot3d",
        "documentation": {}
    },
    {
        "label": "Axes3D",
        "importPath": "mpl_toolkits.mplot3d",
        "description": "mpl_toolkits.mplot3d",
        "isExtraImport": true,
        "detail": "mpl_toolkits.mplot3d",
        "documentation": {}
    },
    {
        "label": "PlaneTracker",
        "importPath": "plane_tracker",
        "description": "plane_tracker",
        "isExtraImport": true,
        "detail": "plane_tracker",
        "documentation": {}
    },
    {
        "label": "PlaneTracker",
        "importPath": "plane_tracker",
        "description": "plane_tracker",
        "isExtraImport": true,
        "detail": "plane_tracker",
        "documentation": {}
    },
    {
        "label": "make_gaussians",
        "importPath": "gaussian_mix",
        "description": "gaussian_mix",
        "isExtraImport": true,
        "detail": "gaussian_mix",
        "documentation": {}
    },
    {
        "label": "getopt,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "getopt.",
        "description": "getopt.",
        "detail": "getopt.",
        "documentation": {}
    },
    {
        "label": "aruco_display",
        "kind": 2,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "def aruco_display(corners, ids, rejected, image):\n    if len(corners) > 0:\n        ids = ids.flatten()\n        for (markerConer, markerID) in zip(corners, ids):\n            corners = markerConer.reshape((4,2))\n            (topLeft, topRight, bottomRight, bottomLeft) = corners\n            topRight = (int(topRight[0]), int(topRight[1]))\n            bottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n            bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n            topLeft = (int(topLeft[0]), int(topLeft[1]))",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "ARUCO_DICT",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "ARUCO_DICT = {\n    \"DICT_4X4_50\" : cv2.aruco.DICT_4X4_50,\n    \"DICT_4X4_100\" : cv2.aruco.DICT_4X4_100,\n    \"DICT_4X4_250\" : cv2.aruco.DICT_4X4_250,\n    \"DICT_4X4_1000\" : cv2.aruco.DICT_4X4_1000,\n    \"DICT_5X5_50\" : cv2.aruco.DICT_5X5_50,\n    \"DICT_5X5_100\" : cv2.aruco.DICT_5X5_100,\n    \"DICT_5X5_250\" : cv2.aruco.DICT_5X5_250,\n    \"DICT_5X5_1000\" : cv2.aruco.DICT_5X5_1000,\n    \"DICT_6X6_50\" : cv2.aruco.DICT_6X6_50,",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "aruco_type",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "aruco_type = \"DICT_5X5_100\"\ndictionary = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[aruco_type])\narucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams)\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "dictionary",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "dictionary = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[aruco_type])\narucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams)\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "arucoParams",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "arucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams)\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000\n    height = int(width * (h / w))",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "detector = cv2.aruco.ArucoDetector(dictionary, arucoParams)\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000\n    height = int(width * (h / w))\n    img = cv2.resize(img, (width, height), interpolation = cv2.INTER_CUBIC)",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_1.camera_test",
        "description": "Hw_1.camera_test",
        "peekOfCode": "cap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000\n    height = int(width * (h / w))\n    img = cv2.resize(img, (width, height), interpolation = cv2.INTER_CUBIC)\n    # corners, ids, rejected = cv2.aruco.detectMarkers(img, dictionary, parameters = arucoParams)",
        "detail": "Hw_1.camera_test",
        "documentation": {}
    },
    {
        "label": "ARUCO_DICT",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "ARUCO_DICT = {\n    \"DICT_4X4_50\" : cv2.aruco.DICT_4X4_50,\n    \"DICT_4X4_100\" : cv2.aruco.DICT_4X4_100,\n    \"DICT_4X4_250\" : cv2.aruco.DICT_4X4_250,\n    \"DICT_4X4_1000\" : cv2.aruco.DICT_4X4_1000,\n    \"DICT_5X5_50\" : cv2.aruco.DICT_5X5_50,\n    \"DICT_5X5_100\" : cv2.aruco.DICT_5X5_100,\n    \"DICT_5X5_250\" : cv2.aruco.DICT_5X5_250,\n    \"DICT_5X5_1000\" : cv2.aruco.DICT_5X5_1000,\n    \"DICT_6X6_50\" : cv2.aruco.DICT_6X6_50,",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "aruco_type",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "aruco_type = \"DICT_6X6_250\"\n# Load the predefined dictionary\ndictionary = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[aruco_type])\narucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams) \n# Initialize the detector parameters using default values\nparameters =  cv2.aruco.DetectorParameters()\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "dictionary",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "dictionary = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[aruco_type])\narucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams) \n# Initialize the detector parameters using default values\nparameters =  cv2.aruco.DetectorParameters()\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "arucoParams",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "arucoParams = cv2.aruco.DetectorParameters()\ndetector = cv2.aruco.ArucoDetector(dictionary, arucoParams) \n# Initialize the detector parameters using default values\nparameters =  cv2.aruco.DetectorParameters()\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "detector = cv2.aruco.ArucoDetector(dictionary, arucoParams) \n# Initialize the detector parameters using default values\nparameters =  cv2.aruco.DetectorParameters()\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "parameters",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "parameters =  cv2.aruco.DetectorParameters()\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000\n    height = int(width * (h / w))\n    img = cv2.resize(img, (width, height), interpolation = cv2.INTER_CUBIC)",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_1.open_source",
        "description": "Hw_1.open_source",
        "peekOfCode": "cap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\nwhile cap.isOpened():\n    ret, img = cap.read()\n    h, w, c = img.shape\n    width = 1000\n    height = int(width * (h / w))\n    img = cv2.resize(img, (width, height), interpolation = cv2.INTER_CUBIC)\n    # Detect the markers in the image",
        "detail": "Hw_1.open_source",
        "documentation": {}
    },
    {
        "label": "PatternMaker",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "description": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "peekOfCode": "class PatternMaker:\n    def __init__(self, cols, rows, output, units, square_size, radius_rate, page_width, page_height, markers):\n        self.cols = cols\n        self.rows = rows\n        self.output = output\n        self.units = units\n        self.square_size = square_size\n        self.radius_rate = radius_rate\n        self.width = page_width\n        self.height = page_height",
        "detail": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "description": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "peekOfCode": "def main():\n    # parse command line options\n    parser = argparse.ArgumentParser(description=\"generate camera-calibration pattern\", add_help=False)\n    parser.add_argument(\"-H\", \"--help\", help=\"show help\", action=\"store_true\", dest=\"show_help\")\n    parser.add_argument(\"-o\", \"--output\", help=\"output file\", default=\"out.svg\", action=\"store\", dest=\"output\")\n    parser.add_argument(\"-c\", \"--columns\", help=\"pattern columns\", default=\"8\", action=\"store\", dest=\"columns\",\n                        type=int)\n    parser.add_argument(\"-r\", \"--rows\", help=\"pattern rows\", default=\"11\", action=\"store\", dest=\"rows\", type=int)\n    parser.add_argument(\"-T\", \"--type\", help=\"type of pattern\", default=\"circles\", action=\"store\", dest=\"p_type\",\n                        choices=[\"circles\", \"acircles\", \"checkerboard\", \"radon_checkerboard\"])",
        "detail": "Hw_2.opencv.doc.pattern_tools.gen_pattern",
        "documentation": {}
    },
    {
        "label": "SVG",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class SVG:\n    \"\"\"A tree representation of an SVG image or image fragment.\n    SVG(t, sub, sub, sub..., attribute=value)\n    t                       required             SVG type name\n    sub                     optional list        nested SVG elements or text/Unicode\n    attribute=value pairs   optional keywords    SVG attributes\n    In attribute names, \"__\" becomes \":\" and \"_\" becomes \"-\".\n    SVG in XML\n    <g id=\"mygroup\" fill=\"blue\">\n        <rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" />",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Fig",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Fig:\n    \"\"\"Stores graphics primitive objects and applies a single coordinate\n    transformation to them. To compose coordinate systems, nest Fig\n    objects.\n    Fig(obj, obj, obj..., trans=function)\n    obj     optional list    a list of drawing primitives\n    trans   default=None     a coordinate transformation function\n    >>> fig = Fig(Line(0,0,1,1), Rect(0.2,0.2,0.8,0.8), trans=\"2*x, 2*y\")\n    >>> print fig.SVG().xml()\n    <g>",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Plot",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Plot:\n    \"\"\"Acts like Fig, but draws a coordinate axis. You also need to supply plot ranges.\n    Plot(xmin, xmax, ymin, ymax, obj, obj, obj..., keyword options...)\n    xmin, xmax      required        minimum and maximum x values (in the objs' coordinates)\n    ymin, ymax      required        minimum and maximum y values (in the objs' coordinates)\n    obj             optional list   drawing primitives\n    keyword options keyword list    options defined below\n    The following are keyword options, with their default values:\n    trans           None          transformation function\n    x, y            5, 5          upper-left corner of the Plot in SVG coordinates",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Frame",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Frame:\n    text_defaults = {\"stroke\": \"none\", \"fill\": \"black\", \"font-size\": 5, }\n    axis_defaults = {}\n    tick_length = 1.5\n    minitick_length = 0.75\n    text_xaxis_offset = 1.\n    text_yaxis_offset = 2.\n    text_xtitle_offset = 6.\n    text_ytitle_offset = 12.\n    def __repr__(self):",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Path",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Path:\n    \"\"\"Path represents an SVG path, an arbitrary set of curves and\n    straight segments. Unlike SVG(\"path\", d=\"...\"), Path stores\n    coordinates as a list of numbers, rather than a string, so that it is\n    transformable in a Fig.\n    Path(d, attribute=value)\n    d                       required        path data\n    attribute=value pairs   keyword list    SVG attributes\n    See http://www.w3.org/TR/SVG/paths.html for specification of paths\n    from text.",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Curve",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Curve:\n    \"\"\"Draws a parametric function as a path.\n    Curve(f, low, high, loop, attribute=value)\n    f                      required         a Python callable or string in\n                                            the form \"f(t), g(t)\"\n    low, high              required         left and right endpoints\n    loop                   default=False    if True, connect the endpoints\n    attribute=value pairs  keyword list     SVG attributes\n    \"\"\"\n    defaults = {}",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Poly",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Poly:\n    \"\"\"Draws a curve specified by a sequence of points. The curve may be\n    piecewise linear, like a polygon, or a Bezier curve.\n    Poly(d, mode, loop, attribute=value)\n    d                       required        list of tuples representing points\n                                            and possibly control points\n    mode                    default=\"L\"     \"lines\", \"bezier\", \"velocity\",\n                                            \"foreback\", \"smooth\", or an abbreviation\n    loop                    default=False   if True, connect the first and last\n                                            point, closing the loop",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Text",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Text:\n    \"\"\"Draws a text string at a specified point in local coordinates.\n    x, y                   required      location of the point in local coordinates\n    d                      required      text/Unicode string\n    attribute=value pairs  keyword list  SVG attributes\n    \"\"\"\n    defaults = {\"stroke\": \"none\", \"fill\": \"black\", \"font-size\": 5, }\n    def __repr__(self):\n        return \"<Text %s at (%g, %g) %s>\" % (repr(self.d), self.x, self.y, self.attr)\n    def __init__(self, x, y, d, **attr):",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "TextGlobal",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class TextGlobal:\n    \"\"\"Draws a text string at a specified point in global coordinates.\n    x, y                   required      location of the point in global coordinates\n    d                      required      text/Unicode string\n    attribute=value pairs  keyword list  SVG attributes\n    \"\"\"\n    defaults = {\"stroke\": \"none\", \"fill\": \"black\", \"font-size\": 5, }\n    def __repr__(self):\n        return \"<TextGlobal %s at (%s, %s) %s>\" % (repr(self.d), str(self.x), str(self.y), self.attr)\n    def __init__(self, x, y, d, **attr):",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Dots",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Dots:\n    \"\"\"Dots draws SVG symbols at a set of points.\n    d                      required               list of (x,y) points\n    symbol                 default=None           SVG symbol or a new identifier to\n                                                  label an auto-generated symbol;\n                                                  if None, use pre-defined _circular_dot\n    width, height          default=1, 1           width and height of the symbols\n                                                  in SVG coordinates\n    attribute=value pairs  keyword list           SVG attributes\n    \"\"\"",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Line",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Line(Curve):\n    \"\"\"Draws a line between two points.\n    Line(x1, y1, x2, y2, arrow_start, arrow_end, attribute=value)\n    x1, y1                  required        the starting point\n    x2, y2                  required        the ending point\n    arrow_start             default=None    if an identifier string/Unicode,\n                                            draw a new arrow object at the\n                                            beginning of the line; if a marker,\n                                            draw that marker instead\n    arrow_end               default=None    same for the end of the line",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "LineGlobal",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class LineGlobal:\n    \"\"\"Draws a line between two points, one or both of which is in\n    global coordinates.\n    Line(x1, y1, x2, y2, lcoal1, local2, arrow_start, arrow_end, attribute=value)\n    x1, y1                  required        the starting point\n    x2, y2                  required        the ending point\n    local1                  default=False   if True, interpret first point as a\n                                            local coordinate (apply transform)\n    local2                  default=False   if True, interpret second point as a\n                                            local coordinate (apply transform)",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "VLine",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class VLine(Line):\n    \"\"\"Draws a vertical line.\n    VLine(y1, y2, x, attribute=value)\n    y1, y2                  required        y range\n    x                       required        x position\n    attribute=value pairs   keyword list    SVG attributes\n    \"\"\"\n    defaults = {}\n    def __repr__(self):\n        return \"<VLine (%g, %g) at x=%s %s>\" % (self.y1, self.y2, self.x, self.attr)",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "HLine",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class HLine(Line):\n    \"\"\"Draws a horizontal line.\n    HLine(x1, x2, y, attribute=value)\n    x1, x2                  required        x range\n    y                       required        y position\n    attribute=value pairs   keyword list    SVG attributes\n    \"\"\"\n    defaults = {}\n    def __repr__(self):\n        return \"<HLine (%g, %g) at y=%s %s>\" % (self.x1, self.x2, self.y, self.attr)",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Rect",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Rect(Curve):\n    \"\"\"Draws a rectangle.\n    Rect(x1, y1, x2, y2, attribute=value)\n    x1, y1                  required        the starting point\n    x2, y2                  required        the ending point\n    attribute=value pairs   keyword list    SVG attributes\n    \"\"\"\n    defaults = {}\n    def __repr__(self):\n        return \"<Rect (%g, %g), (%g, %g) %s>\" % (",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Ellipse",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Ellipse(Curve):\n    \"\"\"Draws an ellipse from a semimajor vector (ax,ay) and a semiminor\n    length (b).\n    Ellipse(x, y, ax, ay, b, attribute=value)\n    x, y                    required        the center of the ellipse/circle\n    ax, ay                  required        a vector indicating the length\n                                            and direction of the semimajor axis\n    b                       required        the length of the semiminor axis.\n                                            If equal to sqrt(ax2 + ay2), the\n                                            ellipse is a circle",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Ticks",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Ticks:\n    \"\"\"Superclass for all graphics primitives that draw ticks,\n    miniticks, and tick labels.  This class only draws the ticks.\n    Ticks(f, low, high, ticks, miniticks, labels, logbase, arrow_start,\n          arrow_end, text_attr, attribute=value)\n    f                       required        parametric function along which ticks\n                                            will be drawn; has the same format as\n                                            the function used in Curve\n    low, high               required        range of the independent variable\n    ticks                   default=-10     request ticks according to the standard",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "CurveAxis",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class CurveAxis(Curve, Ticks):\n    \"\"\"Draw an axis with tick marks along a parametric curve.\n    CurveAxis(f, low, high, ticks, miniticks, labels, logbase, arrow_start, arrow_end,\n    text_attr, attribute=value)\n    f                      required         a Python callable or string in\n                                            the form \"f(t), g(t)\", just like Curve\n    low, high              required         left and right endpoints\n    ticks                  default=-10      request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks              default=True     request miniticks according to the",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "LineAxis",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class LineAxis(Line, Ticks):\n    \"\"\"Draws an axis with tick marks along a line.\n    LineAxis(x1, y1, x2, y2, start, end, ticks, miniticks, labels, logbase,\n    arrow_start, arrow_end, text_attr, attribute=value)\n    x1, y1                  required        starting point\n    x2, y2                  required        ending point\n    start, end              default=0, 1    values to start and end labeling\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=True    request miniticks according to the",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "XAxis",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class XAxis(LineAxis):\n    \"\"\"Draws an x axis with tick marks.\n    XAxis(xmin, xmax, aty, ticks, miniticks, labels, logbase, arrow_start, arrow_end,\n    exclude, text_attr, attribute=value)\n    xmin, xmax              required        the x range\n    aty                     default=0       y position to draw the axis\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=True    request miniticks according to the\n                                            standard minitick specification",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "YAxis",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class YAxis(LineAxis):\n    \"\"\"Draws a y axis with tick marks.\n    YAxis(ymin, ymax, atx, ticks, miniticks, labels, logbase, arrow_start, arrow_end,\n    exclude, text_attr, attribute=value)\n    ymin, ymax              required        the y range\n    atx                     default=0       x position to draw the axis\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=True    request miniticks according to the\n                                            standard minitick specification",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Axes",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Axes:\n    \"\"\"Draw a pair of intersecting x-y axes.\n    Axes(xmin, xmax, ymin, ymax, atx, aty, xticks, xminiticks, xlabels, xlogbase,\n    yticks, yminiticks, ylabels, ylogbase, arrows, text_attr, attribute=value)\n    xmin, xmax               required       the x range\n    ymin, ymax               required       the y range\n    atx, aty                 default=0, 0   point where the axes try to cross;\n                                            if outside the range, the axes will\n                                            cross at the closest corner\n    xticks                   default=-10    request ticks according to the standard",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "HGrid",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class HGrid(Ticks):\n    \"\"\"Draws the horizontal lines of a grid over a specified region\n    using the standard tick specification (see help(Ticks)) to place the\n    grid lines.\n    HGrid(xmin, xmax, low, high, ticks, miniticks, logbase, mini_attr, attribute=value)\n    xmin, xmax              required        the x range\n    low, high               required        the y range\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=False   request miniticks according to the",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "VGrid",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class VGrid(Ticks):\n    \"\"\"Draws the vertical lines of a grid over a specified region\n    using the standard tick specification (see help(Ticks)) to place the\n    grid lines.\n    HGrid(ymin, ymax, low, high, ticks, miniticks, logbase, mini_attr, attribute=value)\n    ymin, ymax              required        the y range\n    low, high               required        the x range\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=False   request miniticks according to the",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "Grid",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class Grid(Ticks):\n    \"\"\"Draws a grid over a specified region using the standard tick\n    specification (see help(Ticks)) to place the grid lines.\n    Grid(xmin, xmax, ymin, ymax, ticks, miniticks, logbase, mini_attr, attribute=value)\n    xmin, xmax              required        the x range\n    ymin, ymax              required        the y range\n    ticks                   default=-10     request ticks according to the standard\n                                            tick specification (see help(Ticks))\n    miniticks               default=False   request miniticks according to the\n                                            standard minitick specification",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "XErrorBars",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class XErrorBars:\n    \"\"\"Draws x error bars at a set of points. This is usually used\n    before (under) a set of Dots at the same points.\n    XErrorBars(d, attribute=value)\n    d                       required        list of (x,y,xerr...) points\n    attribute=value pairs   keyword list    SVG attributes\n    If points in d have\n        * 3 elements, the third is the symmetric error bar\n        * 4 elements, the third and fourth are the asymmetric lower and\n          upper error bar. The third element should be negative,",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "YErrorBars",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "class YErrorBars:\n    \"\"\"Draws y error bars at a set of points. This is usually used\n    before (under) a set of Dots at the same points.\n    YErrorBars(d, attribute=value)\n    d                       required        list of (x,y,yerr...) points\n    attribute=value pairs   keyword list    SVG attributes\n    If points in d have\n        * 3 elements, the third is the symmetric error bar\n        * 4 elements, the third and fourth are the asymmetric lower and\n          upper error bar. The third element should be negative,",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "rgb",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def rgb(r, g, b, maximum=1.):\n    \"\"\"Create an SVG color string \"#xxyyzz\" from r, g, and b.\n    r,g,b = 0 is black and r,g,b = maximum is white.\n    \"\"\"\n    return \"#%02x%02x%02x\" % (max(0, min(r*255./maximum, 255)),\n                              max(0, min(g*255./maximum, 255)),\n                              max(0, min(b*255./maximum, 255)))\ndef attr_preprocess(attr):\n    attrCopy = attr.copy()\n    for name in attr.keys():",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "attr_preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def attr_preprocess(attr):\n    attrCopy = attr.copy()\n    for name in attr.keys():\n        name_colon = re.sub(\"__\", \":\", name)\n        if name_colon != name:\n            attrCopy[name_colon] = attrCopy[name]\n            del attrCopy[name]\n            name = name_colon\n        name_dash = re.sub(\"_\", \"-\", name)\n        if name_dash != name:",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "canvas",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def canvas(*sub, **attr):\n    \"\"\"Creates a top-level SVG object, allowing the user to control the\n    image size and aspect ratio.\n    canvas(sub, sub, sub..., attribute=value)\n    sub                     optional list       nested SVG elements or text/Unicode\n    attribute=value pairs   optional keywords   SVG attributes\n    Default attribute values:\n    width           \"400px\"\n    height          \"400px\"\n    viewBox         \"0 0 100 100\"",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "canvas_outline",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def canvas_outline(*sub, **attr):\n    \"\"\"Same as canvas(), but draws an outline around the drawable area,\n    so that you know how close your image is to the edges.\"\"\"\n    svg = canvas(*sub, **attr)\n    match = re.match(r\"[, \\t]*([0-9e.+\\-]+)[, \\t]+([0-9e.+\\-]+)[, \\t]+([0-9e.+\\-]+)[, \\t]+([0-9e.+\\-]+)[, \\t]*\", svg[\"viewBox\"])\n    if match is None:\n        raise ValueError( \"canvas viewBox is incorrectly formatted\")\n    x, y, width, height = [float(x) for x in match.groups()]\n    svg.prepend(SVG(\"rect\", x=x, y=y, width=width, height=height, stroke=\"none\", fill=\"cornsilk\"))\n    svg.append(SVG(\"rect\", x=x, y=y, width=width, height=height, stroke=\"black\", fill=\"none\"))",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def template(fileName, svg, replaceme=\"REPLACEME\"):\n    \"\"\"Loads an SVG image from a file, replacing instances of\n    <REPLACEME /> with a given svg object.\n    fileName         required                name of the template SVG\n    svg              required                SVG object for replacement\n    replaceme        default=\"REPLACEME\"     fake SVG element to be replaced by the given object\n    >>> print load(\"template.svg\")\n    None                 <svg (2 sub) style=u'stroke:black; fill:none; stroke-width:0.5pt; stroke-linejoi\n    [0]                      <rect height=u'100' width=u'100' stroke=u'none' y=u'0' x=u'0' fill=u'yellow'\n    [1]                      <REPLACEME />",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def load(fileName):\n    \"\"\"Loads an SVG image from a file.\"\"\"\n    return load_stream(open(fileName))\ndef load_stream(stream):\n    \"\"\"Loads an SVG image from a stream (can be a string or a file object).\"\"\"\n    from xml.sax import handler, make_parser\n    from xml.sax.handler import feature_namespaces, feature_external_ges, feature_external_pes\n    class ContentHandler(handler.ContentHandler):\n        def __init__(self):\n            self.stack = []",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "load_stream",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def load_stream(stream):\n    \"\"\"Loads an SVG image from a stream (can be a string or a file object).\"\"\"\n    from xml.sax import handler, make_parser\n    from xml.sax.handler import feature_namespaces, feature_external_ges, feature_external_pes\n    class ContentHandler(handler.ContentHandler):\n        def __init__(self):\n            self.stack = []\n            self.output = None\n            self.all_whitespace = re.compile(r\"^\\s*$\")\n        def startElement(self, name, attr):",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "set_func_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def set_func_name(f, name):\n    \"\"\"try to patch the function name string into a function object\"\"\"\n    try:\n        f.func_name = name\n    except TypeError:\n        # py 2.3 raises: TypeError: readonly attribute\n        pass\ndef totrans(expr, vars=(\"x\", \"y\"), globals=None, locals=None):\n    \"\"\"Converts to a coordinate transformation (a function that accepts\n    two arguments and returns two values).",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "totrans",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def totrans(expr, vars=(\"x\", \"y\"), globals=None, locals=None):\n    \"\"\"Converts to a coordinate transformation (a function that accepts\n    two arguments and returns two values).\n    expr       required                  a string expression or a function\n                                         of two real or one complex value\n    vars       default=(\"x\", \"y\")        independent variable names; a singleton\n                                         (\"z\",) is interpreted as complex\n    globals    default=None              dict of global variables\n    locals     default=None              dict of local variables\n    \"\"\"",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "window",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def window(xmin, xmax, ymin, ymax, x=0, y=0, width=100, height=100,\n           xlogbase=None, ylogbase=None, minusInfinity=-1000, flipx=False, flipy=True):\n    \"\"\"Creates and returns a coordinate transformation (a function that\n    accepts two arguments and returns two values) that transforms from\n        (xmin, ymin), (xmax, ymax)\n    to\n        (x, y), (x + width, y + height).\n    xlogbase, ylogbase    default=None, None     if a number, transform\n                                                 logarithmically with given base\n    minusInfinity         default=-1000          what to return if",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def rotate(angle, cx=0, cy=0):\n    \"\"\"Creates and returns a coordinate transformation which rotates\n    around (cx,cy) by \"angle\" degrees.\"\"\"\n    angle *= math.pi/180.\n    return lambda x, y: (cx + math.cos(angle)*(x - cx) - math.sin(angle)*(y - cy), cy + math.sin(angle)*(x - cx) + math.cos(angle)*(y - cy))\nclass Fig:\n    \"\"\"Stores graphics primitive objects and applies a single coordinate\n    transformation to them. To compose coordinate systems, nest Fig\n    objects.\n    Fig(obj, obj, obj..., trans=function)",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "pathtoPath",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def pathtoPath(svg):\n    \"\"\"Converts SVG(\"path\", d=\"...\") into Path(d=[...]).\"\"\"\n    if not isinstance(svg, SVG) or svg.t != \"path\":\n        raise TypeError (\"Only SVG <path /> objects can be converted into Paths\")\n    attr = dict(svg.attr)\n    d = attr[\"d\"]\n    del attr[\"d\"]\n    for key in attr.keys():\n        if not isinstance(key, str):\n            value = attr[key]",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "funcRtoC",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def funcRtoC(expr, var=\"t\", globals=None, locals=None):\n    \"\"\"Converts a complex \"z(t)\" string to a function acceptable for Curve.\n    expr    required        string in the form \"z(t)\"\n    var     default=\"t\"     name of the independent variable\n    globals default=None    dict of global variables used in the expression;\n                            you may want to use Python's builtin globals()\n    locals  default=None    dict of local variables\n    \"\"\"\n    if locals is None:\n        locals = {}  # python 2.3's eval() won't accept None",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "funcRtoR2",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def funcRtoR2(expr, var=\"t\", globals=None, locals=None):\n    \"\"\"Converts a \"f(t), g(t)\" string to a function acceptable for Curve.\n    expr    required        string in the form \"f(t), g(t)\"\n    var     default=\"t\"     name of the independent variable\n    globals default=None    dict of global variables used in the expression;\n                            you may want to use Python's builtin globals()\n    locals  default=None    dict of local variables\n    \"\"\"\n    if locals is None:\n        locals = {}  # python 2.3's eval() won't accept None",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "funcRtoR",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def funcRtoR(expr, var=\"x\", globals=None, locals=None):\n    \"\"\"Converts a \"f(x)\" string to a function acceptable for Curve.\n    expr    required        string in the form \"f(x)\"\n    var     default=\"x\"     name of the independent variable\n    globals default=None    dict of global variables used in the expression;\n                            you may want to use Python's builtin globals()\n    locals  default=None    dict of local variables\n    \"\"\"\n    if locals is None:\n        locals = {}  # python 2.3's eval() won't accept None",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "make_symbol",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def make_symbol(id, shape=\"dot\", **attr):\n    \"\"\"Creates a new instance of an SVG symbol to avoid cross-linking objects.\n    id                    required         a new identifier (string/Unicode)\n    shape                 default=\"dot\"  the shape name from _symbol_templates\n    attribute=value list  keyword list     modify the SVG attributes of the new symbol\n    \"\"\"\n    output = copy.deepcopy(_symbol_templates[shape])\n    for i in output.sub:\n        i.attr.update(attr_preprocess(attr))\n    output[\"id\"] = id",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "make_marker",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def make_marker(id, shape, **attr):\n    \"\"\"Creates a new instance of an SVG marker to avoid cross-linking objects.\n    id                     required         a new identifier (string/Unicode)\n    shape                  required         the shape name from _marker_templates\n    attribute=value list   keyword list     modify the SVG attributes of the new marker\n    \"\"\"\n    output = copy.deepcopy(_marker_templates[shape])\n    for i in output.sub:\n        i.attr.update(attr_preprocess(attr))\n    output[\"id\"] = id",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "unumber",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "def unumber(x):\n    \"\"\"Converts numbers to a Unicode string, taking advantage of special\n    Unicode characters to make nice minus signs and scientific notation.\n    \"\"\"\n    output = u\"%g\" % x\n    if output[0] == u\"-\":\n        output = u\"\\u2013\" + output[1:]\n    index = output.find(u\"e\")\n    if index != -1:\n        uniout = unicode(output[:index]) + u\"\\u00d710\"",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_epsilon",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_epsilon = 1e-5\nif sys.version_info >= (3,0):\n  long = int\n  basestring = (str,bytes)\n# Fix Python 2.x.\ntry:\n    UNICODE_EXISTS = bool(type(unicode))\nexcept NameError:\n    unicode = lambda s: str(s)\ntry:",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_default_fileName",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_default_fileName = \"tmp.svg\"\n_hacks = {}\n_hacks[\"inkscape-text-vertical-shift\"] = False\ndef rgb(r, g, b, maximum=1.):\n    \"\"\"Create an SVG color string \"#xxyyzz\" from r, g, and b.\n    r,g,b = 0 is black and r,g,b = maximum is white.\n    \"\"\"\n    return \"#%02x%02x%02x\" % (max(0, min(r*255./maximum, 255)),\n                              max(0, min(g*255./maximum, 255)),\n                              max(0, min(b*255./maximum, 255)))",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_hacks",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_hacks = {}\n_hacks[\"inkscape-text-vertical-shift\"] = False\ndef rgb(r, g, b, maximum=1.):\n    \"\"\"Create an SVG color string \"#xxyyzz\" from r, g, and b.\n    r,g,b = 0 is black and r,g,b = maximum is white.\n    \"\"\"\n    return \"#%02x%02x%02x\" % (max(0, min(r*255./maximum, 255)),\n                              max(0, min(g*255./maximum, 255)),\n                              max(0, min(b*255./maximum, 255)))\ndef attr_preprocess(attr):",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_hacks[\"inkscape-text-vertical-shift\"]",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_hacks[\"inkscape-text-vertical-shift\"] = False\ndef rgb(r, g, b, maximum=1.):\n    \"\"\"Create an SVG color string \"#xxyyzz\" from r, g, and b.\n    r,g,b = 0 is black and r,g,b = maximum is white.\n    \"\"\"\n    return \"#%02x%02x%02x\" % (max(0, min(r*255./maximum, 255)),\n                              max(0, min(g*255./maximum, 255)),\n                              max(0, min(b*255./maximum, 255)))\ndef attr_preprocess(attr):\n    attrCopy = attr.copy()",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_canvas_defaults",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_canvas_defaults = {\"width\": \"400px\",\n                    \"height\": \"400px\",\n                    \"viewBox\": \"0 0 100 100\",\n                    \"xmlns\": \"http://www.w3.org/2000/svg\",\n                    \"xmlns:xlink\": \"http://www.w3.org/1999/xlink\",\n                    \"version\": \"1.1\",\n                    \"style\": {\"stroke\": \"black\",\n                              \"fill\": \"none\",\n                              \"stroke-width\": \"0.5pt\",\n                              \"stroke-linejoin\": \"round\",",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_symbol_templates",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_symbol_templates = {\"dot\": SVG(\"symbol\", SVG(\"circle\", cx=0, cy=0, r=1, stroke=\"none\", fill=\"black\"), viewBox=\"0 0 1 1\", overflow=\"visible\"),\n                    \"box\": SVG(\"symbol\", SVG(\"rect\", x1=-1, y1=-1, x2=1, y2=1, stroke=\"none\", fill=\"black\"), viewBox=\"0 0 1 1\", overflow=\"visible\"),\n                    \"uptri\": SVG(\"symbol\", SVG(\"path\", d=\"M -1 0.866 L 1 0.866 L 0 -0.866 Z\", stroke=\"none\", fill=\"black\"), viewBox=\"0 0 1 1\", overflow=\"visible\"),\n                    \"downtri\": SVG(\"symbol\", SVG(\"path\", d=\"M -1 -0.866 L 1 -0.866 L 0 0.866 Z\", stroke=\"none\", fill=\"black\"), viewBox=\"0 0 1 1\", overflow=\"visible\"),\n                    }\ndef make_symbol(id, shape=\"dot\", **attr):\n    \"\"\"Creates a new instance of an SVG symbol to avoid cross-linking objects.\n    id                    required         a new identifier (string/Unicode)\n    shape                 default=\"dot\"  the shape name from _symbol_templates\n    attribute=value list  keyword list     modify the SVG attributes of the new symbol",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_circular_dot",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_circular_dot = make_symbol(\"circular_dot\")\nclass Dots:\n    \"\"\"Dots draws SVG symbols at a set of points.\n    d                      required               list of (x,y) points\n    symbol                 default=None           SVG symbol or a new identifier to\n                                                  label an auto-generated symbol;\n                                                  if None, use pre-defined _circular_dot\n    width, height          default=1, 1           width and height of the symbols\n                                                  in SVG coordinates\n    attribute=value pairs  keyword list           SVG attributes",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "_marker_templates",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "description": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "peekOfCode": "_marker_templates = {\"arrow_start\": SVG(\"marker\", SVG(\"path\", d=\"M 9 3.6 L 10.5 0 L 0 3.6 L 10.5 7.2 L 9 3.6 Z\"), viewBox=\"0 0 10.5 7.2\", refX=\"9\", refY=\"3.6\", markerWidth=\"10.5\", markerHeight=\"7.2\", markerUnits=\"strokeWidth\", orient=\"auto\", stroke=\"none\", fill=\"black\"),\n                    \"arrow_end\": SVG(\"marker\", SVG(\"path\", d=\"M 1.5 3.6 L 0 0 L 10.5 3.6 L 0 7.2 L 1.5 3.6 Z\"), viewBox=\"0 0 10.5 7.2\", refX=\"1.5\", refY=\"3.6\", markerWidth=\"10.5\", markerHeight=\"7.2\", markerUnits=\"strokeWidth\", orient=\"auto\", stroke=\"none\", fill=\"black\"),\n                    }\ndef make_marker(id, shape, **attr):\n    \"\"\"Creates a new instance of an SVG marker to avoid cross-linking objects.\n    id                     required         a new identifier (string/Unicode)\n    shape                  required         the shape name from _marker_templates\n    attribute=value list   keyword list     modify the SVG attributes of the new marker\n    \"\"\"\n    output = copy.deepcopy(_marker_templates[shape])",
        "detail": "Hw_2.opencv.doc.pattern_tools.svgfig",
        "documentation": {}
    },
    {
        "label": "sys.dont_write_bytecode",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "sys.dont_write_bytecode = True  # Don't generate .pyc files / __pycache__ directories\nimport os\nfrom pprint import pprint\nimport re\nimport logging\nimport json\nimport html_functions\nimport doxygen_scan\nloglevel=os.environ.get(\"LOGLEVEL\", None)\nif loglevel:",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "ROOT_DIR = sys.argv[1]\nPYTHON_SIGNATURES_FILE = sys.argv[2]\nJAVA_OR_PYTHON = sys.argv[3]\nADD_JAVA = False\nADD_PYTHON = False\nif JAVA_OR_PYTHON == \"python\":\n    ADD_PYTHON = True\npython_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "PYTHON_SIGNATURES_FILE",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "PYTHON_SIGNATURES_FILE = sys.argv[2]\nJAVA_OR_PYTHON = sys.argv[3]\nADD_JAVA = False\nADD_PYTHON = False\nif JAVA_OR_PYTHON == \"python\":\n    ADD_PYTHON = True\npython_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)\n    print(\"Loaded Python signatures: %d\" % len(python_signatures))",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "JAVA_OR_PYTHON",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "JAVA_OR_PYTHON = sys.argv[3]\nADD_JAVA = False\nADD_PYTHON = False\nif JAVA_OR_PYTHON == \"python\":\n    ADD_PYTHON = True\npython_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)\n    print(\"Loaded Python signatures: %d\" % len(python_signatures))\nimport xml.etree.ElementTree as ET",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "ADD_JAVA",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "ADD_JAVA = False\nADD_PYTHON = False\nif JAVA_OR_PYTHON == \"python\":\n    ADD_PYTHON = True\npython_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)\n    print(\"Loaded Python signatures: %d\" % len(python_signatures))\nimport xml.etree.ElementTree as ET\nroot = ET.parse(ROOT_DIR + 'opencv.tag')",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "ADD_PYTHON",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "ADD_PYTHON = False\nif JAVA_OR_PYTHON == \"python\":\n    ADD_PYTHON = True\npython_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)\n    print(\"Loaded Python signatures: %d\" % len(python_signatures))\nimport xml.etree.ElementTree as ET\nroot = ET.parse(ROOT_DIR + 'opencv.tag')\nfiles_dict = {}",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "python_signatures",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "python_signatures = dict()\nwith open(PYTHON_SIGNATURES_FILE, \"rt\") as f:\n    python_signatures = json.load(f)\n    print(\"Loaded Python signatures: %d\" % len(python_signatures))\nimport xml.etree.ElementTree as ET\nroot = ET.parse(ROOT_DIR + 'opencv.tag')\nfiles_dict = {}\n# constants and function from opencv.tag\nnamespaces = root.findall(\"./compound[@kind='namespace']\")\n#print(\"Found {} namespaces\".format(len(namespaces)))",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "root = ET.parse(ROOT_DIR + 'opencv.tag')\nfiles_dict = {}\n# constants and function from opencv.tag\nnamespaces = root.findall(\"./compound[@kind='namespace']\")\n#print(\"Found {} namespaces\".format(len(namespaces)))\nfor ns in namespaces:\n    ns_name = ns.find(\"./name\").text\n    #print('NS: {}'.format(ns_name))\n    doxygen_scan.scan_namespace_constants(ns, ns_name, files_dict)\n    doxygen_scan.scan_namespace_functions(ns, ns_name, files_dict)",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "files_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "files_dict = {}\n# constants and function from opencv.tag\nnamespaces = root.findall(\"./compound[@kind='namespace']\")\n#print(\"Found {} namespaces\".format(len(namespaces)))\nfor ns in namespaces:\n    ns_name = ns.find(\"./name\").text\n    #print('NS: {}'.format(ns_name))\n    doxygen_scan.scan_namespace_constants(ns, ns_name, files_dict)\n    doxygen_scan.scan_namespace_functions(ns, ns_name, files_dict)\n# class methods from opencv.tag",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "namespaces",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "namespaces = root.findall(\"./compound[@kind='namespace']\")\n#print(\"Found {} namespaces\".format(len(namespaces)))\nfor ns in namespaces:\n    ns_name = ns.find(\"./name\").text\n    #print('NS: {}'.format(ns_name))\n    doxygen_scan.scan_namespace_constants(ns, ns_name, files_dict)\n    doxygen_scan.scan_namespace_functions(ns, ns_name, files_dict)\n# class methods from opencv.tag\nclasses = root.findall(\"./compound[@kind='class']\")\n#print(\"Found {} classes\".format(len(classes)))",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "classes = root.findall(\"./compound[@kind='class']\")\n#print(\"Found {} classes\".format(len(classes)))\nfor c in classes:\n    c_name = c.find(\"./name\").text\n    file = c.find(\"./filename\").text\n    #print('Class: {} => {}'.format(c_name, file))\n    doxygen_scan.scan_class_methods(c, c_name, files_dict)\nprint('Doxygen files to scan: %s' % len(files_dict))\nfiles_processed = 0\nfiles_skipped = 0",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "files_processed",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "files_processed = 0\nfiles_skipped = 0\nsymbols_processed = 0\nfor file in files_dict:\n    #if file != \"dd/d9e/classcv_1_1VideoWriter.html\":\n    #if file != \"d4/d86/group__imgproc__filter.html\":\n    #if file != \"df/dfb/group__imgproc__object.html\":\n    #    continue\n    #print('File: ' + file)\n    anchor_list = files_dict[file]",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "files_skipped",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "files_skipped = 0\nsymbols_processed = 0\nfor file in files_dict:\n    #if file != \"dd/d9e/classcv_1_1VideoWriter.html\":\n    #if file != \"d4/d86/group__imgproc__filter.html\":\n    #if file != \"df/dfb/group__imgproc__object.html\":\n    #    continue\n    #print('File: ' + file)\n    anchor_list = files_dict[file]\n    active_anchors = [a for a in anchor_list if a.cppname in python_signatures]",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "symbols_processed",
        "kind": 5,
        "importPath": "Hw_2.opencv.doc.tools.add_signatures",
        "description": "Hw_2.opencv.doc.tools.add_signatures",
        "peekOfCode": "symbols_processed = 0\nfor file in files_dict:\n    #if file != \"dd/d9e/classcv_1_1VideoWriter.html\":\n    #if file != \"d4/d86/group__imgproc__filter.html\":\n    #if file != \"df/dfb/group__imgproc__object.html\":\n    #    continue\n    #print('File: ' + file)\n    anchor_list = files_dict[file]\n    active_anchors = [a for a in anchor_list if a.cppname in python_signatures]\n    if len(active_anchors) == 0: # no linked Python symbols",
        "detail": "Hw_2.opencv.doc.tools.add_signatures",
        "documentation": {}
    },
    {
        "label": "Symbol",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.tools.doxygen_scan",
        "description": "Hw_2.opencv.doc.tools.doxygen_scan",
        "peekOfCode": "class Symbol(object):\n    def __init__(self, anchor, type, cppname):\n        self.anchor = anchor\n        self.type = type\n        self.cppname = cppname\n        #if anchor == 'ga586ebfb0a7fb604b35a23d85391329be':\n        #    print(repr(self))\n        #    traceback.print_stack()\n    def __repr__(self):\n        return '%s:%s@%s' % (self.type, self.cppname, self.anchor)",
        "detail": "Hw_2.opencv.doc.tools.doxygen_scan",
        "documentation": {}
    },
    {
        "label": "add_to_file",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.doxygen_scan",
        "description": "Hw_2.opencv.doc.tools.doxygen_scan",
        "peekOfCode": "def add_to_file(files_dict, file, anchor):\n    anchors = files_dict.setdefault(file, [])\n    anchors.append(anchor)\ndef scan_namespace_constants(ns, ns_name, files_dict):\n    constants = ns.findall(\"./member[@kind='enumvalue']\")\n    for c in constants:\n        c_name = c.find(\"./name\").text\n        name = ns_name + '::' + c_name\n        file = c.find(\"./anchorfile\").text\n        anchor = c.find(\"./anchor\").text",
        "detail": "Hw_2.opencv.doc.tools.doxygen_scan",
        "documentation": {}
    },
    {
        "label": "scan_namespace_constants",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.doxygen_scan",
        "description": "Hw_2.opencv.doc.tools.doxygen_scan",
        "peekOfCode": "def scan_namespace_constants(ns, ns_name, files_dict):\n    constants = ns.findall(\"./member[@kind='enumvalue']\")\n    for c in constants:\n        c_name = c.find(\"./name\").text\n        name = ns_name + '::' + c_name\n        file = c.find(\"./anchorfile\").text\n        anchor = c.find(\"./anchor\").text\n        #print('    CONST: {} => {}#{}'.format(name, file, anchor))\n        add_to_file(files_dict, file, Symbol(anchor, \"const\", name))\ndef scan_namespace_functions(ns, ns_name, files_dict):",
        "detail": "Hw_2.opencv.doc.tools.doxygen_scan",
        "documentation": {}
    },
    {
        "label": "scan_namespace_functions",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.doxygen_scan",
        "description": "Hw_2.opencv.doc.tools.doxygen_scan",
        "peekOfCode": "def scan_namespace_functions(ns, ns_name, files_dict):\n    functions = ns.findall(\"./member[@kind='function']\")\n    for f in functions:\n        f_name = f.find(\"./name\").text\n        name = ns_name + '::' + f_name\n        file = f.find(\"./anchorfile\").text\n        anchor = f.find(\"./anchor\").text\n        #print('    FN: {} => {}#{}'.format(name, file, anchor))\n        add_to_file(files_dict, file, Symbol(anchor, \"fn\", name))\ndef scan_class_methods(c, c_name, files_dict):",
        "detail": "Hw_2.opencv.doc.tools.doxygen_scan",
        "documentation": {}
    },
    {
        "label": "scan_class_methods",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.doxygen_scan",
        "description": "Hw_2.opencv.doc.tools.doxygen_scan",
        "peekOfCode": "def scan_class_methods(c, c_name, files_dict):\n    methods = c.findall(\"./member[@kind='function']\")\n    for m in methods:\n        m_name = m.find(\"./name\").text\n        name = c_name + '::' + m_name\n        file = m.find(\"./anchorfile\").text\n        anchor = m.find(\"./anchor\").text\n        #print('    Method: {} => {}#{}'.format(name, file, anchor))\n        add_to_file(files_dict, file, Symbol(anchor, \"method\", name))",
        "detail": "Hw_2.opencv.doc.tools.doxygen_scan",
        "documentation": {}
    },
    {
        "label": "load_html_file",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def load_html_file(file_dir):\n    \"\"\" Uses BeautifulSoup to load an html \"\"\"\n    with open(file_dir, 'rb') as fp:\n        data = fp.read()\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        data = data.decode(encoding='utf-8', errors='strict')\n    data = re.sub(r'(\\>)([ ]+)', lambda match: match.group(1) + ('!space!' * len(match.group(2))), data)\n    data = re.sub(r'([ ]+)(\\<)', lambda match: ('!space!' * len(match.group(1))) + match.group(2), data)\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        data = data.encode('utf-8', 'ignore')",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "update_html",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def update_html(file, soup):\n    s = str(soup)\n    s = s.replace('!space!', ' ')\n    if os.name == 'nt' or sys.version_info[0] == 3:\n        s = s.encode('utf-8', 'ignore')\n    with open(file, 'wb') as f:\n        f.write(s)\ndef insert_python_signatures(python_signatures, symbols_dict, filepath):\n    soup = load_html_file(filepath)\n    entries = soup.find_all(lambda tag: tag.name == \"a\" and tag.has_attr('id'))",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "insert_python_signatures",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def insert_python_signatures(python_signatures, symbols_dict, filepath):\n    soup = load_html_file(filepath)\n    entries = soup.find_all(lambda tag: tag.name == \"a\" and tag.has_attr('id'))\n    for e in entries:\n        anchor = e['id']\n        if anchor in symbols_dict:\n            s = symbols_dict[anchor]\n            logging.info('Process: %r' % s)\n            if s.type == 'fn' or s.type == 'method':\n                process_fn(soup, e, python_signatures[s.cppname], s)",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "process_fn",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def process_fn(soup, anchor, python_signature, symbol):\n    try:\n        r = anchor.find_next_sibling(class_='memitem').find(class_='memproto').find('table')\n        insert_python_fn_signature(soup, r, python_signature, symbol)\n    except:\n        logging.error(\"Can't process: %s\" % symbol)\n        traceback.print_exc()\n        pprint(anchor)\ndef process_const(soup, anchor, python_signature, symbol):\n    try:",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "process_const",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def process_const(soup, anchor, python_signature, symbol):\n    try:\n        #pprint(anchor.parent)\n        description = append(soup.new_tag('div', **{'class' : ['python_language']}),\n            'Python: ' + python_signature[0]['name'])\n        old = anchor.find_next_sibling('div', class_='python_language')\n        if old is None:\n            anchor.parent.append(description)\n        else:\n            old.replace_with(description)",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "insert_python_fn_signature",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def insert_python_fn_signature(soup, table, variants, symbol):\n    description = create_python_fn_description(soup, variants)\n    description['class'] = 'python_language'\n    soup = insert_or_replace(table, description, 'table', 'python_language')\n    return soup\ndef create_python_fn_description(soup, variants):\n    language = 'Python:'\n    table = soup.new_tag('table')\n    heading_row = soup.new_tag('th')\n    table.append(",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "create_python_fn_description",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def create_python_fn_description(soup, variants):\n    language = 'Python:'\n    table = soup.new_tag('table')\n    heading_row = soup.new_tag('th')\n    table.append(\n        append(soup.new_tag('tr'),\n               append(soup.new_tag('th', colspan=999, style=\"text-align:left\"), language)))\n    for v in variants:\n        #logging.debug(v)\n        add_signature_to_table(soup, table, v, language, type)",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "add_signature_to_table",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def add_signature_to_table(soup, table, signature, language, type):\n    \"\"\" Add a signature to an html table\"\"\"\n    row = soup.new_tag('tr')\n    row.append(soup.new_tag('td', style='width: 20px;'))\n    row.append(append(soup.new_tag('td'), signature['name'] + '('))\n    row.append(append(soup.new_tag('td', **{'class': 'paramname'}), signature['arg']))\n    row.append(append(soup.new_tag('td'), ') -> '))\n    row.append(append(soup.new_tag('td'), signature['ret']))\n    table.append(row)\ndef append(target, obj):",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "append",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def append(target, obj):\n    target.append(obj)\n    return target\ndef insert_or_replace(element_before, new_element, tag, tag_class):\n    old = element_before.find_next_sibling(tag, class_=tag_class)\n    if old is None:\n        element_before.insert_after(new_element)\n    else:\n        old.replace_with(new_element)",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "insert_or_replace",
        "kind": 2,
        "importPath": "Hw_2.opencv.doc.tools.html_functions",
        "description": "Hw_2.opencv.doc.tools.html_functions",
        "peekOfCode": "def insert_or_replace(element_before, new_element, tag, tag_class):\n    old = element_before.find_next_sibling(tag, class_=tag_class)\n    if old is None:\n        element_before.insert_after(new_element)\n    else:\n        old.replace_with(new_element)",
        "detail": "Hw_2.opencv.doc.tools.html_functions",
        "documentation": {}
    },
    {
        "label": "Tutorial",
        "kind": 6,
        "importPath": "Hw_2.opencv.doc.tools.scan_tutorials",
        "description": "Hw_2.opencv.doc.tools.scan_tutorials",
        "peekOfCode": "class Tutorial(object):\n    def __init__(self, path):\n        self.path = path\n        self.title = None # doxygen title\n        self.children = [] # ordered titles\n        self.prev = None\n        self.next = None\n        with open(path, \"rt\") as f:\n            self.parse(f)\n    def parse(self, f):",
        "detail": "Hw_2.opencv.doc.tools.scan_tutorials",
        "documentation": {}
    },
    {
        "label": "calibration_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.calib3d.misc.python.test.test_calibration",
        "description": "Hw_2.opencv.modules.calib3d.misc.python.test.test_calibration",
        "peekOfCode": "class calibration_test(NewOpenCVTests):\n    def test_calibration(self):\n        img_names = []\n        for i in range(1, 15):\n            if i < 10:\n                img_names.append('samples/data/left0{}.jpg'.format(str(i)))\n            elif i != 10:\n                img_names.append('samples/data/left{}.jpg'.format(str(i)))\n        square_size = 1.0\n        pattern_size = (9, 6)",
        "detail": "Hw_2.opencv.modules.calib3d.misc.python.test.test_calibration",
        "documentation": {}
    },
    {
        "label": "solvepnp_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.calib3d.misc.python.test.test_solvepnp",
        "description": "Hw_2.opencv.modules.calib3d.misc.python.test.test_solvepnp",
        "peekOfCode": "class solvepnp_test(NewOpenCVTests):\n    def test_regression_16040(self):\n        obj_points = np.array([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]], dtype=np.float32)\n        img_points = np.array(\n            [[700, 400], [700, 600], [900, 600], [900, 400]], dtype=np.float32\n        )\n        cameraMatrix = np.array(\n            [[712.0634, 0, 800], [0, 712.540, 500], [0, 0, 1]], dtype=np.float32\n        )\n        distCoeffs = np.array([[0, 0, 0, 0]], dtype=np.float32)",
        "detail": "Hw_2.opencv.modules.calib3d.misc.python.test.test_solvepnp",
        "documentation": {}
    },
    {
        "label": "remove_comments",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def remove_comments(s):\n    def replacer(match):\n        s = match.group(0)\n        if s.startswith('/'):\n            return \"\"\n        else:\n            return s\n    pattern = re.compile(\n        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n        re.DOTALL | re.MULTILINE",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "getTokens",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def getTokens(s):\n    return re.findall(r'[a-z_A-Z0-9_]+|[^[a-z_A-Z0-9_ \\n\\r\\t]', s)\ndef getParameter(pos, tokens):\n    deep = 0\n    p = []\n    while True:\n        if pos >= len(tokens):\n            break\n        if (tokens[pos] == ')' or tokens[pos] == ',') and deep == 0:\n            if tokens[pos] == ')':",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "getParameter",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def getParameter(pos, tokens):\n    deep = 0\n    p = []\n    while True:\n        if pos >= len(tokens):\n            break\n        if (tokens[pos] == ')' or tokens[pos] == ',') and deep == 0:\n            if tokens[pos] == ')':\n                pos = len(tokens)\n            else:",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "getParameters",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def getParameters(i, tokens):\n    assert tokens[i] == '('\n    i += 1\n    params = []\n    while True:\n        if i >= len(tokens) or tokens[i] == ')':\n            break\n        (param, i) = getParameter(i, tokens)\n        if len(param) > 0:\n            params.append(param)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "postProcessParameters",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def postProcessParameters(fns):\n    fns.sort(key=lambda x: x['name'])\n    for fn in fns:\n        fn['params_full'] = list(fn['params'])\n        for i in range(len(fn['params'])):\n            p = fn['params'][i]\n            if p.find('(') != -1:\n                p = re.sub(r'\\* *([a-zA-Z0-9_]*) ?\\)', '*)', p, 1)\n                fn['params'][i] = p\n                continue",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "readFunctionFilter",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def readFunctionFilter(fns, fileName):\n    try:\n        f = open(fileName, \"r\")\n    except:\n        print(\"ERROR: Can't open filter file: %s\" % fileName)\n        return 0\n    count = 0\n    while f:\n        line = f.readline()\n        if not line:",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "outputToString",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def outputToString(f):\n    def wrapped(*args, **kwargs):\n        from io import StringIO\n        old_stdout = sys.stdout\n        sys.stdout = str_stdout = StringIO()\n        res = f(*args, **kwargs)\n        assert res is None\n        sys.stdout = old_stdout\n        result = str_stdout.getvalue()\n        result = re.sub(r'([^\\n /]) [ ]+', r'\\1 ', result)  # don't remove spaces at start of line",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateFilterNames",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateFilterNames(fns):\n    for fn in fns:\n        print('%s%s' % ('' if 'enabled' in fn else '//', fn['name']))\n    print('#total %d' % len(fns))\ncallback_check = re.compile(r'([^\\(]*\\(.*)(\\* *)(\\).*\\(.*\\))')\ndef getTypeWithParam(t, p):\n    if callback_check.match(t):\n        return callback_check.sub(r'\\1 *' + p + r'\\3', t)\n    return t + ' ' + p\n@outputToString",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "getTypeWithParam",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def getTypeWithParam(t, p):\n    if callback_check.match(t):\n        return callback_check.sub(r'\\1 *' + p + r'\\3', t)\n    return t + ' ' + p\n@outputToString\ndef generateStructDefinitions(fns, lprefix='opencl_fn', enumprefix='OPENCL_FN'):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        decl_args = []",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateStructDefinitions",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateStructDefinitions(fns, lprefix='opencl_fn', enumprefix='OPENCL_FN'):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        decl_args = []\n        for (i, t) in enumerate(fn['params']):\n            decl_args.append(getTypeWithParam(t, 'p%d' % (i+1)))\n        decl_args_str = '(' + (', '.join(decl_args)) + ')'\n        print('%s%s%d(%s_%s, %s, %s)' % \\\n             (commentStr, lprefix, len(fn['params']), enumprefix, fn['name'], \\",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateStaticDefinitions",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateStaticDefinitions(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        decl_args = []\n        for (i, t) in enumerate(fn['params']):\n            decl_args.append(getTypeWithParam(t, 'p%d' % (i+1)))\n        decl_args_str = '(' + (', '.join(decl_args)) + ')'\n        print(commentStr + ('CL_RUNTIME_EXPORT %s%s (%s *%s_pfn)(%s) = %s;' % \\\n            ((' '.join(fn['modifiers'] + ' ') if len(fn['modifiers']) > 0 else ''),",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateListOfDefinitions",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateListOfDefinitions(fns, name='opencl_fn_list'):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    print('static const struct DynamicFnEntry* %s[] = {' % (name))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        if 'enabled' in fn:\n            print('    &%s_definition,' % (fn['name']))\n        else:\n            print('    NULL/*&%s_definition*/,' % (fn['name']))\n        first = False",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateEnums",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateEnums(fns, prefix='OPENCL_FN'):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    print('enum %s_ID {' % prefix)\n    for (i, fn) in enumerate(fns):\n        commentStr = '' if 'enabled' in fn else '//'\n        print(commentStr + ('    %s_%s = %d,' % (prefix, fn['name'], i)))\n    print('};')\n@outputToString\ndef generateRemapOrigin(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateRemapOrigin",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateRemapOrigin(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        print('#define %s %s_' % (fn['name'], fn['name']))\n@outputToString\ndef generateRemapDynamic(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        print('#undef %s' % (fn['name']))\n        commentStr = '' if 'enabled' in fn else '//'",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateRemapDynamic",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateRemapDynamic(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        print('#undef %s' % (fn['name']))\n        commentStr = '' if 'enabled' in fn else '//'\n        print(commentStr + ('#define %s %s_pfn' % (fn['name'], fn['name'])))\n@outputToString\ndef generateFnDeclaration(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateFnDeclaration",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateFnDeclaration(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        print(commentStr + ('extern CL_RUNTIME_EXPORT %s %s (%s *%s)(%s);' % (' '.join(fn['modifiers']), ' '.join(fn['ret']), ' '.join(fn['calling']),\n                                  fn['name'], ', '.join(fn['params'] if 'params_full' not in fn else fn['params_full']))))\n@outputToString\ndef generateTemplates(total, lprefix, switch_name, calling_convention=''):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for sz in range(total):",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateTemplates",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateTemplates(total, lprefix, switch_name, calling_convention=''):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for sz in range(total):\n        template_params = ['ID', '_R', 'decl_args']\n        params = ['p%d' % (i + 1) for i in range(0, sz)]\n        print('#define %s%d(%s) \\\\' % (lprefix, sz, ', '.join(template_params)))\n        print('    typedef _R (%s *ID##FN)decl_args; \\\\' % (calling_convention))\n        print('    static _R %s ID##_switch_fn decl_args \\\\' % (calling_convention))\n        print('    { return ((ID##FN)%s(ID))(%s); } \\\\' % (switch_name, ', '.join(params)))\n        print('')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "generateInlineWrappers",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def generateInlineWrappers(fns):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'\n        print('#undef %s' % (fn['name']))\n        print(commentStr + ('#define %s %s_fn' % (fn['name'], fn['name'])))\n        params = []\n        call_params = []\n        for i in range(0, len(fn['params'])):\n            t = fn['params'][i]",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "ProcessTemplate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "def ProcessTemplate(inputFile, ctx, noteLine='//\\n// AUTOGENERATED, DO NOT EDIT\\n//'):\n    f = open(inputFile, \"r\")\n    if noteLine:\n        print(noteLine)\n    for line in f:\n        if line.startswith('@'):\n            assert line[-1] == '\\n'\n            line = line[:-1]  # remove '\\n'\n            assert line[-1] == '@'\n            name = line[1:-1]",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "callback_check",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "peekOfCode": "callback_check = re.compile(r'([^\\(]*\\(.*)(\\* *)(\\).*\\(.*\\))')\ndef getTypeWithParam(t, p):\n    if callback_check.match(t):\n        return callback_check.sub(r'\\1 *' + p + r'\\3', t)\n    return t + ' ' + p\n@outputToString\ndef generateStructDefinitions(fns, lprefix='opencl_fn', enumprefix='OPENCL_FN'):\n    print('// generated by %s' % os.path.basename(sys.argv[0]))\n    for fn in fns:\n        commentStr = '' if 'enabled' in fn else '//'",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.common",
        "documentation": {}
    },
    {
        "label": "fns",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "fns = []\nwhile True:\n    line = f.readline()\n    if len(line) == 0:\n        break\n    assert isinstance(line, str)\n    parts = line.split();\n    if line.startswith('extern') and line.find('CL_API_CALL') != -1:\n        # read block of lines\n        while True:",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "filterFileName",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "filterFileName = './filter/%s_functions.list' % module_name\nnumEnabled = readFunctionFilter(fns, filterFileName)\nfunctionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "numEnabled",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "numEnabled = readFunctionFilter(fns, filterFileName)\nfunctionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "functionsFilter",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "functionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "filter_file",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "filter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx = {}\nctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx['CL_REMAP_ORIGIN']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx['CL_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx['CL_REMAP_DYNAMIC']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx['CL_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)\n    ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_DECLARATIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx['CL_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)\n    ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns)\n    ctx['CL_FN_ENUMS'] = generateEnums(fns)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "sys.stdout = outfile\nProcessTemplate('template/%s.hpp.in' % module_name, ctx)\nctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)\n    ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns)\n    ctx['CL_FN_ENUMS'] = generateEnums(fns)\n    ctx['CL_FN_SWITCH'] = generateTemplates(15, 'opencl_fn', 'opencl_check_fn', 'CL_API_CALL')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_INLINE_WRAPPERS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx['CL_FN_INLINE_WRAPPERS'] = generateInlineWrappers(fns)\nsys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)\n    ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns)\n    ctx['CL_FN_ENUMS'] = generateEnums(fns)\n    ctx['CL_FN_SWITCH'] = generateTemplates(15, 'opencl_fn', 'opencl_check_fn', 'CL_API_CALL')\nelse:\n    lprefix = module_name + '_fn'",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "sys.stdout = outfile_wrappers\nProcessTemplate('template/%s_wrappers.hpp.in' % module_name, ctx)\nif module_name == 'opencl_core':\n    ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns)\n    ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns)\n    ctx['CL_FN_ENUMS'] = generateEnums(fns)\n    ctx['CL_FN_SWITCH'] = generateTemplates(15, 'opencl_fn', 'opencl_check_fn', 'CL_API_CALL')\nelse:\n    lprefix = module_name + '_fn'\n    enumprefix = module_name.upper() + '_FN'",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = outfile_impl\nProcessTemplate('template/%s_impl.hpp.in' % module_name, ctx)\nsys.stdout = outfile_static_impl\nProcessTemplate('template/static_impl.hpp.in', dict(CL_STATIC_DEFINITIONS=generateStaticDefinitions(fns)))",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "sys.stdout = outfile_impl\nProcessTemplate('template/%s_impl.hpp.in' % module_name, ctx)\nsys.stdout = outfile_static_impl\nProcessTemplate('template/static_impl.hpp.in', dict(CL_STATIC_DEFINITIONS=generateStaticDefinitions(fns)))",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "peekOfCode": "sys.stdout = outfile_static_impl\nProcessTemplate('template/static_impl.hpp.in', dict(CL_STATIC_DEFINITIONS=generateStaticDefinitions(fns)))",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_cl",
        "documentation": {}
    },
    {
        "label": "fns",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "fns = []\nwhile True:\n    line = f.readline()\n    if len(line) == 0:\n        break\n    assert isinstance(line, str)\n    line = line.strip()\n    parts = line.split();\n    if (line.startswith('clblas') or line.startswith('cl_') or line == 'void') and len(line.split()) == 1 and line.find('(') == -1:\n        fn = {}",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "numEnabled",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "numEnabled = readFunctionFilter(fns, filterFileName)\nfunctionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDBLAS_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "functionsFilter",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "functionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDBLAS_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "filter_file",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "filter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDBLAS_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx = {}\nctx['CLAMDBLAS_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDBLAS_REMAP_ORIGIN']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CLAMDBLAS_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDBLAS_REMAP_DYNAMIC']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CLAMDBLAS_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDBLAS_FN_DECLARATIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CLAMDBLAS_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "sys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clblas.hpp', 'w')\nProcessTemplate('template/opencl_clblas.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENUMS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDBLAS_FN', )\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_SWITCH']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdblas_fn', 'openclamdblas_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENTRY_DEFINITIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdblas_fn', 'OPENCLAMDBLAS_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENTRY_LIST']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdblas_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "peekOfCode": "sys.stdout = open('../autogenerated/opencl_clblas_impl.hpp', 'w')\nProcessTemplate('template/opencl_clblas_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clblas",
        "documentation": {}
    },
    {
        "label": "fns",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "fns = []\nwhile True:\n    line = f.readline()\n    if len(line) == 0:\n        break\n    assert isinstance(line, str)\n    line = line.strip()\n    if line.startswith('CLFFTAPI'):\n        line = re.sub(r'\\n', r'', line)\n        while True:",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "numEnabled",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "numEnabled = readFunctionFilter(fns, filterFileName)\nfunctionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDFFT_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "functionsFilter",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "functionsFilter = generateFilterNames(fns)\nfilter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDFFT_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "filter_file",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "filter_file = open(filterFileName, 'w')\nfilter_file.write(functionsFilter)\nctx = {}\nctx['CLAMDFFT_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx = {}\nctx['CLAMDFFT_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDFFT_REMAP_ORIGIN']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CLAMDFFT_REMAP_ORIGIN'] = generateRemapOrigin(fns)\nctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDFFT_REMAP_DYNAMIC']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CLAMDFFT_REMAP_DYNAMIC'] = generateRemapDynamic(fns)\nctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CLAMDFFT_FN_DECLARATIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CLAMDFFT_FN_DECLARATIONS'] = generateFnDeclaration(fns)\nsys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "sys.stdout = open('../../../../include/opencv2/core/opencl/runtime/autogenerated/opencl_clfft.hpp', 'w')\nProcessTemplate('template/opencl_clfft.hpp.in', ctx)\nctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENUMS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CL_FN_ENUMS'] = generateEnums(fns, 'OPENCLAMDFFT_FN')\nctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_SWITCH']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CL_FN_SWITCH'] = generateTemplates(23, 'openclamdfft_fn', 'openclamdfft_check_fn', '')\nctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENTRY_DEFINITIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CL_FN_ENTRY_DEFINITIONS'] = generateStructDefinitions(fns, 'openclamdfft_fn', 'OPENCLAMDFFT_FN')\nctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CL_FN_ENTRY_LIST']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CL_FN_ENTRY_LIST'] = generateListOfDefinitions(fns, 'openclamdfft_fn')\nctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "ctx['CL_NUMBER_OF_ENABLED_FUNCTIONS'] = '// number of enabled functions: %d' % (numEnabled)\nsys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "description": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "peekOfCode": "sys.stdout = open('../autogenerated/opencl_clfft_impl.hpp', 'w')\nProcessTemplate('template/opencl_clfft_impl.hpp.in', ctx)",
        "detail": "Hw_2.opencv.modules.core.src.opencl.runtime.generator.parser_clfft",
        "documentation": {}
    },
    {
        "label": "dnn_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "class dnn_test(NewOpenCVTests):\n    def setUp(self):\n        super(dnn_test, self).setUp()\n        global g_dnnBackendsAndTargets\n        if g_dnnBackendsAndTargets is None:\n            g_dnnBackendsAndTargets = self.initBackendsAndTargets()\n        self.dnnBackendsAndTargets = g_dnnBackendsAndTargets\n    def initBackendsAndTargets(self):\n        self.dnnBackendsAndTargets = [\n            [cv.dnn.DNN_BACKEND_OPENCV, cv.dnn.DNN_TARGET_CPU],",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "normAssert",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def normAssert(test, a, b, msg=None, lInf=1e-5):\n    test.assertLess(np.max(np.abs(a - b)), lInf, msg)\ndef inter_area(box1, box2):\n    x_min, x_max = max(box1[0], box2[0]), min(box1[2], box2[2])\n    y_min, y_max = max(box1[1], box2[1]), min(box1[3], box2[3])\n    return (x_max - x_min) * (y_max - y_min)\ndef area(box):\n    return (box[2] - box[0]) * (box[3] - box[1])\ndef box2str(box):\n    left, top = box[0], box[1]",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "inter_area",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def inter_area(box1, box2):\n    x_min, x_max = max(box1[0], box2[0]), min(box1[2], box2[2])\n    y_min, y_max = max(box1[1], box2[1]), min(box1[3], box2[3])\n    return (x_max - x_min) * (y_max - y_min)\ndef area(box):\n    return (box[2] - box[0]) * (box[3] - box[1])\ndef box2str(box):\n    left, top = box[0], box[1]\n    width, height = box[2] - left, box[3] - top\n    return '[%f x %f from (%f, %f)]' % (width, height, left, top)",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def area(box):\n    return (box[2] - box[0]) * (box[3] - box[1])\ndef box2str(box):\n    left, top = box[0], box[1]\n    width, height = box[2] - left, box[3] - top\n    return '[%f x %f from (%f, %f)]' % (width, height, left, top)\ndef normAssertDetections(test, refClassIds, refScores, refBoxes, testClassIds, testScores, testBoxes,\n                 confThreshold=0.0, scores_diff=1e-5, boxes_iou_diff=1e-4):\n    matchedRefBoxes = [False] * len(refBoxes)\n    errMsg = ''",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "box2str",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def box2str(box):\n    left, top = box[0], box[1]\n    width, height = box[2] - left, box[3] - top\n    return '[%f x %f from (%f, %f)]' % (width, height, left, top)\ndef normAssertDetections(test, refClassIds, refScores, refBoxes, testClassIds, testScores, testBoxes,\n                 confThreshold=0.0, scores_diff=1e-5, boxes_iou_diff=1e-4):\n    matchedRefBoxes = [False] * len(refBoxes)\n    errMsg = ''\n    for i in range(len(testBoxes)):\n        testScore = testScores[i]",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "normAssertDetections",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def normAssertDetections(test, refClassIds, refScores, refBoxes, testClassIds, testScores, testBoxes,\n                 confThreshold=0.0, scores_diff=1e-5, boxes_iou_diff=1e-4):\n    matchedRefBoxes = [False] * len(refBoxes)\n    errMsg = ''\n    for i in range(len(testBoxes)):\n        testScore = testScores[i]\n        if testScore < confThreshold:\n            continue\n        testClassId, testBox = testClassIds[i], testBoxes[i]\n        matched = False",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "printParams",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def printParams(backend, target):\n    backendNames = {\n        cv.dnn.DNN_BACKEND_OPENCV: 'OCV',\n        cv.dnn.DNN_BACKEND_INFERENCE_ENGINE: 'DLIE'\n    }\n    targetNames = {\n        cv.dnn.DNN_TARGET_CPU: 'CPU',\n        cv.dnn.DNN_TARGET_OPENCL: 'OCL',\n        cv.dnn.DNN_TARGET_OPENCL_FP16: 'OCL_FP16',\n        cv.dnn.DNN_TARGET_MYRIAD: 'MYRIAD'",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "getDefaultThreshold",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "def getDefaultThreshold(target):\n    if target == cv.dnn.DNN_TARGET_OPENCL_FP16 or target == cv.dnn.DNN_TARGET_MYRIAD:\n        return 4e-3\n    else:\n        return 1e-5\ntestdata_required = bool(os.environ.get('OPENCV_DNN_TEST_REQUIRE_TESTDATA', False))\ng_dnnBackendsAndTargets = None\nclass dnn_test(NewOpenCVTests):\n    def setUp(self):\n        super(dnn_test, self).setUp()",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "testdata_required",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "testdata_required = bool(os.environ.get('OPENCV_DNN_TEST_REQUIRE_TESTDATA', False))\ng_dnnBackendsAndTargets = None\nclass dnn_test(NewOpenCVTests):\n    def setUp(self):\n        super(dnn_test, self).setUp()\n        global g_dnnBackendsAndTargets\n        if g_dnnBackendsAndTargets is None:\n            g_dnnBackendsAndTargets = self.initBackendsAndTargets()\n        self.dnnBackendsAndTargets = g_dnnBackendsAndTargets\n    def initBackendsAndTargets(self):",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "g_dnnBackendsAndTargets",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "description": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "peekOfCode": "g_dnnBackendsAndTargets = None\nclass dnn_test(NewOpenCVTests):\n    def setUp(self):\n        super(dnn_test, self).setUp()\n        global g_dnnBackendsAndTargets\n        if g_dnnBackendsAndTargets is None:\n            g_dnnBackendsAndTargets = self.initBackendsAndTargets()\n        self.dnnBackendsAndTargets = g_dnnBackendsAndTargets\n    def initBackendsAndTargets(self):\n        self.dnnBackendsAndTargets = [",
        "detail": "Hw_2.opencv.modules.dnn.misc.python.test.test_dnn",
        "documentation": {}
    },
    {
        "label": "ellipse2Rect",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]\n    center_y = params[4]\n    pts = cv.ellipse2Poly((int(center_x), int(center_y)), (int(rad_x), int(rad_y)),\n                          int(angle), 0, 360, 10)\n    rect = cv.boundingRect(pts)\n    left = rect[0]",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "addImage",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def addImage(imagePath):\n    assert('images' in  dataset)\n    imageId = len(dataset['images'])\n    dataset['images'].append({\n        'id': int(imageId),\n        'file_name': imagePath\n    })\n    return imageId\ndef addBBox(imageId, left, top, width, height):\n    assert('annotations' in  dataset)",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "addBBox",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def addBBox(imageId, left, top, width, height):\n    assert('annotations' in  dataset)\n    dataset['annotations'].append({\n        'id': len(dataset['annotations']),\n        'image_id': int(imageId),\n        'category_id': 0,  # Face\n        'bbox': [int(left), int(top), int(width), int(height)],\n        'iscrowd': 0,\n        'area': float(width * height)\n    })",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "addDetection",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def addDetection(detections, imageId, left, top, width, height, score):\n    detections.append({\n      'image_id': int(imageId),\n      'category_id': 0,  # Face\n      'bbox': [int(left), int(top), int(width), int(height)],\n      'score': float(score)\n    })\ndef fddb_dataset(annotations, images):\n    for d in os.listdir(annotations):\n        if fnmatch(d, 'FDDB-fold-*-ellipseList.txt'):",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "fddb_dataset",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def fddb_dataset(annotations, images):\n    for d in os.listdir(annotations):\n        if fnmatch(d, 'FDDB-fold-*-ellipseList.txt'):\n            with open(os.path.join(annotations, d), 'rt') as f:\n                lines = [line.rstrip('\\n') for line in f]\n                lineId = 0\n                while lineId < len(lines):\n                    # Image\n                    imgPath = lines[lineId]\n                    lineId += 1",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "wider_dataset",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def wider_dataset(annotations, images):\n    with open(annotations, 'rt') as f:\n        lines = [line.rstrip('\\n') for line in f]\n        lineId = 0\n        while lineId < len(lines):\n            # Image\n            imgPath = lines[lineId]\n            lineId += 1\n            imageId = addImage(os.path.join(images, imgPath))\n            # Faces",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def evaluate():\n    cocoGt = COCO('annotations.json')\n    cocoDt = cocoGt.loadRes('detections.json')\n    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n### Convert to COCO annotations format #########################################\nassert(args.fddb or args.wider)\nif args.fddb:",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "rm",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "def rm(f):\n    if os.path.exists(f):\n        os.remove(f)\nrm('annotations.json')\nrm('detections.json')",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "parser = argparse.ArgumentParser(\n        description='Evaluate OpenCV face detection algorithms '\n                    'using COCO evaluation tool, http://cocodataset.org/#detections-eval')\nparser.add_argument('--proto', help='Path to .prototxt of Caffe model or .pbtxt of TensorFlow graph')\nparser.add_argument('--model', help='Path to .caffemodel trained in Caffe or .pb from TensorFlow')\nparser.add_argument('--cascade', help='Optional path to trained Haar cascade as '\n                                      'an additional model for evaluation')\nparser.add_argument('--ann', help='Path to text file with ground truth annotations')\nparser.add_argument('--pics', help='Path to images root directory')\nparser.add_argument('--fddb', help='Evaluate FDDB dataset, http://vis-www.cs.umass.edu/fddb/', action='store_true')",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "args = parser.parse_args()\ndataset = {}\ndataset['images'] = []\ndataset['categories'] = [{ 'id': 0, 'name': 'face' }]\ndataset['annotations'] = []\ndef ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "dataset = {}\ndataset['images'] = []\ndataset['categories'] = [{ 'id': 0, 'name': 'face' }]\ndataset['annotations'] = []\ndef ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]\n    center_y = params[4]",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "dataset['images']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "dataset['images'] = []\ndataset['categories'] = [{ 'id': 0, 'name': 'face' }]\ndataset['annotations'] = []\ndef ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]\n    center_y = params[4]\n    pts = cv.ellipse2Poly((int(center_x), int(center_y)), (int(rad_x), int(rad_y)),",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "dataset['categories']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "dataset['categories'] = [{ 'id': 0, 'name': 'face' }]\ndataset['annotations'] = []\ndef ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]\n    center_y = params[4]\n    pts = cv.ellipse2Poly((int(center_x), int(center_y)), (int(rad_x), int(rad_y)),\n                          int(angle), 0, 360, 10)",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "dataset['annotations']",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "dataset['annotations'] = []\ndef ellipse2Rect(params):\n    rad_x = params[0]\n    rad_y = params[1]\n    angle = params[2] * 180.0 / pi\n    center_x = params[3]\n    center_y = params[4]\n    pts = cv.ellipse2Poly((int(center_x), int(center_y)), (int(rad_x), int(rad_y)),\n                          int(angle), 0, 360, 10)\n    rect = cv.boundingRect(pts)",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "detections",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "description": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "peekOfCode": "detections = []\nif args.proto and args.model:\n    net = cv.dnn.readNet(args.proto, args.model)\n    def detect(img, imageId):\n        imgWidth = img.shape[1]\n        imgHeight = img.shape[0]\n        net.setInput(cv.dnn.blobFromImage(img, 1.0, (300, 300), (104., 177., 123.), False, False))\n        out = net.forward()\n        for i in range(out.shape[2]):\n            confidence = out[0, 0, i, 2]",
        "detail": "Hw_2.opencv.modules.dnn.misc.face_detector_accuracy",
        "documentation": {}
    },
    {
        "label": "dnnLayer",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def dnnLayer(name):\n    return cvNet.getLayer(long(cvNet.getLayerId(name)))\ndef scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        if len(layer.blobs) > 1:\n            b = tf.Variable(layer.blobs[1].flatten(), dtype=dtype, name='add')\n            return tf.nn.bias_add(tf.multiply(x, w), b)\n        else:",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        if len(layer.blobs) > 1:\n            b = tf.Variable(layer.blobs[1].flatten(), dtype=dtype, name='add')\n            return tf.nn.bias_add(tf.multiply(x, w), b)\n        else:\n            return tf.multiply(x, w, name)\ndef conv(x, name, stride=1, pad='SAME', dilation=1, activ=None):",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def conv(x, name, stride=1, pad='SAME', dilation=1, activ=None):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].transpose(2, 3, 1, 0), dtype=dtype, name='weights')\n        if dilation == 1:\n            conv = tf.nn.conv2d(x, filter=w, strides=(1, stride, stride, 1), padding=pad)\n        else:\n            assert(stride == 1)\n            conv = tf.nn.atrous_conv2d(x, w, rate=dilation, padding=pad)\n        if len(layer.blobs) > 1:",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "batch_norm",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def batch_norm(x, name):\n    with tf.variable_scope(name):\n        # Unfortunately, TensorFlow's batch normalization layer doesn't work with fp16 input.\n        # Here we do a cast to fp32 but remove it in the frozen graph.\n        if x.dtype != tf.float32:\n            x = tf.cast(x, tf.float32)\n        layer = dnnLayer(name)\n        assert(len(layer.blobs) >= 3)\n        mean = layer.blobs[0].flatten()\n        std = layer.blobs[1].flatten()",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "l2norm",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def l2norm(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        return tf.nn.l2_normalize(x, 3, epsilon=1e-10) * w\n### Graph definition ###########################################################\ninp = tf.placeholder(dtype, [1, 300, 300, 3], 'data')\ndata_bn = batch_norm(inp, 'data_bn')\ndata_scale = scale(data_bn, 'data_scale')\n# Instead of tf.pad we use tf.space_to_batch_nd layers which override convolution's padding strategy to explicit numbers",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "tensorMsg",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "def tensorMsg(values):\n    msg = 'tensor { dtype: DT_FLOAT tensor_shape { dim { size: %d } }' % len(values)\n    for value in values:\n        msg += 'float_val: %f ' % value\n    return msg + '}'\n# Remove Const nodes and unused attributes.\nfor i in reversed(range(len(graph_def.node))):\n    if graph_def.node[i].op in ['Const', 'Dequantize']:\n        del graph_def.node[i]\n    for attr in ['T', 'data_format', 'Tshape', 'N', 'Tidx', 'Tdim',",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "parser = argparse.ArgumentParser(description=\"Use this script to create TensorFlow graph \"\n                                             \"with weights from OpenCV's face detection network. \"\n                                             \"Only backbone part of SSD model is converted this way. \"\n                                             \"Look for .pbtxt configuration file at \"\n                                             \"https://github.com/opencv/opencv_extra/tree/4.x/testdata/dnn/opencv_face_detector.pbtxt\")\nparser.add_argument('--model', help='Path to .caffemodel weights', required=True)\nparser.add_argument('--proto', help='Path to .prototxt Caffe model definition', required=True)\nparser.add_argument('--pb', help='Path to output .pb TensorFlow model', required=True)\nparser.add_argument('--pbtxt', help='Path to output .pbxt TensorFlow graph', required=True)\nparser.add_argument('--quantize', help='Quantize weights to uint8', action='store_true')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "args = parser.parse_args()\nassert(not args.quantize or not args.fp16)\ndtype = tf.float16 if args.fp16 else tf.float32\n################################################################################\ncvNet = cv.dnn.readNetFromCaffe(args.proto, args.model)\ndef dnnLayer(name):\n    return cvNet.getLayer(long(cvNet.getLayerId(name)))\ndef scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "dtype",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "dtype = tf.float16 if args.fp16 else tf.float32\n################################################################################\ncvNet = cv.dnn.readNetFromCaffe(args.proto, args.model)\ndef dnnLayer(name):\n    return cvNet.getLayer(long(cvNet.getLayerId(name)))\ndef scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        if len(layer.blobs) > 1:",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "cvNet",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "cvNet = cv.dnn.readNetFromCaffe(args.proto, args.model)\ndef dnnLayer(name):\n    return cvNet.getLayer(long(cvNet.getLayerId(name)))\ndef scale(x, name):\n    with tf.variable_scope(name):\n        layer = dnnLayer(name)\n        w = tf.Variable(layer.blobs[0].flatten(), dtype=dtype, name='mul')\n        if len(layer.blobs) > 1:\n            b = tf.Variable(layer.blobs[1].flatten(), dtype=dtype, name='add')\n            return tf.nn.bias_add(tf.multiply(x, w), b)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "inp = tf.placeholder(dtype, [1, 300, 300, 3], 'data')\ndata_bn = batch_norm(inp, 'data_bn')\ndata_scale = scale(data_bn, 'data_scale')\n# Instead of tf.pad we use tf.space_to_batch_nd layers which override convolution's padding strategy to explicit numbers\n# data_scale = tf.pad(data_scale, [[0, 0], [3, 3], [3, 3], [0, 0]])\ndata_scale = tf.space_to_batch_nd(data_scale, [1, 1], [[3, 3], [3, 3]], name='Pad')\nconv1_h = conv(data_scale, stride=2, pad='VALID', name='conv1_h')\nconv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "data_bn",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "data_bn = batch_norm(inp, 'data_bn')\ndata_scale = scale(data_bn, 'data_scale')\n# Instead of tf.pad we use tf.space_to_batch_nd layers which override convolution's padding strategy to explicit numbers\n# data_scale = tf.pad(data_scale, [[0, 0], [3, 3], [3, 3], [0, 0]])\ndata_scale = tf.space_to_batch_nd(data_scale, [1, 1], [[3, 3], [3, 3]], name='Pad')\nconv1_h = conv(data_scale, stride=2, pad='VALID', name='conv1_h')\nconv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "data_scale",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "data_scale = scale(data_bn, 'data_scale')\n# Instead of tf.pad we use tf.space_to_batch_nd layers which override convolution's padding strategy to explicit numbers\n# data_scale = tf.pad(data_scale, [[0, 0], [3, 3], [3, 3], [0, 0]])\ndata_scale = tf.space_to_batch_nd(data_scale, [1, 1], [[3, 3], [3, 3]], name='Pad')\nconv1_h = conv(data_scale, stride=2, pad='VALID', name='conv1_h')\nconv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "data_scale",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "data_scale = tf.space_to_batch_nd(data_scale, [1, 1], [[3, 3], [3, 3]], name='Pad')\nconv1_h = conv(data_scale, stride=2, pad='VALID', name='conv1_h')\nconv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv1_h = conv(data_scale, stride=2, pad='VALID', name='conv1_h')\nconv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv1_bn_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv1_bn_h = batch_norm(conv1_h, 'conv1_bn_h')\nconv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv1_scale_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv1_scale_h = scale(conv1_bn_h, 'conv1_scale_h')\nconv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv1_relu",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv1_relu = tf.nn.relu(conv1_scale_h)\nconv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv1_pool",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv1_pool = tf.layers.max_pooling2d(conv1_relu, pool_size=(3, 3), strides=(2, 2),\n                                     padding='SAME', name='conv1_pool')\nlayer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_conv1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_conv1_h = conv(conv1_pool, 'layer_64_1_conv1_h')\nlayer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_bn2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_bn2_h = batch_norm(layer_64_1_conv1_h, 'layer_64_1_bn2_h')\nlayer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_scale2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_scale2_h = scale(layer_64_1_bn2_h, 'layer_64_1_scale2_h')\nlayer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_relu2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_relu2 = tf.nn.relu(layer_64_1_scale2_h)\nlayer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_conv2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_conv2_h = conv(layer_64_1_relu2, 'layer_64_1_conv2_h')\nlayer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_64_1_sum",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_64_1_sum = layer_64_1_conv2_h + conv1_pool\nlayer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_bn1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_bn1_h = batch_norm(layer_64_1_sum, 'layer_128_1_bn1_h')\nlayer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_scale1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_scale1_h = scale(layer_128_1_bn1_h, 'layer_128_1_scale1_h')\nlayer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_relu1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_relu1 = tf.nn.relu(layer_128_1_scale1_h)\nlayer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_conv1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_conv1_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv1_h')\nlayer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_bn2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_bn2 = batch_norm(layer_128_1_conv1_h, 'layer_128_1_bn2')\nlayer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_scale2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_scale2 = scale(layer_128_1_bn2, 'layer_128_1_scale2')\nlayer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_relu2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_relu2 = tf.nn.relu(layer_128_1_scale2)\nlayer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_conv2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_conv2 = conv(layer_128_1_relu2, 'layer_128_1_conv2')\nlayer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_conv_expand_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_conv_expand_h = conv(layer_128_1_relu1, stride=2, name='layer_128_1_conv_expand_h')\nlayer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_128_1_sum",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_128_1_sum = layer_128_1_conv2 + layer_128_1_conv_expand_h\nlayer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_bn1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_bn1 = batch_norm(layer_128_1_sum, 'layer_256_1_bn1')\nlayer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_scale1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_scale1 = scale(layer_256_1_bn1, 'layer_256_1_scale1')\nlayer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_relu1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_relu1 = tf.nn.relu(layer_256_1_scale1)\n# layer_256_1_conv1 = tf.pad(layer_256_1_relu1, [[0, 0], [1, 1], [1, 1], [0, 0]])\nlayer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_conv1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_conv1 = tf.space_to_batch_nd(layer_256_1_relu1, [1, 1], [[1, 1], [1, 1]], name='Pad_1')\nlayer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_conv1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_conv1 = conv(layer_256_1_conv1, stride=2, pad='VALID', name='layer_256_1_conv1')\nlayer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_bn2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_bn2 = batch_norm(layer_256_1_conv1, 'layer_256_1_bn2')\nlayer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_scale2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_scale2 = scale(layer_256_1_bn2, 'layer_256_1_scale2')\nlayer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_relu2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_relu2 = tf.nn.relu(layer_256_1_scale2)\nlayer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_conv2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_conv2 = conv(layer_256_1_relu2, 'layer_256_1_conv2')\nlayer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_conv_expand",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_conv_expand = conv(layer_256_1_relu1, stride=2, name='layer_256_1_conv_expand')\nlayer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_256_1_sum",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_256_1_sum = layer_256_1_conv2 + layer_256_1_conv_expand\nlayer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_bn1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_bn1 = batch_norm(layer_256_1_sum, 'layer_512_1_bn1')\nlayer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_scale1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_scale1 = scale(layer_512_1_bn1, 'layer_512_1_scale1')\nlayer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_relu1",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_relu1 = tf.nn.relu(layer_512_1_scale1)\nlayer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_conv1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_conv1_h = conv(layer_512_1_relu1, 'layer_512_1_conv1_h')\nlayer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_bn2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_bn2_h = batch_norm(layer_512_1_conv1_h, 'layer_512_1_bn2_h')\nlayer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_scale2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_scale2_h = scale(layer_512_1_bn2_h, 'layer_512_1_scale2_h')\nlayer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_relu2",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_relu2 = tf.nn.relu(layer_512_1_scale2_h)\nlayer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_conv2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_conv2_h = conv(layer_512_1_relu2, dilation=2, name='layer_512_1_conv2_h')\nlayer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_conv_expand_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_conv_expand_h = conv(layer_512_1_relu1, 'layer_512_1_conv_expand_h')\nlayer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layer_512_1_sum",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layer_512_1_sum = layer_512_1_conv2_h + layer_512_1_conv_expand_h\nlast_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "last_bn_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "last_bn_h = batch_norm(layer_512_1_sum, 'last_bn_h')\nlast_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "last_scale_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "last_scale_h = scale(last_bn_h, 'last_scale_h')\nfc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "fc7",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "fc7 = tf.nn.relu(last_scale_h, name='last_relu')\nconv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv6_1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv6_1_h = conv(fc7, 'conv6_1_h', activ=tf.nn.relu)\nconv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv6_2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv6_2_h = conv(conv6_1_h, stride=2, name='conv6_2_h', activ=tf.nn.relu)\nconv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv7_1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv7_1_h = conv(conv6_2_h, 'conv7_1_h', activ=tf.nn.relu)\n# conv7_2_h = tf.pad(conv7_1_h, [[0, 0], [1, 1], [1, 1], [0, 0]])\nconv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv7_2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv7_2_h = tf.space_to_batch_nd(conv7_1_h, [1, 1], [[1, 1], [1, 1]], name='Pad_2')\nconv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv7_2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv7_2_h = conv(conv7_2_h, stride=2, pad='VALID', name='conv7_2_h', activ=tf.nn.relu)\nconv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv8_1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv8_1_h = conv(conv7_2_h, pad='SAME', name='conv8_1_h', activ=tf.nn.relu)\nconv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv8_2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv8_2_h = conv(conv8_1_h, pad='VALID', name='conv8_2_h', activ=tf.nn.relu)\nconv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv9_1_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv9_1_h = conv(conv8_2_h, 'conv9_1_h', activ=tf.nn.relu)\nconv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv9_2_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv9_2_h = conv(conv9_1_h, pad='VALID', name='conv9_2_h', activ=tf.nn.relu)\nconv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):\n        name += suffix",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "conv4_3_norm",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "conv4_3_norm = l2norm(layer_256_1_relu1, 'conv4_3_norm')\n### Locations and confidences ##################################################\nlocations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):\n        name += suffix\n        flat = tf.layers.flatten(conv(bottom, name))",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "locations",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "locations = []\nconfidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):\n        name += suffix\n        flat = tf.layers.flatten(conv(bottom, name))\n        flattenLayersNames.append(flat.name[:flat.name.find(':')])\n        top.append(flat)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "confidences",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "confidences = []\nflattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):\n        name += suffix\n        flat = tf.layers.flatten(conv(bottom, name))\n        flattenLayersNames.append(flat.name[:flat.name.find(':')])\n        top.append(flat)\nmbox_loc = tf.concat(locations, axis=-1, name='mbox_loc')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "flattenLayersNames",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "flattenLayersNames = []  # Collect all reshape layers names that should be replaced to flattens.\nfor top, suffix in zip([locations, confidences], ['_mbox_loc', '_mbox_conf']):\n    for bottom, name in zip([conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h],\n                            ['conv4_3_norm', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']):\n        name += suffix\n        flat = tf.layers.flatten(conv(bottom, name))\n        flattenLayersNames.append(flat.name[:flat.name.find(':')])\n        top.append(flat)\nmbox_loc = tf.concat(locations, axis=-1, name='mbox_loc')\nmbox_conf = tf.concat(confidences, axis=-1, name='mbox_conf')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "mbox_loc",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "mbox_loc = tf.concat(locations, axis=-1, name='mbox_loc')\nmbox_conf = tf.concat(confidences, axis=-1, name='mbox_conf')\ntotal = int(np.prod(mbox_conf.shape[1:]))\nmbox_conf_reshape = tf.reshape(mbox_conf, [-1, 2], name='mbox_conf_reshape')\nmbox_conf_softmax = tf.nn.softmax(mbox_conf_reshape, name='mbox_conf_softmax')\nmbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "mbox_conf",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "mbox_conf = tf.concat(confidences, axis=-1, name='mbox_conf')\ntotal = int(np.prod(mbox_conf.shape[1:]))\nmbox_conf_reshape = tf.reshape(mbox_conf, [-1, 2], name='mbox_conf_reshape')\nmbox_conf_softmax = tf.nn.softmax(mbox_conf_reshape, name='mbox_conf_softmax')\nmbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################\n    out_nodes = ['mbox_loc', 'mbox_conf_flatten']",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "total",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "total = int(np.prod(mbox_conf.shape[1:]))\nmbox_conf_reshape = tf.reshape(mbox_conf, [-1, 2], name='mbox_conf_reshape')\nmbox_conf_softmax = tf.nn.softmax(mbox_conf_reshape, name='mbox_conf_softmax')\nmbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################\n    out_nodes = ['mbox_loc', 'mbox_conf_flatten']\n    inp_nodes = [inp.name[:inp.name.find(':')]]",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "mbox_conf_reshape",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "mbox_conf_reshape = tf.reshape(mbox_conf, [-1, 2], name='mbox_conf_reshape')\nmbox_conf_softmax = tf.nn.softmax(mbox_conf_reshape, name='mbox_conf_softmax')\nmbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################\n    out_nodes = ['mbox_loc', 'mbox_conf_flatten']\n    inp_nodes = [inp.name[:inp.name.find(':')]]\n    np.random.seed(2701)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "mbox_conf_softmax",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "mbox_conf_softmax = tf.nn.softmax(mbox_conf_reshape, name='mbox_conf_softmax')\nmbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################\n    out_nodes = ['mbox_loc', 'mbox_conf_flatten']\n    inp_nodes = [inp.name[:inp.name.find(':')]]\n    np.random.seed(2701)\n    inputData = np.random.standard_normal([1, 3, 300, 300]).astype(np.float32)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "mbox_conf_flatten",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "mbox_conf_flatten = tf.reshape(mbox_conf_softmax, [-1, total], name='mbox_conf_flatten')\nflattenLayersNames.append('mbox_conf_flatten')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    ### Check correctness ######################################################\n    out_nodes = ['mbox_loc', 'mbox_conf_flatten']\n    inp_nodes = [inp.name[:inp.name.find(':')]]\n    np.random.seed(2701)\n    inputData = np.random.standard_normal([1, 3, 300, 300]).astype(np.float32)\n    cvNet.setInput(inputData)",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "min_sizes",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "min_sizes = [30, 60, 111, 162, 213, 264]\nmax_sizes = [60, 111, 162, 213, 264, 315]\nsteps = [8, 16, 32, 64, 100, 300]\naspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\nlayers = [conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h]\nfor i in range(6):\n    priorBox = NodeDef()\n    priorBox.name = 'PriorBox_%d' % i\n    priorBox.op = 'PriorBox'\n    priorBox.input.append(layers[i].name[:layers[i].name.find(':')])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "max_sizes",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "max_sizes = [60, 111, 162, 213, 264, 315]\nsteps = [8, 16, 32, 64, 100, 300]\naspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\nlayers = [conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h]\nfor i in range(6):\n    priorBox = NodeDef()\n    priorBox.name = 'PriorBox_%d' % i\n    priorBox.op = 'PriorBox'\n    priorBox.input.append(layers[i].name[:layers[i].name.find(':')])\n    priorBox.input.append(inp_nodes[0])  # data",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "steps",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "steps = [8, 16, 32, 64, 100, 300]\naspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\nlayers = [conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h]\nfor i in range(6):\n    priorBox = NodeDef()\n    priorBox.name = 'PriorBox_%d' % i\n    priorBox.op = 'PriorBox'\n    priorBox.input.append(layers[i].name[:layers[i].name.find(':')])\n    priorBox.input.append(inp_nodes[0])  # data\n    text_format.Merge('i: %d' % min_sizes[i], priorBox.attr[\"min_size\"])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "aspect_ratios",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\nlayers = [conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h]\nfor i in range(6):\n    priorBox = NodeDef()\n    priorBox.name = 'PriorBox_%d' % i\n    priorBox.op = 'PriorBox'\n    priorBox.input.append(layers[i].name[:layers[i].name.find(':')])\n    priorBox.input.append(inp_nodes[0])  # data\n    text_format.Merge('i: %d' % min_sizes[i], priorBox.attr[\"min_size\"])\n    text_format.Merge('i: %d' % max_sizes[i], priorBox.attr[\"max_size\"])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "layers = [conv4_3_norm, fc7, conv6_2_h, conv7_2_h, conv8_2_h, conv9_2_h]\nfor i in range(6):\n    priorBox = NodeDef()\n    priorBox.name = 'PriorBox_%d' % i\n    priorBox.op = 'PriorBox'\n    priorBox.input.append(layers[i].name[:layers[i].name.find(':')])\n    priorBox.input.append(inp_nodes[0])  # data\n    text_format.Merge('i: %d' % min_sizes[i], priorBox.attr[\"min_size\"])\n    text_format.Merge('i: %d' % max_sizes[i], priorBox.attr[\"max_size\"])\n    text_format.Merge('b: true', priorBox.attr[\"flip\"])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "concat",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "concat = NodeDef()\nconcat.name = 'mbox_priorbox'\nconcat.op = 'ConcatV2'\nfor i in range(6):\n    concat.input.append('PriorBox_%d' % i)\nconcat.input.append('mbox_loc/axis')\ngraph_def.node.extend([concat])\n# DetectionOutput layer\ndetectionOut = NodeDef()\ndetectionOut.name = 'detection_out'",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "concat.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "concat.name = 'mbox_priorbox'\nconcat.op = 'ConcatV2'\nfor i in range(6):\n    concat.input.append('PriorBox_%d' % i)\nconcat.input.append('mbox_loc/axis')\ngraph_def.node.extend([concat])\n# DetectionOutput layer\ndetectionOut = NodeDef()\ndetectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "concat.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "concat.op = 'ConcatV2'\nfor i in range(6):\n    concat.input.append('PriorBox_%d' % i)\nconcat.input.append('mbox_loc/axis')\ngraph_def.node.extend([concat])\n# DetectionOutput layer\ndetectionOut = NodeDef()\ndetectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('mbox_loc')",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "detectionOut",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "detectionOut = NodeDef()\ndetectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('mbox_loc')\ndetectionOut.input.append('mbox_conf_flatten')\ndetectionOut.input.append('mbox_priorbox')\ntext_format.Merge('i: 2', detectionOut.attr['num_classes'])\ntext_format.Merge('b: true', detectionOut.attr['share_location'])\ntext_format.Merge('i: 0', detectionOut.attr['background_label_id'])\ntext_format.Merge('f: 0.45', detectionOut.attr['nms_threshold'])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "detectionOut.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "detectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('mbox_loc')\ndetectionOut.input.append('mbox_conf_flatten')\ndetectionOut.input.append('mbox_priorbox')\ntext_format.Merge('i: 2', detectionOut.attr['num_classes'])\ntext_format.Merge('b: true', detectionOut.attr['share_location'])\ntext_format.Merge('i: 0', detectionOut.attr['background_label_id'])\ntext_format.Merge('f: 0.45', detectionOut.attr['nms_threshold'])\ntext_format.Merge('i: 400', detectionOut.attr['top_k'])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "detectionOut.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "detectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('mbox_loc')\ndetectionOut.input.append('mbox_conf_flatten')\ndetectionOut.input.append('mbox_priorbox')\ntext_format.Merge('i: 2', detectionOut.attr['num_classes'])\ntext_format.Merge('b: true', detectionOut.attr['share_location'])\ntext_format.Merge('i: 0', detectionOut.attr['background_label_id'])\ntext_format.Merge('f: 0.45', detectionOut.attr['nms_threshold'])\ntext_format.Merge('i: 400', detectionOut.attr['top_k'])\ntext_format.Merge('s: \"CENTER_SIZE\"', detectionOut.attr['code_type'])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "softmaxShape",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "softmaxShape = NodeDef()\nsoftmaxShape.name = 'reshape_before_softmax'\nsoftmaxShape.op = 'Const'\ntext_format.Merge(\n'tensor {'\n'  dtype: DT_INT32'\n'  tensor_shape { dim { size: 3 } }'\n'  int_val: 0'\n'  int_val: -1'\n'  int_val: 2'",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "softmaxShape.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "softmaxShape.name = 'reshape_before_softmax'\nsoftmaxShape.op = 'Const'\ntext_format.Merge(\n'tensor {'\n'  dtype: DT_INT32'\n'  tensor_shape { dim { size: 3 } }'\n'  int_val: 0'\n'  int_val: -1'\n'  int_val: 2'\n'}', softmaxShape.attr[\"value\"])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "softmaxShape.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "description": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "peekOfCode": "softmaxShape.op = 'Const'\ntext_format.Merge(\n'tensor {'\n'  dtype: DT_INT32'\n'  tensor_shape { dim { size: 3 } }'\n'  int_val: 0'\n'  int_val: -1'\n'  int_val: 2'\n'}', softmaxShape.attr[\"value\"])\ngraph_def.node.extend([softmaxShape])",
        "detail": "Hw_2.opencv.modules.dnn.misc.quantize_face_detector",
        "documentation": {}
    },
    {
        "label": "dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "dir = \"./\"\nlicense_decl = \\\n'// This file is part of OpenCV project.\\n'\\\n'// It is subject to the license terms in the LICENSE file found in the top-level directory\\n'\\\n'// of this distribution and at http://opencv.org/license.html.\\n'\\\n'//\\n'\\\n'// Copyright (C) 2018, Intel Corporation, all rights reserved.\\n'\\\n'// Third party copyrights are property of their respective owners.\\n\\n'\nprecomp = '#include \\\"../../precomp.hpp\\\"\\n'\nns_head = '\\nnamespace cv { namespace dnn { namespace vkcom {\\n\\n'",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "license_decl",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "license_decl = \\\n'// This file is part of OpenCV project.\\n'\\\n'// It is subject to the license terms in the LICENSE file found in the top-level directory\\n'\\\n'// of this distribution and at http://opencv.org/license.html.\\n'\\\n'//\\n'\\\n'// Copyright (C) 2018, Intel Corporation, all rights reserved.\\n'\\\n'// Third party copyrights are property of their respective owners.\\n\\n'\nprecomp = '#include \\\"../../precomp.hpp\\\"\\n'\nns_head = '\\nnamespace cv { namespace dnn { namespace vkcom {\\n\\n'\nns_tail = '\\n}}} // namespace cv::dnn::vkcom\\n'",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "precomp",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "precomp = '#include \\\"../../precomp.hpp\\\"\\n'\nns_head = '\\nnamespace cv { namespace dnn { namespace vkcom {\\n\\n'\nns_tail = '\\n}}} // namespace cv::dnn::vkcom\\n'\nheadfile = open('spv_shader.hpp', 'w')\nheadfile.write(license_decl)\nheadfile.write('#ifndef OPENCV_DNN_SPV_SHADER_HPP\\n')\nheadfile.write('#define OPENCV_DNN_SPV_SHADER_HPP\\n\\n')\nheadfile.write(ns_head)\ncmd_remove = ''\nnull_out = ''",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "ns_head",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "ns_head = '\\nnamespace cv { namespace dnn { namespace vkcom {\\n\\n'\nns_tail = '\\n}}} // namespace cv::dnn::vkcom\\n'\nheadfile = open('spv_shader.hpp', 'w')\nheadfile.write(license_decl)\nheadfile.write('#ifndef OPENCV_DNN_SPV_SHADER_HPP\\n')\nheadfile.write('#define OPENCV_DNN_SPV_SHADER_HPP\\n\\n')\nheadfile.write(ns_head)\ncmd_remove = ''\nnull_out = ''\nif sys.platform.find('win32') != -1:",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "ns_tail",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "ns_tail = '\\n}}} // namespace cv::dnn::vkcom\\n'\nheadfile = open('spv_shader.hpp', 'w')\nheadfile.write(license_decl)\nheadfile.write('#ifndef OPENCV_DNN_SPV_SHADER_HPP\\n')\nheadfile.write('#define OPENCV_DNN_SPV_SHADER_HPP\\n\\n')\nheadfile.write(ns_head)\ncmd_remove = ''\nnull_out = ''\nif sys.platform.find('win32') != -1:\n    cmd_remove = 'del'",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "headfile",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "headfile = open('spv_shader.hpp', 'w')\nheadfile.write(license_decl)\nheadfile.write('#ifndef OPENCV_DNN_SPV_SHADER_HPP\\n')\nheadfile.write('#define OPENCV_DNN_SPV_SHADER_HPP\\n\\n')\nheadfile.write(ns_head)\ncmd_remove = ''\nnull_out = ''\nif sys.platform.find('win32') != -1:\n    cmd_remove = 'del'\n    null_out = ' >>nul 2>nul'",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "cmd_remove",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "cmd_remove = ''\nnull_out = ''\nif sys.platform.find('win32') != -1:\n    cmd_remove = 'del'\n    null_out = ' >>nul 2>nul'\nelif sys.platform.find('linux') != -1:\n    cmd_remove = 'rm'\n    null_out = ' > /dev/null 2>&1'\nlist = os.listdir(dir)\nfor i in range(0, len(list)):",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "null_out",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "null_out = ''\nif sys.platform.find('win32') != -1:\n    cmd_remove = 'del'\n    null_out = ' >>nul 2>nul'\nelif sys.platform.find('linux') != -1:\n    cmd_remove = 'rm'\n    null_out = ' > /dev/null 2>&1'\nlist = os.listdir(dir)\nfor i in range(0, len(list)):\n    if (os.path.splitext(list[i])[-1] != '.comp'):",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "description": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "peekOfCode": "list = os.listdir(dir)\nfor i in range(0, len(list)):\n    if (os.path.splitext(list[i])[-1] != '.comp'):\n        continue\n    prefix = os.path.splitext(list[i])[0];\n    path = os.path.join(dir, list[i])\n    bin_file = prefix + '.tmp'\n    cmd = ' glslangValidator -V ' + path + ' -S comp -o ' + bin_file\n    print('compiling')\n    if os.system(cmd) != 0:",
        "detail": "Hw_2.opencv.modules.dnn.src.vkcom.shader.spirv_generator",
        "documentation": {}
    },
    {
        "label": "NormalizePreproc",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "description": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "peekOfCode": "class NormalizePreproc:\n    def __init__(self):\n        pass\n    @staticmethod\n    def process(img):\n        image_data = np.array(img).transpose(2, 0, 1).astype(np.float32)\n        image_data = np.expand_dims(image_data, 0)\n        image_data /= 255.0\n        return image_data\nclass CityscapesDataFetch(DatasetImageFetch):",
        "detail": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "documentation": {}
    },
    {
        "label": "CityscapesDataFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "description": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "peekOfCode": "class CityscapesDataFetch(DatasetImageFetch):\n    img_dir = ''\n    segm_dir = ''\n    segm_files = []\n    colors = []\n    i = 0\n    def __init__(self, img_dir, segm_dir, preproc):\n        self.img_dir = img_dir\n        self.segm_dir = segm_dir\n        self.segm_files = sorted([img for img in self.locate('*_color.png', segm_dir)])",
        "detail": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "documentation": {}
    },
    {
        "label": "TorchModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "description": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "peekOfCode": "class TorchModel(Framework):\n    net = object\n    def __init__(self, model_file):\n        self.net = load_lua(model_file)\n    def get_name(self):\n        return 'Torch'\n    def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        out = self.net.forward(tensor).numpy()\n        return out",
        "detail": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "documentation": {}
    },
    {
        "label": "DnnTorchModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "description": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "peekOfCode": "class DnnTorchModel(DnnCaffeModel):\n    net = cv.dnn.Net()\n    def __init__(self, model_file):\n        self.net = cv.dnn.readNetFromTorch(model_file)\n    def get_output(self, input_blob):\n        self.net.setBlob(\"\", input_blob)\n        self.net.forward()\n        return self.net.getBlob(self.net.getLayerNames()[-1])\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()",
        "detail": "Hw_2.opencv.modules.dnn.test.cityscapes_semsegm_test_enet",
        "documentation": {}
    },
    {
        "label": "DataFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class DataFetch(object):\n    imgs_dir = ''\n    frame_size = 0\n    bgr_to_rgb = False\n    __metaclass__ = ABCMeta\n    @abstractmethod\n    def preprocess(self, img):\n        pass\n    def get_batch(self, imgs_names):\n        assert type(imgs_names) is list",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "MeanBlobFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class MeanBlobFetch(DataFetch):\n    mean_blob = np.ndarray(())\n    def __init__(self, frame_size, mean_blob_path, imgs_dir):\n        self.imgs_dir = imgs_dir\n        self.frame_size = frame_size\n        blob = caffe.proto.caffe_pb2.BlobProto()\n        data = open(mean_blob_path, 'rb').read()\n        blob.ParseFromString(data)\n        self.mean_blob = np.array(caffe.io.blobproto_to_array(blob))\n        start = (self.mean_blob.shape[2] - self.frame_size) / 2",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "MeanChannelsFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class MeanChannelsFetch(MeanBlobFetch):\n    def __init__(self, frame_size, imgs_dir):\n        self.imgs_dir = imgs_dir\n        self.frame_size = frame_size\n        self.mean_blob = np.ones((3, self.frame_size, self.frame_size)).astype(np.float32)\n        self.mean_blob[0] *= 104\n        self.mean_blob[1] *= 117\n        self.mean_blob[2] *= 123\nclass MeanValueFetch(MeanBlobFetch):\n    def __init__(self, frame_size, imgs_dir, bgr_to_rgb):",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "MeanValueFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class MeanValueFetch(MeanBlobFetch):\n    def __init__(self, frame_size, imgs_dir, bgr_to_rgb):\n        self.imgs_dir = imgs_dir\n        self.frame_size = frame_size\n        self.mean_blob = np.ones((3, self.frame_size, self.frame_size)).astype(np.float32)\n        self.mean_blob *= 117\n        self.bgr_to_rgb = bgr_to_rgb\ndef get_correct_answers(img_list, img_classes, net_output_blob):\n    correct_answers = 0\n    for i in range(len(img_list)):",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "Framework",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class Framework(object):\n    in_blob_name = ''\n    out_blob_name = ''\n    __metaclass__ = ABCMeta\n    @abstractmethod\n    def get_name(self):\n        pass\n    @abstractmethod\n    def get_output(self, input_blob):\n        pass",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "CaffeModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class CaffeModel(Framework):\n    net = caffe.Net\n    need_reshape = False\n    def __init__(self, prototxt, caffemodel, in_blob_name, out_blob_name, need_reshape=False):\n        caffe.set_mode_cpu()\n        self.net = caffe.Net(prototxt, caffemodel, caffe.TEST)\n        self.in_blob_name = in_blob_name\n        self.out_blob_name = out_blob_name\n        self.need_reshape = need_reshape\n    def get_name(self):",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "DnnCaffeModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class DnnCaffeModel(Framework):\n    net = object\n    def __init__(self, prototxt, caffemodel, in_blob_name, out_blob_name):\n        self.net = cv.dnn.readNetFromCaffe(prototxt, caffemodel)\n        self.in_blob_name = in_blob_name\n        self.out_blob_name = out_blob_name\n    def get_name(self):\n        return 'DNN'\n    def get_output(self, input_blob):\n        self.net.setInput(input_blob, self.in_blob_name)",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "ClsAccEvaluation",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "class ClsAccEvaluation:\n    log = sys.stdout\n    img_classes = {}\n    batch_size = 0\n    def __init__(self, log_path, img_classes_file, batch_size):\n        self.log = open(log_path, 'w')\n        self.img_classes = self.read_classes(img_classes_file)\n        self.batch_size = batch_size\n    @staticmethod\n    def read_classes(img_classes_file):",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "get_correct_answers",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "peekOfCode": "def get_correct_answers(img_list, img_classes, net_output_blob):\n    correct_answers = 0\n    for i in range(len(img_list)):\n        indexes = np.argsort(net_output_blob[i])[-5:]\n        correct_index = img_classes[img_list[i]]\n        if correct_index in indexes:\n            correct_answers += 1\n    return correct_answers\nclass Framework(object):\n    in_blob_name = ''",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_alexnet",
        "documentation": {}
    },
    {
        "label": "TensorflowModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "peekOfCode": "class TensorflowModel(Framework):\n    sess = tf.Session\n    output = tf.Graph\n    def __init__(self, model_file, in_blob_name, out_blob_name):\n        self.in_blob_name = in_blob_name\n        self.sess = tf.Session()\n        with gfile.FastGFile(model_file, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            self.sess.graph.as_default()",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "documentation": {}
    },
    {
        "label": "DnnTfInceptionModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "description": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "peekOfCode": "class DnnTfInceptionModel(DnnCaffeModel):\n    net = cv.dnn.Net()\n    def __init__(self, model_file, in_blob_name, out_blob_name):\n        self.net = cv.dnn.readNetFromTensorflow(model_file)\n        self.in_blob_name = in_blob_name\n        self.out_blob_name = out_blob_name\n    def get_output(self, input_blob):\n        return super(DnnTfInceptionModel, self).get_output(input_blob)[..., 1:1001]\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()",
        "detail": "Hw_2.opencv.modules.dnn.test.imagenet_cls_test_inception",
        "documentation": {}
    },
    {
        "label": "MeanChannelsPreproc",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "class MeanChannelsPreproc:\n    def __init__(self):\n        pass\n    @staticmethod\n    def process(img):\n        image_data = np.array(img).transpose(2, 0, 1).astype(np.float32)\n        mean = np.ones(image_data.shape)\n        mean[0] *= 104\n        mean[1] *= 117\n        mean[2] *= 123",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "DatasetImageFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "class DatasetImageFetch(object):\n    __metaclass__ = ABCMeta\n    data_prepoc = object\n    @abstractmethod\n    def __iter__(self):\n        pass\n    @abstractmethod\n    def next(self):\n        pass\n    @staticmethod",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "PASCALDataFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "class PASCALDataFetch(DatasetImageFetch):\n    img_dir = ''\n    segm_dir = ''\n    names = []\n    colors = []\n    i = 0\n    def __init__(self, img_dir, segm_dir, names_file, segm_cls_colors_file, preproc):\n        self.img_dir = img_dir\n        self.segm_dir = segm_dir\n        self.colors = self.read_colors(segm_cls_colors_file)",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "SemSegmEvaluation",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "class SemSegmEvaluation:\n    log = sys.stdout\n    def __init__(self, log_path,):\n        self.log = open(log_path, 'w')\n    def process(self, frameworks, data_fetcher):\n        samples_handled = 0\n        conf_mats = [np.zeros((data_fetcher.get_num_classes(), data_fetcher.get_num_classes())) for i in range(len(frameworks))]\n        blobs_l1_diff = [0] * len(frameworks)\n        blobs_l1_diff_count = [0] * len(frameworks)\n        blobs_l_inf_diff = [sys.float_info.min] * len(frameworks)",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "get_metrics",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "def get_metrics(conf_mat):\n    pix_accuracy = np.trace(conf_mat) / np.sum(conf_mat)\n    t = np.sum(conf_mat, 1)\n    num_cl = np.count_nonzero(t)\n    assert num_cl\n    mean_accuracy = np.sum(np.nan_to_num(np.divide(np.diagonal(conf_mat), t))) / num_cl\n    col_sum = np.sum(conf_mat, 0)\n    mean_iou = np.sum(\n        np.nan_to_num(np.divide(np.diagonal(conf_mat), (t + col_sum - np.diagonal(conf_mat))))) / num_cl\n    return pix_accuracy, mean_accuracy, mean_iou",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "eval_segm_result",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "def eval_segm_result(net_out):\n    assert type(net_out) is np.ndarray\n    assert len(net_out.shape) == 4\n    channels_dim = 1\n    y_dim = channels_dim + 1\n    x_dim = y_dim + 1\n    res = np.zeros(net_out.shape).astype(int)\n    for i in range(net_out.shape[y_dim]):\n        for j in range(net_out.shape[x_dim]):\n            max_ch = np.argmax(net_out[..., i, j])",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "get_conf_mat",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "description": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "peekOfCode": "def get_conf_mat(gt, prob):\n    assert type(gt) is np.ndarray\n    assert type(prob) is np.ndarray\n    conf_mat = np.zeros((gt.shape[0], gt.shape[0]))\n    for ch_gt in range(conf_mat.shape[0]):\n        gt_channel = gt[ch_gt, ...]\n        for ch_pr in range(conf_mat.shape[1]):\n            prob_channel = prob[ch_pr, ...]\n            conf_mat[ch_gt][ch_pr] = np.count_nonzero(np.multiply(gt_channel, prob_channel))\n    return conf_mat",
        "detail": "Hw_2.opencv.modules.dnn.test.pascal_semsegm_test_fcn",
        "documentation": {}
    },
    {
        "label": "feature_homography_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "class feature_homography_test(NewOpenCVTests):\n    render = None\n    tracker = None\n    framesCounter = 0\n    frame = None\n    def test_feature_homography(self):\n        self.render = TestSceneRender(self.get_sample('samples/data/graf1.png'),\n            self.get_sample('samples/data/box.png'), noise = 0.5, speed = 0.5)\n        self.frame = self.render.getNextFrame()\n        self.tracker = PlaneTracker()",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "PlaneTracker",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "class PlaneTracker:\n    def __init__(self):\n        self.detector = cv.AKAZE_create(threshold = 0.003)\n        self.matcher = cv.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n        self.targets = []\n        self.frame_points = []\n    def add_target(self, image, rect, data=None):\n        '''Add a new tracking target.'''\n        x0, y0, x1, y1 = rect\n        raw_points, raw_descrs = self.detect_features(image)",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))\nfrom tests_common import NewOpenCVTests\nclass feature_homography_test(NewOpenCVTests):\n    render = None\n    tracker = None\n    framesCounter = 0",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\n# local modules\nfrom tst_scene_render import TestSceneRender\ndef intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, np.array(s2))\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(np.array(s2)))",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "FLANN_INDEX_KDTREE",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "FLANN_INDEX_KDTREE = 1\nFLANN_INDEX_LSH    = 6\nflann_params= dict(algorithm = FLANN_INDEX_LSH,\n                   table_number = 6, # 12\n                   key_size = 12,     # 20\n                   multi_probe_level = 1) #2\nMIN_MATCH_COUNT = 10\n'''\n  image     - image to track\n  rect      - tracked rectangle (x1, y1, x2, y2)",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "MIN_MATCH_COUNT",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "MIN_MATCH_COUNT = 10\n'''\n  image     - image to track\n  rect      - tracked rectangle (x1, y1, x2, y2)\n  keypoints - keypoints detected inside rect\n  descrs    - their descriptors\n  data      - some user-provided data\n'''\nPlanarTarget = namedtuple('PlaneTarget', 'image, rect, keypoints, descrs, data')\n'''",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "PlanarTarget",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "PlanarTarget = namedtuple('PlaneTarget', 'image, rect, keypoints, descrs, data')\n'''\n  target - reference to PlanarTarget\n  p0     - matched points coords in target image\n  p1     - matched points coords in input frame\n  H      - homography matrix from p0 to p1\n  quad   - target boundary quad in input frame\n'''\nTrackedTarget = namedtuple('TrackedTarget', 'target, p0, p1, H, quad')\nclass PlaneTracker:",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "TrackedTarget",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "description": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "peekOfCode": "TrackedTarget = namedtuple('TrackedTarget', 'target, p0, p1, H, quad')\nclass PlaneTracker:\n    def __init__(self):\n        self.detector = cv.AKAZE_create(threshold = 0.003)\n        self.matcher = cv.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n        self.targets = []\n        self.frame_points = []\n    def add_target(self, image, rect, data=None):\n        '''Add a new tracking target.'''\n        x0, y0, x1, y1 = rect",
        "detail": "Hw_2.opencv.modules.features2d.misc.python.test.test_feature_homography",
        "documentation": {}
    },
    {
        "label": "GProcessPoses",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GProcessPoses:\n    @staticmethod\n    def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc()\n@cv.gapi.op('custom.GParseEyes',\n            in_types=[cv.GArray.GMat, cv.GArray.Rect, cv.GOpaque.Size],\n            out_types=[cv.GArray.Rect, cv.GArray.Rect, cv.GArray.Point, cv.GArray.Point])\nclass GParseEyes:\n    @staticmethod\n    def outMeta(arr_desc0, arr_desc1, arr_desc2):",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "GParseEyes",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GParseEyes:\n    @staticmethod\n    def outMeta(arr_desc0, arr_desc1, arr_desc2):\n        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n               cv.empty_array_desc(), cv.empty_array_desc()\n@cv.gapi.op('custom.GGetStates',\n            in_types=[cv.GArray.GMat, cv.GArray.GMat],\n            out_types=[cv.GArray.Int, cv.GArray.Int])\nclass GGetStates:\n    @staticmethod",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "GGetStates",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GGetStates:\n    @staticmethod\n    def outMeta(arr_desc0, arr_desc1):\n        return cv.empty_array_desc(), cv.empty_array_desc()\n# ------------------------Custom kernels------------------------\n@cv.gapi.kernel(GProcessPoses)\nclass GProcessPosesImpl:\n    \"\"\" Custom kernel. Processed poses of heads\n    \"\"\"\n    @staticmethod",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "GProcessPosesImpl",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GProcessPosesImpl:\n    \"\"\" Custom kernel. Processed poses of heads\n    \"\"\"\n    @staticmethod\n    def run(in_ys, in_ps, in_rs):\n        \"\"\" ustom kernel executable code\n        Params:\n        in_ys: yaw angle of head\n        in_ps: pitch angle of head\n        in_rs: roll angle of head",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "GParseEyesImpl",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GParseEyesImpl:\n    \"\"\" Custom kernel. Get information about eyes\n    \"\"\"\n    @staticmethod\n    def run(in_landm_per_face, in_face_rcs, frame_size):\n        \"\"\" ustom kernel executable code\n        Params:\n        in_landm_per_face: landmarks from inference of facial-landmarks network for each face\n        in_face_rcs: bounding boxes for each face\n        frame_size: size of input image",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "GGetStatesImpl",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "class GGetStatesImpl:\n    \"\"\" Custom kernel. Get state of eye - open or closed\n    \"\"\"\n    @staticmethod\n    def run(eyesl, eyesr):\n        \"\"\" ustom kernel executable code\n        Params:\n        eyesl: result of inference of open-closed-eye network for left eye\n        eyesr: result of inference of open-closed-eye network for right eye\n        Return:",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "weight_path",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "def weight_path(model_path):\n    \"\"\" Get path of weights based on path to IR\n    Params:\n    model_path: the string contains path to IR file\n    Return:\n    Path to weights file\n    \"\"\"\n    assert model_path.endswith('.xml'), \"Wrong topology path was provided\"\n    return model_path[:-3] + 'bin'\ndef build_argparser():",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "build_argparser",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "def build_argparser():\n    \"\"\" Parse arguments from command line\n    Return:\n    Pack of arguments from command line\n    \"\"\"\n    parser = argparse.ArgumentParser(description='This is an OpenCV-based version of Gaze Estimation example')\n    parser.add_argument('--input',\n                        help='Path to the input video file or camera device number')\n    parser.add_argument('--out',\n                        help='Path to the output video file')",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "def intersection(surface, rect):\n    \"\"\" Remove zone of out of bound from ROI\n    Params:\n    surface: image bounds is rect representation (top left coordinates and width and height)\n    rect: region of interest is also has rect representation\n    Return:\n    Modified ROI with correct bounds\n    \"\"\"\n    l_x = max(surface[0], rect[0])\n    l_y = max(surface[1], rect[1])",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "process_landmarks",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "def process_landmarks(r_x, r_y, r_w, r_h, landmarks):\n    \"\"\" Create points from result of inference of facial-landmarks network and size of input image\n    Params:\n    r_x: x coordinate of top left corner of input image\n    r_y: y coordinate of top left corner of input image\n    r_w: width of input image\n    r_h: height of input image\n    landmarks: result of inference of facial-landmarks network\n    Return:\n    Array of landmarks points for one face",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "eye_box",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "description": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "peekOfCode": "def eye_box(p_1, p_2, scale=1.8):\n    \"\"\" Get bounding box of eye\n    Params:\n    p_1: point of left edge of eye\n    p_2: point of right edge of eye\n    scale: change size of box with this value\n    Return:\n    Bounding box of eye and its midpoint\n    \"\"\"\n    size = np.linalg.norm(p_1 - p_2)",
        "detail": "Hw_2.opencv.modules.gapi.misc.python.samples.gaze_estimation",
        "documentation": {}
    },
    {
        "label": "get_output",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def get_output(exec_str):\n    try:\n        out = subprocess.check_output(exec_str,\n                                      stderr=subprocess.STDOUT,\n                                      shell=True).strip().decode()\n    except subprocess.CalledProcessError as exc:\n        out = exc.output.strip().decode()\n    return out\ndef test_error_no_config_specified():\n    out = get_output(pipeline_modeling_tool)",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_config_specified",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_config_specified():\n    out = get_output(pipeline_modeling_tool)\n    assert out.startswith('Config must be specified via --cfg option')\ndef test_error_no_config_exists():\n    cfg_file = 'not_existing_cfg.yml'\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert 'Failed to open config file: not_existing_cfg.yml' in out\ndef test_error_work_time_not_positive():\n    cfg_file = \"\"\"\\\"%YAML:1.0",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_config_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_config_exists():\n    cfg_file = 'not_existing_cfg.yml'\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert 'Failed to open config file: not_existing_cfg.yml' in out\ndef test_error_work_time_not_positive():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: -1\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_work_time_not_positive",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_work_time_not_positive():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: -1\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('work_time must be positive')\ndef test_error_no_pipelines():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_pipelines",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_pipelines():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('Config must contain field: Pipelines')\ndef test_error_pipelines_node_not_map():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\\\" \"\"\"",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_pipelines_node_not_map",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_pipelines_node_not_map():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('Pipelines field must be a map')\ndef test_error_config_not_contain_pl():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_config_not_contain_pl",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_config_not_contain_pl():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\\\" \"\"\"\n    exec_str = '{} --cfg={} --exec_list=PL2'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('Pipelines must contain field: PL2')\ndef test_error_no_source():\n    cfg_file = \"\"\"\\\"%YAML:1.0",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_source",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_source():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    queue_capacity: 1\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('PL1 must contain field: source')\ndef test_error_source_no_name():",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_source_no_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_source_no_name():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('source must contain field: name')\ndef test_error_source_no_latency():",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_source_no_latency",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_source_no_latency():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)\n    assert out.startswith('source must contain field: latency')",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_source_no_output",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_source_no_output():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)\n    out = get_output(exec_str)",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_source_output_no_dims",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_source_output_no_dims():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\\\" \"\"\"\n    exec_str = '{} --cfg={}'.format(pipeline_modeling_tool, cfg_file)",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_source_output_no_precision",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_source_output_no_precision():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]\\\" \"\"\"",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_nodes",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_nodes():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_nodes_not_sequence",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_nodes_not_sequence():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_no_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_no_name():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_no_type",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_no_type():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_unknown_type",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_unknown_type():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_dummy_no_time",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_dummy_no_time():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_dummy_not_positive_time",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_dummy_not_positive_time():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_dummy_no_output",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_dummy_no_output():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_infer_no_model_path",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_infer_no_model_path():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_infer_no_input_layers",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_infer_no_input_layers():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_infer_input_layers_are_empty",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_infer_input_layers_are_empty():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_infer_no_output_layers",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_infer_no_output_layers():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_node_infer_output_layers_are_empty",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_node_infer_output_layers_are_empty():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_edges",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_edges():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_not_sequence",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_not_sequence():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_no_from",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_no_from():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_no_to",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_no_to():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_from_not_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_from_not_exists():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_from_port_not_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_from_port_not_exists():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_to_not_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_to_not_exists():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_edges_to_port_not_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_edges_to_port_not_exists():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_connect_to_source",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_connect_to_source():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_double_edge",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_double_edge():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_double_edge",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_double_edge():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_node_has_dangling_input",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_node_has_dangling_input():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_has_cycle_0",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_has_cycle_0():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_has_cycle_0",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_has_cycle_0():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_load_config_exists",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_load_config_exists():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_invalid_app_mode",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_invalid_app_mode():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_invalid_pl_mode",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_invalid_pl_mode():\n  cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_drop_frames_with_streaming",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_drop_frames_with_streaming():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_incorrect_call_every_nth",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_incorrect_call_every_nth():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nwork_time: 1000\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,2,3,4]",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "test_error_no_worktime_and_num_iters",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "def test_error_no_worktime_and_num_iters():\n    cfg_file = \"\"\"\\\"%YAML:1.0\nPipelines:\n  PL1:\n    source:\n      name: 'Src'\n      latency: 20\n      output:\n        dims: [1,1]\n        precision: 'U8'",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "pipeline_modeling_tool",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "description": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "peekOfCode": "pipeline_modeling_tool = os.getenv('PIPELINE_MODELING_TOOL')\ndef get_output(exec_str):\n    try:\n        out = subprocess.check_output(exec_str,\n                                      stderr=subprocess.STDOUT,\n                                      shell=True).strip().decode()\n    except subprocess.CalledProcessError as exc:\n        out = exc.output.strip().decode()\n    return out\ndef test_error_no_config_specified():",
        "detail": "Hw_2.opencv.modules.gapi.samples.pipeline_modeling_tool.test_pipeline_modeling_tool",
        "documentation": {}
    },
    {
        "label": "Policy",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "class Policy(Enum):\n    Traditional = 1\n    Streaming   = 2\n# From mode to cmd arg\nmods = [ (Policy.Traditional, True)\n       , (Policy.Streaming,  False)\n       ]\nclass UI(Enum):\n    With    = 1\n    Without = 2",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "UI",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "class UI(Enum):\n    With    = 1\n    Without = 2\n# From mode to cmd arg\nui = [ (UI.With,   False)\n     , (UI.Without, True)\n     ]\nfd_fmt_bin  = lambda prec : fmt_bin(intel_models_path, prec, intel_fd_model)\nlpd_fmt_bin = lambda prec : fmt_bin(intel_models_path, prec, intel_lpd_model)\n# Performance comparison table",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "fmt_bool",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "def fmt_bool(x):\n    return (\"true\" if x else \"false\")\ndef fmt_bin(base, prec, model):\n    return \"%s/%s/%s/%s.xml\" % (base, model, prec, model)\n## The script itself #################################################\n##\nif len(sys.argv) != 3:\n    print(\"Usage: %s /path/to/input/video /path/to/models\" % sys.argv[0])\n    exit(1)\ninput_file_path   = sys.argv[1]",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "fmt_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "def fmt_bin(base, prec, model):\n    return \"%s/%s/%s/%s.xml\" % (base, model, prec, model)\n## The script itself #################################################\n##\nif len(sys.argv) != 3:\n    print(\"Usage: %s /path/to/input/video /path/to/models\" % sys.argv[0])\n    exit(1)\ninput_file_path   = sys.argv[1]\nintel_models_path = sys.argv[2]\napp             = \"bin/example_gapi_privacy_masking_camera\"",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "intel_models_path",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "intel_models_path = sys.argv[2]\napp             = \"bin/example_gapi_privacy_masking_camera\"\nintel_fd_model  = \"face-detection-retail-0005\"\nintel_lpd_model = \"vehicle-license-plate-detection-barrier-0106\"\noutput_file     = \"out_results.csv\"\ntgts = [ (\"CPU\", \"INT8\")\n       , (\"CPU\", \"FP32\")\n       , (\"GPU\", \"FP16\")\n       ]\nclass Policy(Enum):",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "intel_lpd_model",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "intel_lpd_model = \"vehicle-license-plate-detection-barrier-0106\"\noutput_file     = \"out_results.csv\"\ntgts = [ (\"CPU\", \"INT8\")\n       , (\"CPU\", \"FP32\")\n       , (\"GPU\", \"FP16\")\n       ]\nclass Policy(Enum):\n    Traditional = 1\n    Streaming   = 2\n# From mode to cmd arg",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "tgts",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "tgts = [ (\"CPU\", \"INT8\")\n       , (\"CPU\", \"FP32\")\n       , (\"GPU\", \"FP16\")\n       ]\nclass Policy(Enum):\n    Traditional = 1\n    Streaming   = 2\n# From mode to cmd arg\nmods = [ (Policy.Traditional, True)\n       , (Policy.Streaming,  False)",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "mods",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "mods = [ (Policy.Traditional, True)\n       , (Policy.Streaming,  False)\n       ]\nclass UI(Enum):\n    With    = 1\n    Without = 2\n# From mode to cmd arg\nui = [ (UI.With,   False)\n     , (UI.Without, True)\n     ]",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "ui",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "ui = [ (UI.With,   False)\n     , (UI.Without, True)\n     ]\nfd_fmt_bin  = lambda prec : fmt_bin(intel_models_path, prec, intel_fd_model)\nlpd_fmt_bin = lambda prec : fmt_bin(intel_models_path, prec, intel_lpd_model)\n# Performance comparison table\ntable={}\n# Collect the performance data\nfor m in mods:              # Execution mode (trad/stream)\n    for u in ui:            # UI mode (on/off)",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "lpd_fmt_bin",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "description": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "peekOfCode": "lpd_fmt_bin = lambda prec : fmt_bin(intel_models_path, prec, intel_lpd_model)\n# Performance comparison table\ntable={}\n# Collect the performance data\nfor m in mods:              # Execution mode (trad/stream)\n    for u in ui:            # UI mode (on/off)\n        for f in tgts:      # FD model\n            for p in tgts:  # LPD model\n                cmd = [ app\n                      , (\"--input=%s\"  % input_file_path)   # input file",
        "detail": "Hw_2.opencv.modules.gapi.scripts.measure_privacy_masking",
        "documentation": {}
    },
    {
        "label": "GeneralInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.parent_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        self.cname = get_cname(self.symbol_id)\n        # parse doxygen comments\n        self.params={}\n        self.annotation=[]\n        if type == \"class\":\n            docstring=\"// C++: class \" + self.name + \"\\n\"\n        else:",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ConstInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class ConstInfo(GeneralInfo):\n    def __init__(self, decl, addedManually=False, namespaces=[], enumType=None):\n        GeneralInfo.__init__(self, \"const\", decl, namespaces)\n        self.value = decl[1]\n        self.enumType = enumType\n        self.addedManually = addedManually\n        if self.namespace in namespaces_dict:\n            prefix = namespaces_dict[self.namespace]\n            if prefix:\n                self.name = '%s_%s' % (prefix, self.name)",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ClassPropInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class ClassPropInfo():\n    def __init__(self, decl): # [f_ctype, f_name, '', '/RW']\n        self.ctype = decl[0]\n        self.name = decl[1]\n        self.rw = \"/RW\" in decl[3]\n    def __repr__(self):\n        return Template(\"PROP $ctype $name\").substitute(ctype=self.ctype, name=self.name)\nclass ClassInfo(GeneralInfo):\n    def __init__(self, decl, namespaces=[]): # [ 'class/struct cname', ': base', [modlist] ]\n        GeneralInfo.__init__(self, \"class\", decl, namespaces)",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ClassInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class ClassInfo(GeneralInfo):\n    def __init__(self, decl, namespaces=[]): # [ 'class/struct cname', ': base', [modlist] ]\n        GeneralInfo.__init__(self, \"class\", decl, namespaces)\n        self.methods = []\n        self.methods_suffixes = {}\n        self.consts = [] # using a list to save the occurrence order\n        self.private_consts = []\n        self.imports = set()\n        self.props= []\n        self.jname = self.name",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ArgInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class ArgInfo():\n    def __init__(self, arg_tuple): # [ ctype, name, def val, [mod], argno ]\n        self.pointer = False\n        ctype = arg_tuple[0]\n        if ctype.endswith(\"*\"):\n            ctype = ctype[:-1]\n            self.pointer = True\n        self.ctype = ctype\n        self.name = arg_tuple[1]\n        self.defval = arg_tuple[2]",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "FuncInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class FuncInfo(GeneralInfo):\n    def __init__(self, decl, namespaces=[]): # [ funcname, return_ctype, [modifiers], [args] ]\n        GeneralInfo.__init__(self, \"func\", decl, namespaces)\n        self.cname = get_cname(decl[0])\n        self.jname = self.name\n        self.isconstructor = self.name == self.classname\n        if \"[\" in self.name:\n            self.jname = \"getelem\"\n        for m in decl[2]:\n            if m.startswith(\"=\"):  # alias from WRAP_AS",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "JavaWrapperGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class JavaWrapperGenerator(object):\n    def __init__(self):\n        self.cpp_files = []\n        self.clear()\n    def clear(self):\n        self.namespaces = [\"cv\"]\n        classinfo_Mat = ClassInfo([ 'class cv.Mat', '', ['/Simple'], [] ], self.namespaces)\n        self.classes = { \"Mat\" : classinfo_Mat }\n        self.module = \"\"\n        self.Module = \"\"",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "checkFileRemap",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def checkFileRemap(path):\n    path = os.path.realpath(path)\n    if path in FILES_REMAP:\n        return FILES_REMAP[path]\n    assert path[-3:] != '.in', path\n    return path\ntotal_files = 0\nupdated_files = 0\nmodule_imports = []\nmodule_j_code = None",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "read_contents",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):\n    ''' mkdir -p '''\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "mkdir_p",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def mkdir_p(path):\n    ''' mkdir -p '''\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\nT_JAVA_START_INHERITED = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_class_inherited.prolog'))",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "normalize_field_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def normalize_field_name(name):\n    return name.replace(\".\",\"_\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"_getNativeObjAddr()\",\"_nativeObj\")\ndef normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "normalize_class_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "get_cname",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "cast_from",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t\nclass ClassPropInfo():\n    def __init__(self, decl): # [f_ctype, f_name, '', '/RW']",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "cast_to",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t\nclass ClassPropInfo():\n    def __init__(self, decl): # [f_ctype, f_name, '', '/RW']\n        self.ctype = decl[0]\n        self.name = decl[1]\n        self.rw = \"/RW\" in decl[3]\n    def __repr__(self):",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "copy_java_files",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def copy_java_files(java_files_dir, java_base_path, default_package_path='org/opencv/'):\n    global total_files, updated_files\n    java_files = []\n    re_filter = re.compile(r'^.+\\.(java|aidl|kt)(.in)?$')\n    for root, dirnames, filenames in os.walk(java_files_dir):\n       java_files += [os.path.join(root, filename) for filename in filenames if re_filter.match(filename)]\n    java_files = [f.replace('\\\\', '/') for f in java_files]\n    re_package = re.compile(r'^package +(.+);')\n    re_prefix = re.compile(r'^.+[\\+/]([^\\+]+).(java|aidl|kt)(.in)?$')\n    for java_file in java_files:",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "sanitize_java_documentation_string",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "def sanitize_java_documentation_string(doc, type):\n    if type == \"class\":\n        doc = doc.replace(\"@param \", \"\")\n    doc = re.sub(re.compile('\\\\\\\\f\\\\$(.*?)\\\\\\\\f\\\\$', re.DOTALL), '\\\\(' + r'\\1' + '\\\\)', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\[(.*?)\\\\\\\\f\\\\]', re.DOTALL), '\\\\(' + r'\\1' + '\\\\)', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{(.*?)\\\\\\\\f\\\\}', re.DOTALL), '\\\\(' + r'\\1' + '\\\\)', doc)\n    doc = doc.replace(\"&\", \"&amp;\") \\\n        .replace(\"\\\\<\", \"&lt;\") \\\n        .replace(\"\\\\>\", \"&gt;\") \\\n        .replace(\"<\", \"&lt;\") \\",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "SCRIPT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n# list of modules + files remap\nconfig = None\nROOT_DIR = None\nFILES_REMAP = {}\ndef checkFileRemap(path):\n    path = os.path.realpath(path)\n    if path in FILES_REMAP:\n        return FILES_REMAP[path]\n    assert path[-3:] != '.in', path",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "config = None\nROOT_DIR = None\nFILES_REMAP = {}\ndef checkFileRemap(path):\n    path = os.path.realpath(path)\n    if path in FILES_REMAP:\n        return FILES_REMAP[path]\n    assert path[-3:] != '.in', path\n    return path\ntotal_files = 0",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "ROOT_DIR = None\nFILES_REMAP = {}\ndef checkFileRemap(path):\n    path = os.path.realpath(path)\n    if path in FILES_REMAP:\n        return FILES_REMAP[path]\n    assert path[-3:] != '.in', path\n    return path\ntotal_files = 0\nupdated_files = 0",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "FILES_REMAP",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "FILES_REMAP = {}\ndef checkFileRemap(path):\n    path = os.path.realpath(path)\n    if path in FILES_REMAP:\n        return FILES_REMAP[path]\n    assert path[-3:] != '.in', path\n    return path\ntotal_files = 0\nupdated_files = 0\nmodule_imports = []",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "total_files",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "total_files = 0\nupdated_files = 0\nmodule_imports = []\nmodule_j_code = None\nmodule_jn_code = None\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/java/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "updated_files",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "updated_files = 0\nmodule_imports = []\nmodule_j_code = None\nmodule_jn_code = None\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/java/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "module_imports",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "module_imports = []\nmodule_j_code = None\nmodule_jn_code = None\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/java/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "module_j_code",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "module_j_code = None\nmodule_jn_code = None\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/java/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants\nconst_private_list = []",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "module_jn_code",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "module_jn_code = None\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/java/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "class_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "class_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\n# c_type    : { java/jni correspondence }\n# Complex data types are configured for each module using misc/java/gen_dict.json",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "const_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "const_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\n# c_type    : { java/jni correspondence }\n# Complex data types are configured for each module using misc/java/gen_dict.json\ntype_dict = {\n# \"simple\"  : { j_type : \"?\", jn_type : \"?\", jni_type : \"?\", suffix : \"?\" },\n    \"\"        : { \"j_type\" : \"\", \"jn_type\" : \"long\", \"jni_type\" : \"jlong\" }, # c-tor ret_type",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "const_private_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "const_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\n# c_type    : { java/jni correspondence }\n# Complex data types are configured for each module using misc/java/gen_dict.json\ntype_dict = {\n# \"simple\"  : { j_type : \"?\", jn_type : \"?\", jni_type : \"?\", suffix : \"?\" },\n    \"\"        : { \"j_type\" : \"\", \"jn_type\" : \"long\", \"jni_type\" : \"jlong\" }, # c-tor ret_type\n    \"void\"    : { \"j_type\" : \"void\", \"jn_type\" : \"void\", \"jni_type\" : \"void\" },\n    \"env\"     : { \"j_type\" : \"\", \"jn_type\" : \"\", \"jni_type\" : \"JNIEnv*\"},",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "missing_consts",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "missing_consts = {}\n# c_type    : { java/jni correspondence }\n# Complex data types are configured for each module using misc/java/gen_dict.json\ntype_dict = {\n# \"simple\"  : { j_type : \"?\", jn_type : \"?\", jni_type : \"?\", suffix : \"?\" },\n    \"\"        : { \"j_type\" : \"\", \"jn_type\" : \"long\", \"jni_type\" : \"jlong\" }, # c-tor ret_type\n    \"void\"    : { \"j_type\" : \"void\", \"jn_type\" : \"void\", \"jni_type\" : \"void\" },\n    \"env\"     : { \"j_type\" : \"\", \"jn_type\" : \"\", \"jni_type\" : \"JNIEnv*\"},\n    \"cls\"     : { \"j_type\" : \"\", \"jn_type\" : \"\", \"jni_type\" : \"jclass\"},\n    \"bool\"    : { \"j_type\" : \"boolean\", \"jn_type\" : \"boolean\", \"jni_type\" : \"jboolean\", \"suffix\" : \"Z\" },",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "type_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "type_dict = {\n# \"simple\"  : { j_type : \"?\", jn_type : \"?\", jni_type : \"?\", suffix : \"?\" },\n    \"\"        : { \"j_type\" : \"\", \"jn_type\" : \"long\", \"jni_type\" : \"jlong\" }, # c-tor ret_type\n    \"void\"    : { \"j_type\" : \"void\", \"jn_type\" : \"void\", \"jni_type\" : \"void\" },\n    \"env\"     : { \"j_type\" : \"\", \"jn_type\" : \"\", \"jni_type\" : \"JNIEnv*\"},\n    \"cls\"     : { \"j_type\" : \"\", \"jn_type\" : \"\", \"jni_type\" : \"jclass\"},\n    \"bool\"    : { \"j_type\" : \"boolean\", \"jn_type\" : \"boolean\", \"jni_type\" : \"jboolean\", \"suffix\" : \"Z\" },\n    \"char\"    : { \"j_type\" : \"char\", \"jn_type\" : \"char\", \"jni_type\" : \"jchar\", \"suffix\" : \"C\" },\n    \"int\"     : { \"j_type\" : \"int\", \"jn_type\" : \"int\", \"jni_type\" : \"jint\", \"suffix\" : \"I\" },\n    \"long\"    : { \"j_type\" : \"int\", \"jn_type\" : \"int\", \"jni_type\" : \"jint\", \"suffix\" : \"I\" },",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "namespaces_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "namespaces_dict = {}\n# { class : { func : {j_code, jn_code, cpp_code} } }\nManualFuncs = {}\n# { class : { func : { arg_name : {\"ctype\" : ctype, \"attrib\" : [attrib]} } } }\nfunc_arg_fix = {}\ndef read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "ManualFuncs",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "ManualFuncs = {}\n# { class : { func : { arg_name : {\"ctype\" : ctype, \"attrib\" : [attrib]} } } }\nfunc_arg_fix = {}\ndef read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):\n    ''' mkdir -p '''\n    try:",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "func_arg_fix",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "func_arg_fix = {}\ndef read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):\n    ''' mkdir -p '''\n    try:\n        os.makedirs(path)\n    except OSError as exc:",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "T_JAVA_START_INHERITED",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "T_JAVA_START_INHERITED = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_class_inherited.prolog'))\nT_JAVA_START_ORPHAN = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_class.prolog'))\nT_JAVA_START_MODULE = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_module.prolog'))\nT_CPP_MODULE = Template(read_contents(os.path.join(SCRIPT_DIR, 'templates/cpp_module.template')))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.parent_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        self.cname = get_cname(self.symbol_id)\n        # parse doxygen comments\n        self.params={}",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "T_JAVA_START_ORPHAN",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "T_JAVA_START_ORPHAN = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_class.prolog'))\nT_JAVA_START_MODULE = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_module.prolog'))\nT_CPP_MODULE = Template(read_contents(os.path.join(SCRIPT_DIR, 'templates/cpp_module.template')))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.parent_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        self.cname = get_cname(self.symbol_id)\n        # parse doxygen comments\n        self.params={}\n        self.annotation=[]",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "T_JAVA_START_MODULE",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "T_JAVA_START_MODULE = read_contents(os.path.join(SCRIPT_DIR, 'templates/java_module.prolog'))\nT_CPP_MODULE = Template(read_contents(os.path.join(SCRIPT_DIR, 'templates/cpp_module.template')))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.parent_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        self.cname = get_cname(self.symbol_id)\n        # parse doxygen comments\n        self.params={}\n        self.annotation=[]\n        if type == \"class\":",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "T_CPP_MODULE",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.generator.gen_java",
        "description": "Hw_2.opencv.modules.java.generator.gen_java",
        "peekOfCode": "T_CPP_MODULE = Template(read_contents(os.path.join(SCRIPT_DIR, 'templates/cpp_module.template')))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.parent_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        self.cname = get_cname(self.symbol_id)\n        # parse doxygen comments\n        self.params={}\n        self.annotation=[]\n        if type == \"class\":\n            docstring=\"// C++: class \" + self.name + \"\\n\"",
        "detail": "Hw_2.opencv.modules.java.generator.gen_java",
        "documentation": {}
    },
    {
        "label": "JavaParser",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.java.check-tests",
        "description": "Hw_2.opencv.modules.java.check-tests",
        "peekOfCode": "class JavaParser:\n    def __init__(self):\n        self.clear()\n    def clear(self):\n        self.mdict = {}\n        self.tdict = {}\n        self.mwhere = {}\n        self.twhere = {}\n        self.empty_stubs_cnt = 0\n        self.r1 = re.compile(\"\\s*public\\s+(?:static\\s+)?(\\w+)\\(([^)]*)\\)\") # c-tor",
        "detail": "Hw_2.opencv.modules.java.check-tests",
        "documentation": {}
    },
    {
        "label": "classes_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.check-tests",
        "description": "Hw_2.opencv.modules.java.check-tests",
        "peekOfCode": "classes_ignore_list = (\n    'OpenCV(Test)?Case',\n    'OpenCV(Test)?Runner',\n    'CvException',\n)\nfuncs_ignore_list = (\n    '\\w+--HashCode',\n    'Mat--MatLong',\n    '\\w+--Equals',\n    'Core--MinMaxLocResult',",
        "detail": "Hw_2.opencv.modules.java.check-tests",
        "documentation": {}
    },
    {
        "label": "funcs_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.java.check-tests",
        "description": "Hw_2.opencv.modules.java.check-tests",
        "peekOfCode": "funcs_ignore_list = (\n    '\\w+--HashCode',\n    'Mat--MatLong',\n    '\\w+--Equals',\n    'Core--MinMaxLocResult',\n)\nclass JavaParser:\n    def __init__(self):\n        self.clear()\n    def clear(self):",
        "detail": "Hw_2.opencv.modules.java.check-tests",
        "documentation": {}
    },
    {
        "label": "ClassProp",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class ClassProp(object):\n    def __init__(self, decl):\n        self.tp = decl[0].replace(\"*\", \"_ptr\").strip()\n        self.name = decl[1]\n        self.readonly = True\n        if \"/RW\" in decl[3]:\n            self.readonly = False\nclass ClassInfo(object):\n    def __init__(self, name, decl=None):\n        self.cname = name.replace(\".\", \"::\")",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "ClassInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class ClassInfo(object):\n    def __init__(self, name, decl=None):\n        self.cname = name.replace(\".\", \"::\")\n        self.name = self.wname = normalize_class_name(name)\n        self.ismap = False\n        self.issimple = False\n        self.isalgorithm = False\n        self.methods = {}\n        self.ext_constructors = {}\n        self.props = []",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "ArgInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class ArgInfo(object):\n    def __init__(self, arg_tuple):\n        self.tp = handle_ptr(arg_tuple[0]).strip()\n        self.name = arg_tuple[1]\n        self.defval = arg_tuple[2]\n        self.isarray = False\n        self.arraylen = 0\n        self.arraycvt = None\n        self.inputarg = True\n        self.outputarg = False",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "FuncVariant",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class FuncVariant(object):\n    def __init__(self, class_name, name, decl, is_constructor, is_class_method, is_const, is_virtual, is_pure_virtual, ref_return, const_return):\n        self.class_name = class_name\n        self.name = self.wname = name\n        self.is_constructor = is_constructor\n        self.is_class_method = is_class_method\n        self.is_const = is_const\n        self.is_virtual = is_virtual\n        self.is_pure_virtual = is_pure_virtual\n        self.refret = ref_return",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "FuncInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class FuncInfo(object):\n    def __init__(self, class_name, name, cname, namespace, isconstructor):\n        self.name_id = '_'.join([namespace] + ([class_name] if class_name else []) + [name])  # unique id for dict key\n        self.class_name = class_name\n        self.name = name\n        self.cname = cname\n        self.namespace = namespace\n        self.variants = []\n        self.is_constructor = isconstructor\n    def add_variant(self, variant):",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class Namespace(object):\n    def __init__(self):\n        self.funcs = {}\n        self.enums = {}\n        self.consts = {}\nclass JSWrapperGenerator(object):\n    def __init__(self):\n        self.bindings = []\n        self.wrapper_funcs = []\n        self.classes = {}  # FIXIT 'classes' should belong to 'namespaces'",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "JSWrapperGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "class JSWrapperGenerator(object):\n    def __init__(self):\n        self.bindings = []\n        self.wrapper_funcs = []\n        self.classes = {}  # FIXIT 'classes' should belong to 'namespaces'\n        self.namespaces = {}\n        self.enums = {}  # FIXIT 'enums' should belong to 'namespaces'\n        self.parser = hdr_parser.CppHeaderParser()\n        self.class_idx = 0\n    def add_class(self, stype, name, decl):",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "makeWhiteList",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "def makeWhiteList(module_list):\n    wl = {}\n    for m in module_list:\n        for k in m.keys():\n            if k in wl:\n                wl[k] += m[k]\n            else:\n                wl[k] = m[k]\n    return wl\nwhite_list = None",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "normalize_class_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "def normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\nclass ClassProp(object):\n    def __init__(self, decl):\n        self.tp = decl[0].replace(\"*\", \"_ptr\").strip()\n        self.name = decl[1]\n        self.readonly = True\n        if \"/RW\" in decl[3]:\n            self.readonly = False\nclass ClassInfo(object):",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "handle_ptr",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "def handle_ptr(tp):\n    if tp.startswith('Ptr_'):\n        tp = 'Ptr<' + \"::\".join(tp.split('_')[1:]) + '>'\n    return tp\ndef handle_vector(tp):\n    if tp.startswith('vector_'):\n        tp = handle_vector(tp[tp.find('_') + 1:])\n        tp = 'std::vector<' + \"::\".join(tp.split('_')) + '>'\n    return tp\nclass ArgInfo(object):",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "handle_vector",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "def handle_vector(tp):\n    if tp.startswith('vector_'):\n        tp = handle_vector(tp[tp.find('_') + 1:])\n        tp = 'std::vector<' + \"::\".join(tp.split('_')) + '>'\n    return tp\nclass ArgInfo(object):\n    def __init__(self, arg_tuple):\n        self.tp = handle_ptr(arg_tuple[0]).strip()\n        self.name = arg_tuple[1]\n        self.defval = arg_tuple[2]",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "func_table",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "func_table = {}\n# Ignore these functions due to Embind limitations for now\nignore_list = ['locate',  #int&\n               'minEnclosingCircle',  #float&\n               'checkRange',\n               'minMaxLoc',   #double*\n               'floodFill', # special case, implemented in core_bindings.cpp\n               'phaseCorrelate',\n               'randShuffle',\n               'calibrationMatrixValues', #double&",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "ignore_list = ['locate',  #int&\n               'minEnclosingCircle',  #float&\n               'checkRange',\n               'minMaxLoc',   #double*\n               'floodFill', # special case, implemented in core_bindings.cpp\n               'phaseCorrelate',\n               'randShuffle',\n               'calibrationMatrixValues', #double&\n               'undistortPoints', # global redefinition\n               'CamShift', #Rect&",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "white_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "white_list = None\nnamespace_prefix_override = {\n    'dnn' : '',\n    'aruco' : '',\n}\n# Features to be exported\nexport_enums = False\nexport_consts = True\nwith_wrapped_functions = True\nwith_default_params = True",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "namespace_prefix_override",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "namespace_prefix_override = {\n    'dnn' : '',\n    'aruco' : '',\n}\n# Features to be exported\nexport_enums = False\nexport_consts = True\nwith_wrapped_functions = True\nwith_default_params = True\nwith_vec_from_js_array = True",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "export_enums",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "export_enums = False\nexport_consts = True\nwith_wrapped_functions = True\nwith_default_params = True\nwith_vec_from_js_array = True\nwrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "export_consts",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "export_consts = True\nwith_wrapped_functions = True\nwith_default_params = True\nwith_vec_from_js_array = True\nwrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "with_wrapped_functions",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "with_wrapped_functions = True\nwith_default_params = True\nwith_vec_from_js_array = True\nwrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',\n    'OutputArrayOfArrays': 'std::vector<cv::Mat>&',",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "with_default_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "with_default_params = True\nwith_vec_from_js_array = True\nwrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',\n    'OutputArrayOfArrays': 'std::vector<cv::Mat>&',\n    'string': 'std::string',",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "with_vec_from_js_array",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "with_vec_from_js_array = True\nwrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',\n    'OutputArrayOfArrays': 'std::vector<cv::Mat>&',\n    'string': 'std::string',\n    'String': 'std::string',",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "wrapper_namespace",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "wrapper_namespace = \"Wrappers\"\ntype_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',\n    'OutputArrayOfArrays': 'std::vector<cv::Mat>&',\n    'string': 'std::string',\n    'String': 'std::string',\n    'const String&':'const std::string&'",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "type_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.embindgen",
        "description": "Hw_2.opencv.modules.js.generator.embindgen",
        "peekOfCode": "type_dict = {\n    'InputArray': 'const cv::Mat&',\n    'OutputArray': 'cv::Mat&',\n    'InputOutputArray': 'cv::Mat&',\n    'InputArrayOfArrays': 'const std::vector<cv::Mat>&',\n    'OutputArrayOfArrays': 'std::vector<cv::Mat>&',\n    'string': 'std::string',\n    'String': 'std::string',\n    'const String&':'const std::string&'\n}",
        "detail": "Hw_2.opencv.modules.js.generator.embindgen",
        "documentation": {}
    },
    {
        "label": "wrapper_codes_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "wrapper_codes_template = Template(\"namespace $ns {\\n$defs\\n}\")\ncall_template = Template(\"\"\"$func($args)\"\"\")\nclass_call_template = Template(\"\"\"$obj.$func($args)\"\"\")\nstatic_class_call_template = Template(\"\"\"$scope$func($args)\"\"\")\nwrapper_function_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        return $cpp_call;\n    }\n    \"\"\")\nwrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "call_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "call_template = Template(\"\"\"$func($args)\"\"\")\nclass_call_template = Template(\"\"\"$obj.$func($args)\"\"\")\nstatic_class_call_template = Template(\"\"\"$scope$func($args)\"\"\")\nwrapper_function_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        return $cpp_call;\n    }\n    \"\"\")\nwrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args\n    }",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "class_call_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "class_call_template = Template(\"\"\"$obj.$func($args)\"\"\")\nstatic_class_call_template = Template(\"\"\"$scope$func($args)\"\"\")\nwrapper_function_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        return $cpp_call;\n    }\n    \"\"\")\nwrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args\n    }\n    \"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "static_class_call_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "static_class_call_template = Template(\"\"\"$scope$func($args)\"\"\")\nwrapper_function_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        return $cpp_call;\n    }\n    \"\"\")\nwrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args\n    }\n    \"\"\")\nwrapper_overload_def_values = [",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "wrapper_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "wrapper_function_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        return $cpp_call;\n    }\n    \"\"\")\nwrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args\n    }\n    \"\"\")\nwrapper_overload_def_values = [\n    Template(\"\"\"return $cpp_call;\"\"\"), Template(\"\"\"if ($arg0.isUndefined())",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "wrapper_function_with_def_args_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "wrapper_function_with_def_args_template = Template(\"\"\"    $ret_val $func($signature)$const {\n        $check_args\n    }\n    \"\"\")\nwrapper_overload_def_values = [\n    Template(\"\"\"return $cpp_call;\"\"\"), Template(\"\"\"if ($arg0.isUndefined())\n            return $cpp_call;\n        else\n            $next\"\"\"),\n    Template(\"\"\"if ($arg0.isUndefined() && $arg1.isUndefined())",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "wrapper_overload_def_values",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "wrapper_overload_def_values = [\n    Template(\"\"\"return $cpp_call;\"\"\"), Template(\"\"\"if ($arg0.isUndefined())\n            return $cpp_call;\n        else\n            $next\"\"\"),\n    Template(\"\"\"if ($arg0.isUndefined() && $arg1.isUndefined())\n            return $cpp_call;\n        else $next\"\"\"),\n    Template(\"\"\"if ($arg0.isUndefined() && $arg1.isUndefined() && $arg2.isUndefined())\n            return $cpp_call;",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "emscripten_binding_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "emscripten_binding_template = Template(\"\"\"\nEMSCRIPTEN_BINDINGS($binding_name) {$bindings\n}\n\"\"\")\nsimple_function_template = Template(\"\"\"\n    emscripten::function(\"$js_name\", &$cpp_name);\n\"\"\")\nsmart_ptr_reg_template = Template(\"\"\"\n        .smart_ptr<Ptr<$cname>>(\"Ptr<$name>\")\n\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "simple_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "simple_function_template = Template(\"\"\"\n    emscripten::function(\"$js_name\", &$cpp_name);\n\"\"\")\nsmart_ptr_reg_template = Template(\"\"\"\n        .smart_ptr<Ptr<$cname>>(\"Ptr<$name>\")\n\"\"\")\noverload_function_template = Template(\"\"\"\n    function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional);\n\"\"\")\noverload_class_function_template = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "smart_ptr_reg_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "smart_ptr_reg_template = Template(\"\"\"\n        .smart_ptr<Ptr<$cname>>(\"Ptr<$name>\")\n\"\"\")\noverload_function_template = Template(\"\"\"\n    function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional);\n\"\"\")\noverload_class_function_template = Template(\"\"\"\n        .function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\noverload_class_static_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "overload_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "overload_function_template = Template(\"\"\"\n    function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional);\n\"\"\")\noverload_class_function_template = Template(\"\"\"\n        .function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\noverload_class_static_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nclass_property_template = Template(\"\"\"\n        .property(\"$js_name\", &$cpp_name)\"\"\")\nclass_property_enum_template = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "overload_class_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "overload_class_function_template = Template(\"\"\"\n        .function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\noverload_class_static_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nclass_property_template = Template(\"\"\"\n        .property(\"$js_name\", &$cpp_name)\"\"\")\nclass_property_enum_template = Template(\"\"\"\n        .property(\"$js_name\", binding_utils::underlying_ptr(&$cpp_name))\"\"\")\nctr_template = Template(\"\"\"\n        .constructor(select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "overload_class_static_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "overload_class_static_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nclass_property_template = Template(\"\"\"\n        .property(\"$js_name\", &$cpp_name)\"\"\")\nclass_property_enum_template = Template(\"\"\"\n        .property(\"$js_name\", binding_utils::underlying_ptr(&$cpp_name))\"\"\")\nctr_template = Template(\"\"\"\n        .constructor(select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nsmart_ptr_ctr_overload_template = Template(\"\"\"\n        .smart_ptr_constructor(\"$ptr_type\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "class_property_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "class_property_template = Template(\"\"\"\n        .property(\"$js_name\", &$cpp_name)\"\"\")\nclass_property_enum_template = Template(\"\"\"\n        .property(\"$js_name\", binding_utils::underlying_ptr(&$cpp_name))\"\"\")\nctr_template = Template(\"\"\"\n        .constructor(select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nsmart_ptr_ctr_overload_template = Template(\"\"\"\n        .smart_ptr_constructor(\"$ptr_type\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nfunction_template = Template(\"\"\"\n        .function(\"$js_name\", &$cpp_name)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "class_property_enum_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "class_property_enum_template = Template(\"\"\"\n        .property(\"$js_name\", binding_utils::underlying_ptr(&$cpp_name))\"\"\")\nctr_template = Template(\"\"\"\n        .constructor(select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nsmart_ptr_ctr_overload_template = Template(\"\"\"\n        .smart_ptr_constructor(\"$ptr_type\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nfunction_template = Template(\"\"\"\n        .function(\"$js_name\", &$cpp_name)\"\"\")\nstatic_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", &$cpp_name)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "ctr_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "ctr_template = Template(\"\"\"\n        .constructor(select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nsmart_ptr_ctr_overload_template = Template(\"\"\"\n        .smart_ptr_constructor(\"$ptr_type\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nfunction_template = Template(\"\"\"\n        .function(\"$js_name\", &$cpp_name)\"\"\")\nstatic_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", &$cpp_name)\"\"\")\nconstructor_template = Template(\"\"\"\n        .constructor<$signature>()\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "smart_ptr_ctr_overload_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "smart_ptr_ctr_overload_template = Template(\"\"\"\n        .smart_ptr_constructor(\"$ptr_type\", select_overload<$ret($args)$const>(&$cpp_name)$optional)\"\"\")\nfunction_template = Template(\"\"\"\n        .function(\"$js_name\", &$cpp_name)\"\"\")\nstatic_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", &$cpp_name)\"\"\")\nconstructor_template = Template(\"\"\"\n        .constructor<$signature>()\"\"\")\nenum_item_template = Template(\"\"\"\n        .value(\"$val\", $cpp_val)\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "function_template = Template(\"\"\"\n        .function(\"$js_name\", &$cpp_name)\"\"\")\nstatic_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", &$cpp_name)\"\"\")\nconstructor_template = Template(\"\"\"\n        .constructor<$signature>()\"\"\")\nenum_item_template = Template(\"\"\"\n        .value(\"$val\", $cpp_val)\"\"\")\nenum_template = Template(\"\"\"\n    emscripten::enum_<$cpp_name>(\"$js_name\")$enum_items;",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "static_function_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "static_function_template = Template(\"\"\"\n        .class_function(\"$js_name\", &$cpp_name)\"\"\")\nconstructor_template = Template(\"\"\"\n        .constructor<$signature>()\"\"\")\nenum_item_template = Template(\"\"\"\n        .value(\"$val\", $cpp_val)\"\"\")\nenum_template = Template(\"\"\"\n    emscripten::enum_<$cpp_name>(\"$js_name\")$enum_items;\n\"\"\")\nconst_template = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "constructor_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "constructor_template = Template(\"\"\"\n        .constructor<$signature>()\"\"\")\nenum_item_template = Template(\"\"\"\n        .value(\"$val\", $cpp_val)\"\"\")\nenum_template = Template(\"\"\"\n    emscripten::enum_<$cpp_name>(\"$js_name\")$enum_items;\n\"\"\")\nconst_template = Template(\"\"\"\n    constant(\"$js_name\", static_cast<long>($value));\n\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "enum_item_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "enum_item_template = Template(\"\"\"\n        .value(\"$val\", $cpp_val)\"\"\")\nenum_template = Template(\"\"\"\n    emscripten::enum_<$cpp_name>(\"$js_name\")$enum_items;\n\"\"\")\nconst_template = Template(\"\"\"\n    constant(\"$js_name\", static_cast<long>($value));\n\"\"\")\nvector_template = Template(\"\"\"\n     emscripten::register_vector<$cType>(\"$js_name\");",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "enum_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "enum_template = Template(\"\"\"\n    emscripten::enum_<$cpp_name>(\"$js_name\")$enum_items;\n\"\"\")\nconst_template = Template(\"\"\"\n    constant(\"$js_name\", static_cast<long>($value));\n\"\"\")\nvector_template = Template(\"\"\"\n     emscripten::register_vector<$cType>(\"$js_name\");\n\"\"\")\nmap_template = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "const_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "const_template = Template(\"\"\"\n    constant(\"$js_name\", static_cast<long>($value));\n\"\"\")\nvector_template = Template(\"\"\"\n     emscripten::register_vector<$cType>(\"$js_name\");\n\"\"\")\nmap_template = Template(\"\"\"\n     emscripten::register_map<cpp_type_key,$cpp_type_val>(\"$js_name\");\n\"\"\")\nclass_template = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "vector_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "vector_template = Template(\"\"\"\n     emscripten::register_vector<$cType>(\"$js_name\");\n\"\"\")\nmap_template = Template(\"\"\"\n     emscripten::register_map<cpp_type_key,$cpp_type_val>(\"$js_name\");\n\"\"\")\nclass_template = Template(\"\"\"\n    emscripten::class_<$cpp_name $derivation>(\"$js_name\")$class_templates;\n\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "map_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "map_template = Template(\"\"\"\n     emscripten::register_map<cpp_type_key,$cpp_type_val>(\"$js_name\");\n\"\"\")\nclass_template = Template(\"\"\"\n    emscripten::class_<$cpp_name $derivation>(\"$js_name\")$class_templates;\n\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "class_template",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.generator.templates",
        "description": "Hw_2.opencv.modules.js.generator.templates",
        "peekOfCode": "class_template = Template(\"\"\"\n    emscripten::class_<$cpp_name $derivation>(\"$js_name\")$class_templates;\n\"\"\")",
        "detail": "Hw_2.opencv.modules.js.generator.templates",
        "documentation": {}
    },
    {
        "label": "make_umd",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.js.src.make_umd",
        "description": "Hw_2.opencv.modules.js.src.make_umd",
        "peekOfCode": "def make_umd(opencvjs, cvjs):\n    with open(opencvjs, 'r+b') as src:\n        content = src.read()\n    if PY3:  # content is bytes\n        content = content.decode('utf-8')\n    with open(cvjs, 'w+b') as dst:\n        # inspired by https://github.com/umdjs/umd/blob/95563fd6b46f06bda0af143ff67292e7f6ede6b7/templates/returnExportsGlobal.js\n        dst.write((\"\"\"\n(function (root, factory) {\n  if (typeof define === 'function' && define.amd) {",
        "detail": "Hw_2.opencv.modules.js.src.make_umd",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.js.src.make_umd",
        "description": "Hw_2.opencv.modules.js.src.make_umd",
        "peekOfCode": "PY3 = sys.version_info >= (3, 0)\ndef make_umd(opencvjs, cvjs):\n    with open(opencvjs, 'r+b') as src:\n        content = src.read()\n    if PY3:  # content is bytes\n        content = content.decode('utf-8')\n    with open(cvjs, 'w+b') as dst:\n        # inspired by https://github.com/umdjs/umd/blob/95563fd6b46f06bda0af143ff67292e7f6ede6b7/templates/returnExportsGlobal.js\n        dst.write((\"\"\"\n(function (root, factory) {",
        "detail": "Hw_2.opencv.modules.js.src.make_umd",
        "documentation": {}
    },
    {
        "label": "StatModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "class StatModel(object):\n    def load(self, fn):\n        self.model.load(fn)  # Known bug: https://github.com/opencv/opencv/issues/4969\n    def save(self, fn):\n        self.model.save(fn)\nclass KNearest(StatModel):\n    def __init__(self, k = 3):\n        self.k = k\n        self.model = cv.ml.KNearest_create()\n    def train(self, samples, responses):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "KNearest",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "class KNearest(StatModel):\n    def __init__(self, k = 3):\n        self.k = k\n        self.model = cv.ml.KNearest_create()\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()\nclass SVM(StatModel):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "SVM",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "class SVM(StatModel):\n    def __init__(self, C = 1, gamma = 0.5):\n        self.model = cv.ml.SVM_create()\n        self.model.setGamma(gamma)\n        self.model.setC(C)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setType(cv.ml.SVM_C_SVC)\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "digits_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "class digits_test(NewOpenCVTests):\n    def load_digits(self, fn):\n        digits_img = self.get_sample(fn, 0)\n        digits = split2d(digits_img, (SZ, SZ))\n        labels = np.repeat(np.arange(CLASS_N), len(digits)/CLASS_N)\n        return digits, labels\n    def test_digits(self):\n        digits, labels = self.load_digits(DIGITS_FN)\n        # shuffle digits\n        rand = np.random.RandomState(321)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "split2d",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "def split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells\ndef deskew(img):\n    m = cv.moments(img)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "deskew",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "def deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img, M, (SZ, SZ), flags=cv.WARP_INVERSE_MAP | cv.INTER_LINEAR)\n    return img\nclass StatModel(object):\n    def load(self, fn):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "def evaluate_model(model, digits, samples, labels):\n    resp = model.predict(samples)\n    err = (labels != resp).mean()\n    confusion = np.zeros((10, 10), np.int32)\n    for i, j in zip(labels, resp):\n        confusion[int(i), int(j)] += 1\n    return err, confusion\ndef preprocess_simple(digits):\n    return np.float32(digits).reshape(-1, SZ*SZ) / 255.0\ndef preprocess_hog(digits):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "preprocess_simple",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "def preprocess_simple(digits):\n    return np.float32(digits).reshape(-1, SZ*SZ) / 255.0\ndef preprocess_hog(digits):\n    samples = []\n    for img in digits:\n        gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n        gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n        mag, ang = cv.cartToPolar(gx, gy)\n        bin_n = 16\n        bin = np.int32(bin_n*ang/(2*np.pi))",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "preprocess_hog",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "def preprocess_hog(digits):\n    samples = []\n    for img in digits:\n        gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n        gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n        mag, ang = cv.cartToPolar(gx, gy)\n        bin_n = 16\n        bin = np.int32(bin_n*ang/(2*np.pi))\n        bin_cells = bin[:10,:10], bin[10:,:10], bin[:10,10:], bin[10:,10:]\n        mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "SZ",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "SZ = 20 # size of each digit is SZ x SZ\nCLASS_N = 10\nDIGITS_FN = 'samples/data/digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "CLASS_N",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "CLASS_N = 10\nDIGITS_FN = 'samples/data/digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "DIGITS_FN",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "peekOfCode": "DIGITS_FN = 'samples/data/digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells\ndef deskew(img):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_digits",
        "documentation": {}
    },
    {
        "label": "TestGoodFeaturesToTrack_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_goodfeatures",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_goodfeatures",
        "peekOfCode": "class TestGoodFeaturesToTrack_test(NewOpenCVTests):\n    def test_goodFeaturesToTrack(self):\n        arr = self.get_sample('samples/data/lena.jpg', 0)\n        original = arr.copy()\n        threshes = [ x / 100. for x in range(1,10) ]\n        numPoints = 20000\n        results = dict([(t, cv.goodFeaturesToTrack(arr, numPoints, t, 2, useHarrisDetector=True)) for t in threshes])\n        # Check that GoodFeaturesToTrack has not modified input image\n        self.assertTrue(arr.tostring() == original.tostring())\n        # Check for repeatability",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_goodfeatures",
        "documentation": {}
    },
    {
        "label": "knearest_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_knearest",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_knearest",
        "peekOfCode": "class knearest_test(NewOpenCVTests):\n    def test_load(self):\n        k_nearest = cv.ml.KNearest_load(self.find_file(\"ml/opencv_ml_knn.xml\"))\n        self.assertFalse(k_nearest.empty())\n        self.assertTrue(k_nearest.isTrained())\nif __name__ == '__main__':\n    NewOpenCVTests.bootstrap()",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_knearest",
        "documentation": {}
    },
    {
        "label": "LetterStatModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class LetterStatModel(object):\n    class_n = 26\n    train_ratio = 0.5\n    def load(self, fn):\n        self.model.load(fn)\n    def save(self, fn):\n        self.model.save(fn)\n    def unroll_samples(self, samples):\n        sample_n, var_n = samples.shape\n        new_samples = np.zeros((sample_n * self.class_n, var_n+1), np.float32)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "RTrees",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class RTrees(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.RTrees_create()\n    def train(self, samples, responses):\n        #sample_n, var_n = samples.shape\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))\n    def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "KNearest",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class KNearest(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.KNearest_create()\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()\nclass Boost(LetterStatModel):\n    def __init__(self):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "Boost",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class Boost(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.Boost_create()\n    def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_samples = self.unroll_samples(samples)\n        new_responses = self.unroll_responses(responses)\n        var_types = np.array([cv.ml.VAR_NUMERICAL] * var_n + [cv.ml.VAR_CATEGORICAL, cv.ml.VAR_CATEGORICAL], np.uint8)\n        self.model.setWeakCount(15)\n        self.model.setMaxDepth(10)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "SVM",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class SVM(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.SVM_create()\n    def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))\n    def predict(self, samples):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class MLP(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.ANN_MLP_create()\n    def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_responses = self.unroll_responses(responses).reshape(-1, self.class_n)\n        layer_sizes = np.int32([var_n, 100, 100, self.class_n])\n        self.model.setLayerSizes(layer_sizes)\n        self.model.setTrainMethod(cv.ml.ANN_MLP_BACKPROP)\n        self.model.setBackpropMomentumScale(0)",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "letter_recog_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "class letter_recog_test(NewOpenCVTests):\n    def test_letter_recog(self):\n        eps = 0.01\n        models = [RTrees, KNearest, Boost, SVM, MLP]\n        models = dict( [(cls.__name__.lower(), cls) for cls in models] )\n        testErrors = {RTrees: (98.930000, 92.390000), KNearest: (94.960000, 92.010000),\n         Boost: (85.970000, 74.920000), SVM: (99.780000, 95.680000), MLP: (90.060000, 87.410000)}\n        for model in models:\n            Model = models[model]\n            classifier = Model()",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "load_base",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "description": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "peekOfCode": "def load_base(fn):\n    a = np.loadtxt(fn, np.float32, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n    samples, responses = a[:,1:], a[:,0]\n    return samples, responses\nclass LetterStatModel(object):\n    class_n = 26\n    train_ratio = 0.5\n    def load(self, fn):\n        self.model.load(fn)\n    def save(self, fn):",
        "detail": "Hw_2.opencv.modules.ml.misc.python.test.test_letter_recog",
        "documentation": {}
    },
    {
        "label": "SkipSymbolException",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class SkipSymbolException(Exception):\n    def __init__(self, text):\n        self.t = text\n    def __str__(self):\n        return self.t\ndef read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "GeneralInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        for ns_ignore in namespace_ignore_list:\n            if self.symbol_id.startswith(ns_ignore + '.'):\n                raise SkipSymbolException('ignored namespace ({}): {}'.format(ns_ignore, self.symbol_id))\n        # parse doxygen comments\n        self.params={}\n        self.deprecated = False\n        if type == \"class\":",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ConstInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class ConstInfo(GeneralInfo):\n    def __init__(self, decl, addedManually=False, namespaces=[], enumType=None):\n        GeneralInfo.__init__(self, \"const\", decl, namespaces)\n        self.cname = get_cname(self.name)\n        self.swift_name = None\n        self.value = decl[1]\n        self.enumType = enumType\n        self.addedManually = addedManually\n        if self.namespace in namespaces_dict:\n            self.name = '%s_%s' % (namespaces_dict[self.namespace], self.name)",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ClassPropInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class ClassPropInfo():\n    def __init__(self, decl): # [f_ctype, f_name, '', '/RW']\n        self.ctype = decl[0]\n        self.name = decl[1]\n        self.rw = \"/RW\" in decl[3]\n    def __repr__(self):\n        return Template(\"PROP $ctype $name\").substitute(ctype=self.ctype, name=self.name)\nclass ClassInfo(GeneralInfo):\n    def __init__(self, decl, namespaces=[]): # [ 'class/struct cname', ': base', [modlist] ]\n        GeneralInfo.__init__(self, \"class\", decl, namespaces)",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ClassInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class ClassInfo(GeneralInfo):\n    def __init__(self, decl, namespaces=[]): # [ 'class/struct cname', ': base', [modlist] ]\n        GeneralInfo.__init__(self, \"class\", decl, namespaces)\n        self.cname = self.name if not self.classname else self.classname + \"_\" + self.name\n        self.real_cname = self.name if not self.classname else self.classname + \"::\" + self.name\n        self.methods = []\n        self.methods_suffixes = {}\n        self.consts = [] # using a list to save the occurrence order\n        self.private_consts = []\n        self.imports = set()",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ArgInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class ArgInfo():\n    def __init__(self, arg_tuple): # [ ctype, name, def val, [mod], argno ]\n        self.pointer = False\n        ctype = arg_tuple[0]\n        if ctype.endswith(\"*\"):\n            ctype = ctype[:-1]\n            self.pointer = True\n        self.ctype = ctype\n        self.name = arg_tuple[1]\n        self.defval = arg_tuple[2]",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "FuncInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class FuncInfo(GeneralInfo):\n    def __init__(self, decl, module, namespaces=[]): # [ funcname, return_ctype, [modifiers], [args] ]\n        GeneralInfo.__init__(self, \"func\", decl, namespaces)\n        self.cname = get_cname(decl[0])\n        nested_type = self.classpath.find(\".\") != -1\n        self.objc_name = self.name if not nested_type else self.classpath.replace(\".\", \"\")\n        self.classname = self.classname if not nested_type else self.classpath.replace(\".\", \"_\")\n        self.swift_name = self.name\n        self.cv_name = self.fullName(isCPP=True)\n        self.isconstructor = self.name == self.classname",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ObjectiveCWrapperGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class ObjectiveCWrapperGenerator(object):\n    def __init__(self):\n        self.header_files = []\n        self.clear()\n    def clear(self):\n        self.namespaces = [\"cv\"]\n        mat_class_info = ClassInfo([ 'class Mat', '', [], [] ], self.namespaces)\n        mat_class_info.namespace = \"cv\"\n        self.classes = { \"Mat\" : mat_class_info }\n        self.classes[\"Mat\"].namespace = \"cv\"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "read_contents",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data\ndef mkdir_p(path):\n    ''' mkdir -p '''\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "mkdir_p",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def mkdir_p(path):\n    ''' mkdir -p '''\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\ndef header_import(hdr):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "header_import",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def header_import(hdr):\n    \"\"\" converts absolute header path to import parameter \"\"\"\n    pos = hdr.find('/include/')\n    hdr = hdr[pos+9 if pos >= 0 else 0:]\n    #pos = hdr.find('opencv2/')\n    #hdr = hdr[pos+8 if pos >= 0 else 0:]\n    return hdr\nT_OBJC_CLASS_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_class_header.template'))\nT_OBJC_CLASS_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_class_body.template'))\nT_OBJC_MODULE_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_header.template'))",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "normalize_field_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def normalize_field_name(name):\n    return name.replace(\".\",\"_\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"_getNativeObjAddr()\",\"_nativeObj\")\ndef normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "normalize_class_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "get_cname",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def get_cname(name):\n    return name.replace(\".\", \"::\")\ndef cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "cast_from",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def cast_from(t):\n    if t in type_dict and \"cast_from\" in type_dict[t]:\n        return type_dict[t][\"cast_from\"]\n    return t\ndef cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t\ndef gen_class_doc(docstring, module, members, enums):\n    lines = docstring.splitlines()",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "cast_to",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def cast_to(t):\n    if t in type_dict and \"cast_to\" in type_dict[t]:\n        return type_dict[t][\"cast_to\"]\n    return t\ndef gen_class_doc(docstring, module, members, enums):\n    lines = docstring.splitlines()\n    lines.insert(len(lines)-1, \" *\")\n    if len(members) > 0:\n        lines.insert(len(lines)-1, \" * Member classes: \" + \", \".join([(\"`\" + m + \"`\") for m in members]))\n        lines.insert(len(lines)-1, \" *\")",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "gen_class_doc",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def gen_class_doc(docstring, module, members, enums):\n    lines = docstring.splitlines()\n    lines.insert(len(lines)-1, \" *\")\n    if len(members) > 0:\n        lines.insert(len(lines)-1, \" * Member classes: \" + \", \".join([(\"`\" + m + \"`\") for m in members]))\n        lines.insert(len(lines)-1, \" *\")\n    else:\n        lines.insert(len(lines)-1, \" * Member of `\" + module + \"`\")\n    if len(enums) > 0:\n        lines.insert(len(lines)-1, \" * Member enums: \" + \", \".join([(\"`\" + m + \"`\") for m in enums]))",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "type_complete",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def type_complete(args, ctype):\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue\n            return False\n    if ctype not in type_dict:",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_objc_args",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_objc_args(args):\n    objc_args = []\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue\n        if not a.ctype:  # hidden",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_objc_method_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_objc_method_name(args):\n    objc_method_name = \"\"\n    for a in args[1:]:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue\n        if not a.ctype:  # hidden",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "get_swift_type",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def get_swift_type(ctype):\n    has_swift_type = \"swift_type\" in type_dict[ctype]\n    swift_type = type_dict[ctype][\"swift_type\"] if has_swift_type else type_dict[ctype][\"objc_type\"]\n    if swift_type[-1:] == \"*\":\n        swift_type = swift_type[:-1]\n    if not has_swift_type:\n        if \"v_type\" in type_dict[ctype]:\n            swift_type = \"[\" + swift_type + \"]\"\n        elif \"v_v_type\" in type_dict[ctype]:\n            swift_type = \"[[\" + swift_type + \"]]\"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_swift_extension_decl",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_swift_extension_decl(name, args, constructor, static, ret_type):\n    extension_decl = \"@nonobjc \" + (\"class \" if static else \"\") + ((\"func \" + name) if not constructor else \"convenience init\") + \"(\"\n    swift_args = []\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "extension_arg",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def extension_arg(a):\n    return a.ctype in type_dict and (type_dict[a.ctype].get(\"primitive_vector\", False) or type_dict[a.ctype].get(\"primitive_vector_vector\", False) or ((\"v_type\" in type_dict[a.ctype] or \"v_v_type\" in type_dict[a.ctype]) and \"O\" in a.out))\ndef extension_tmp_arg(a):\n    if a.ctype in type_dict:\n        if type_dict[a.ctype].get(\"primitive_vector\", False) or type_dict[a.ctype].get(\"primitive_vector_vector\", False):\n            return a.name + \"Vector\"\n        elif (\"v_type\" in type_dict[a.ctype] or \"v_v_type\" in type_dict[a.ctype]) and \"O\" in a.out:\n            return a.name + \"Array\"\n    return a.name\ndef make_swift_extension(args):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "extension_tmp_arg",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def extension_tmp_arg(a):\n    if a.ctype in type_dict:\n        if type_dict[a.ctype].get(\"primitive_vector\", False) or type_dict[a.ctype].get(\"primitive_vector_vector\", False):\n            return a.name + \"Vector\"\n        elif (\"v_type\" in type_dict[a.ctype] or \"v_v_type\" in type_dict[a.ctype]) and \"O\" in a.out:\n            return a.name + \"Array\"\n    return a.name\ndef make_swift_extension(args):\n    for a in args:\n        if extension_arg(a):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "make_swift_extension",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def make_swift_extension(args):\n    for a in args:\n        if extension_arg(a):\n            return True\n    return False\ndef build_swift_signature(args):\n    swift_signature = \"\"\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_swift_signature",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_swift_signature(args):\n    swift_signature = \"\"\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue\n        if not a.ctype:  # hidden",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_unrefined_call",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_unrefined_call(name, args, constructor, static, classname, has_ret):\n    swift_refine_call = (\"let ret = \" if has_ret and not constructor else \"\") + ((classname + \".\") if static else \"\") + (name if not constructor else \"self.init\")\n    call_args = []\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "build_swift_logues",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def build_swift_logues(args):\n    prologue = []\n    epilogue = []\n    for a in args:\n        if a.ctype not in type_dict:\n            if not a.defval and a.ctype.endswith(\"*\"):\n                a.defval = 0\n            if a.defval:\n                a.ctype = ''\n                continue",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "add_method_to_dict",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def add_method_to_dict(class_name, fi):\n    static = fi.static if fi.classname else True\n    if (class_name, fi.objc_name) not in method_dict:\n        objc_method_name = (\"+\" if static else \"-\") + fi.objc_name + \":\" + build_objc_method_name(fi.args)\n        method_dict[(class_name, fi.objc_name)] = objc_method_name\ndef see_lookup(objc_class, see):\n    semi_colon = see.find(\"::\")\n    see_class = see[:semi_colon] if semi_colon > 0 else objc_class\n    see_method = see[(semi_colon + 2):] if semi_colon != -1 else see\n    if (see_class, see_method) in method_dict:",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "see_lookup",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def see_lookup(objc_class, see):\n    semi_colon = see.find(\"::\")\n    see_class = see[:semi_colon] if semi_colon > 0 else objc_class\n    see_method = see[(semi_colon + 2):] if semi_colon != -1 else see\n    if (see_class, see_method) in method_dict:\n        method = method_dict[(see_class, see_method)]\n        if see_class == objc_class:\n            return method\n        else:\n            return (\"-\" if method[0] == \"-\" else \"\") + \"[\" + see_class + \" \" + method[1:] + \"]\"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "copy_objc_files",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def copy_objc_files(objc_files_dir, objc_base_path, module_path, include = False):\n    global total_files, updated_files\n    objc_files = []\n    re_filter = re.compile(r'^.+\\.(h|m|mm|swift)$')\n    for root, dirnames, filenames in os.walk(objc_files_dir):\n       objc_files += [os.path.join(root, filename) for filename in filenames if re_filter.match(filename)]\n    objc_files = [f.replace('\\\\', '/') for f in objc_files]\n    re_prefix = re.compile(r'^.+/(.+)\\.(h|m|mm|swift)$')\n    for objc_file in objc_files:\n        src = objc_file",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "unescape",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def unescape(str):\n    return str.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\ndef escape_underscore(str):\n    return str.replace('_', '\\\\_')\ndef escape_texttt(str):\n    return re.sub(re.compile('texttt{(.*?)\\}', re.DOTALL), lambda x: 'texttt{' + escape_underscore(x.group(1)) + '}', str)\ndef get_macros(tex):\n    out = \"\"\n    if re.search(\"\\\\\\\\fork\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\fork}[4]{ \\\\left\\\\{ \\\\begin{array}{l l} #1 & \\\\text{#2}\\\\\\\\\\\\\\\\ #3 & \\\\text{#4}\\\\\\\\\\\\\\\\ \\\\end{array} \\\\right.} \"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "escape_underscore",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def escape_underscore(str):\n    return str.replace('_', '\\\\_')\ndef escape_texttt(str):\n    return re.sub(re.compile('texttt{(.*?)\\}', re.DOTALL), lambda x: 'texttt{' + escape_underscore(x.group(1)) + '}', str)\ndef get_macros(tex):\n    out = \"\"\n    if re.search(\"\\\\\\\\fork\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\fork}[4]{ \\\\left\\\\{ \\\\begin{array}{l l} #1 & \\\\text{#2}\\\\\\\\\\\\\\\\ #3 & \\\\text{#4}\\\\\\\\\\\\\\\\ \\\\end{array} \\\\right.} \"\n    if re.search(\"\\\\\\\\vecthreethree\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\vecthreethree}[9]{ \\\\begin{bmatrix} #1 & #2 & #3\\\\\\\\\\\\\\\\ #4 & #5 & #6\\\\\\\\\\\\\\\\ #7 & #8 & #9 \\\\end{bmatrix} } \"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "escape_texttt",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def escape_texttt(str):\n    return re.sub(re.compile('texttt{(.*?)\\}', re.DOTALL), lambda x: 'texttt{' + escape_underscore(x.group(1)) + '}', str)\ndef get_macros(tex):\n    out = \"\"\n    if re.search(\"\\\\\\\\fork\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\fork}[4]{ \\\\left\\\\{ \\\\begin{array}{l l} #1 & \\\\text{#2}\\\\\\\\\\\\\\\\ #3 & \\\\text{#4}\\\\\\\\\\\\\\\\ \\\\end{array} \\\\right.} \"\n    if re.search(\"\\\\\\\\vecthreethree\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\vecthreethree}[9]{ \\\\begin{bmatrix} #1 & #2 & #3\\\\\\\\\\\\\\\\ #4 & #5 & #6\\\\\\\\\\\\\\\\ #7 & #8 & #9 \\\\end{bmatrix} } \"\n    return out\ndef fix_tex(tex):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "get_macros",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def get_macros(tex):\n    out = \"\"\n    if re.search(\"\\\\\\\\fork\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\fork}[4]{ \\\\left\\\\{ \\\\begin{array}{l l} #1 & \\\\text{#2}\\\\\\\\\\\\\\\\ #3 & \\\\text{#4}\\\\\\\\\\\\\\\\ \\\\end{array} \\\\right.} \"\n    if re.search(\"\\\\\\\\vecthreethree\\s*{\", tex):\n        out += \"\\\\newcommand{\\\\vecthreethree}[9]{ \\\\begin{bmatrix} #1 & #2 & #3\\\\\\\\\\\\\\\\ #4 & #5 & #6\\\\\\\\\\\\\\\\ #7 & #8 & #9 \\\\end{bmatrix} } \"\n    return out\ndef fix_tex(tex):\n    macros = get_macros(tex)\n    fix_escaping = escape_texttt(unescape(tex))",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "fix_tex",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def fix_tex(tex):\n    macros = get_macros(tex)\n    fix_escaping = escape_texttt(unescape(tex))\n    return macros + fix_escaping\ndef sanitize_documentation_string(doc, type):\n    if type == \"class\":\n        doc = doc.replace(\"@param \", \"\")\n    doc = re.sub(re.compile('`\\\\$\\\\$(.*?)\\\\$\\\\$`', re.DOTALL), lambda x: '`$$' + fix_tex(x.group(1)) + '$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{align\\\\*\\\\}\\\\{?(.*?)\\\\\\\\f\\\\}', re.DOTALL), lambda x: '`$$\\\\begin{aligned} ' + fix_tex(x.group(1)) + ' \\\\end{aligned}$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{equation\\\\*\\\\}\\\\{(.*?)\\\\\\\\f\\\\}', re.DOTALL), lambda x: '`$$\\\\begin{aligned} ' + fix_tex(x.group(1)) + ' \\\\end{aligned}$$`', doc)",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "sanitize_documentation_string",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "def sanitize_documentation_string(doc, type):\n    if type == \"class\":\n        doc = doc.replace(\"@param \", \"\")\n    doc = re.sub(re.compile('`\\\\$\\\\$(.*?)\\\\$\\\\$`', re.DOTALL), lambda x: '`$$' + fix_tex(x.group(1)) + '$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{align\\\\*\\\\}\\\\{?(.*?)\\\\\\\\f\\\\}', re.DOTALL), lambda x: '`$$\\\\begin{aligned} ' + fix_tex(x.group(1)) + ' \\\\end{aligned}$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{equation\\\\*\\\\}\\\\{(.*?)\\\\\\\\f\\\\}', re.DOTALL), lambda x: '`$$\\\\begin{aligned} ' + fix_tex(x.group(1)) + ' \\\\end{aligned}$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\$(.*?)\\\\\\\\f\\\\$', re.DOTALL), lambda x: '`$$' + fix_tex(x.group(1)) + '$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\[(.*?)\\\\\\\\f\\\\]', re.DOTALL), lambda x: '`$$' + fix_tex(x.group(1)) + '$$`', doc)\n    doc = re.sub(re.compile('\\\\\\\\f\\\\{(.*?)\\\\\\\\f\\\\}', re.DOTALL), lambda x: '`$$' + fix_tex(x.group(1)) + '$$`', doc)\n    doc = doc.replace(\"@anchor\", \"\") \\",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "SCRIPT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n# list of modules\nconfig = None\nROOT_DIR = None\ntotal_files = 0\nupdated_files = 0\nmodule_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "config = None\nROOT_DIR = None\ntotal_files = 0\nupdated_files = 0\nmodule_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "ROOT_DIR = None\ntotal_files = 0\nupdated_files = 0\nmodule_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "total_files",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "total_files = 0\nupdated_files = 0\nmodule_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of enum names, which should be skipped by wrapper generator",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "updated_files",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "updated_files = 0\nmodule_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of enum names, which should be skipped by wrapper generator\nenum_ignore_list = []",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "module_imports",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "module_imports = []\n# list of namespaces, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module only\nnamespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of enum names, which should be skipped by wrapper generator\nenum_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "namespace_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "namespace_ignore_list = []\n# list of class names, which should be skipped by wrapper generator\n# the list is loaded from misc/objc/gen_dict.json defined for the module and its dependencies\nclass_ignore_list = []\n# list of enum names, which should be skipped by wrapper generator\nenum_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "class_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "class_ignore_list = []\n# list of enum names, which should be skipped by wrapper generator\nenum_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "enum_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "enum_ignore_list = []\n# list of constant names, which should be skipped by wrapper generator\n# ignored constants can be defined using regular expressions\nconst_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\ntype_dict = {\n    \"\"        : {\"objc_type\" : \"\"}, # c-tor ret_type",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "const_ignore_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "const_ignore_list = []\n# list of private constants\nconst_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\ntype_dict = {\n    \"\"        : {\"objc_type\" : \"\"}, # c-tor ret_type\n    \"void\"    : {\"objc_type\" : \"void\", \"is_primitive\" : True, \"swift_type\": \"Void\"},\n    \"bool\"    : {\"objc_type\" : \"BOOL\", \"is_primitive\" : True, \"to_cpp\": \"(bool)%(n)s\", \"swift_type\": \"Bool\"},\n    \"char\"    : {\"objc_type\" : \"char\", \"is_primitive\" : True, \"swift_type\": \"Int8\"},",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "const_private_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "const_private_list = []\n# { Module : { public : [[name, val],...], private : [[]...] } }\nmissing_consts = {}\ntype_dict = {\n    \"\"        : {\"objc_type\" : \"\"}, # c-tor ret_type\n    \"void\"    : {\"objc_type\" : \"void\", \"is_primitive\" : True, \"swift_type\": \"Void\"},\n    \"bool\"    : {\"objc_type\" : \"BOOL\", \"is_primitive\" : True, \"to_cpp\": \"(bool)%(n)s\", \"swift_type\": \"Bool\"},\n    \"char\"    : {\"objc_type\" : \"char\", \"is_primitive\" : True, \"swift_type\": \"Int8\"},\n    \"int\"     : {\"objc_type\" : \"int\", \"is_primitive\" : True, \"out_type\" : \"int*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(int*)(%(n)s)\", \"swift_type\": \"Int32\"},\n    \"long\"    : {\"objc_type\" : \"long\", \"is_primitive\" : True, \"swift_type\": \"Int\"},",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "missing_consts",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "missing_consts = {}\ntype_dict = {\n    \"\"        : {\"objc_type\" : \"\"}, # c-tor ret_type\n    \"void\"    : {\"objc_type\" : \"void\", \"is_primitive\" : True, \"swift_type\": \"Void\"},\n    \"bool\"    : {\"objc_type\" : \"BOOL\", \"is_primitive\" : True, \"to_cpp\": \"(bool)%(n)s\", \"swift_type\": \"Bool\"},\n    \"char\"    : {\"objc_type\" : \"char\", \"is_primitive\" : True, \"swift_type\": \"Int8\"},\n    \"int\"     : {\"objc_type\" : \"int\", \"is_primitive\" : True, \"out_type\" : \"int*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(int*)(%(n)s)\", \"swift_type\": \"Int32\"},\n    \"long\"    : {\"objc_type\" : \"long\", \"is_primitive\" : True, \"swift_type\": \"Int\"},\n    \"float\"   : {\"objc_type\" : \"float\", \"is_primitive\" : True, \"out_type\" : \"float*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(float*)(%(n)s)\", \"swift_type\": \"Float\"},\n    \"double\"  : {\"objc_type\" : \"double\", \"is_primitive\" : True, \"out_type\" : \"double*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(double*)(%(n)s)\", \"swift_type\": \"Double\"},",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "type_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "type_dict = {\n    \"\"        : {\"objc_type\" : \"\"}, # c-tor ret_type\n    \"void\"    : {\"objc_type\" : \"void\", \"is_primitive\" : True, \"swift_type\": \"Void\"},\n    \"bool\"    : {\"objc_type\" : \"BOOL\", \"is_primitive\" : True, \"to_cpp\": \"(bool)%(n)s\", \"swift_type\": \"Bool\"},\n    \"char\"    : {\"objc_type\" : \"char\", \"is_primitive\" : True, \"swift_type\": \"Int8\"},\n    \"int\"     : {\"objc_type\" : \"int\", \"is_primitive\" : True, \"out_type\" : \"int*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(int*)(%(n)s)\", \"swift_type\": \"Int32\"},\n    \"long\"    : {\"objc_type\" : \"long\", \"is_primitive\" : True, \"swift_type\": \"Int\"},\n    \"float\"   : {\"objc_type\" : \"float\", \"is_primitive\" : True, \"out_type\" : \"float*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(float*)(%(n)s)\", \"swift_type\": \"Float\"},\n    \"double\"  : {\"objc_type\" : \"double\", \"is_primitive\" : True, \"out_type\" : \"double*\", \"out_type_ptr\": \"%(n)s\", \"out_type_ref\": \"*(double*)(%(n)s)\", \"swift_type\": \"Double\"},\n    \"size_t\"  : {\"objc_type\" : \"size_t\", \"is_primitive\" : True},",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "namespaces_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "namespaces_dict = {}\n# { module: { class | \"*\" : [ header ]} }\nAdditionalImports = {}\n# { class : { func : {declaration, implementation} } }\nManualFuncs = {}\n# { class : { func : { arg_name : {\"ctype\" : ctype, \"attrib\" : [attrib]} } } }\nfunc_arg_fix = {}\n# { class : { func : { prolog : \"\", epilog : \"\" } } }\nheader_fix = {}\n# { class : { enum: fixed_enum } }",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "AdditionalImports",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "AdditionalImports = {}\n# { class : { func : {declaration, implementation} } }\nManualFuncs = {}\n# { class : { func : { arg_name : {\"ctype\" : ctype, \"attrib\" : [attrib]} } } }\nfunc_arg_fix = {}\n# { class : { func : { prolog : \"\", epilog : \"\" } } }\nheader_fix = {}\n# { class : { enum: fixed_enum } }\nenum_fix = {}\n# { class : { enum: { const: fixed_const} } }",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "ManualFuncs",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "ManualFuncs = {}\n# { class : { func : { arg_name : {\"ctype\" : ctype, \"attrib\" : [attrib]} } } }\nfunc_arg_fix = {}\n# { class : { func : { prolog : \"\", epilog : \"\" } } }\nheader_fix = {}\n# { class : { enum: fixed_enum } }\nenum_fix = {}\n# { class : { enum: { const: fixed_const} } }\nconst_fix = {}\n# { (class, func) : objc_signature }",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "func_arg_fix",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "func_arg_fix = {}\n# { class : { func : { prolog : \"\", epilog : \"\" } } }\nheader_fix = {}\n# { class : { enum: fixed_enum } }\nenum_fix = {}\n# { class : { enum: { const: fixed_const} } }\nconst_fix = {}\n# { (class, func) : objc_signature }\nmethod_dict = {\n    (\"Mat\", \"convertTo\") : \"-convertTo:rtype:alpha:beta:\",",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "header_fix",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "header_fix = {}\n# { class : { enum: fixed_enum } }\nenum_fix = {}\n# { class : { enum: { const: fixed_const} } }\nconst_fix = {}\n# { (class, func) : objc_signature }\nmethod_dict = {\n    (\"Mat\", \"convertTo\") : \"-convertTo:rtype:alpha:beta:\",\n    (\"Mat\", \"setTo\") : \"-setToScalar:mask:\",\n    (\"Mat\", \"zeros\") : \"+zeros:cols:type:\",",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "enum_fix",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "enum_fix = {}\n# { class : { enum: { const: fixed_const} } }\nconst_fix = {}\n# { (class, func) : objc_signature }\nmethod_dict = {\n    (\"Mat\", \"convertTo\") : \"-convertTo:rtype:alpha:beta:\",\n    (\"Mat\", \"setTo\") : \"-setToScalar:mask:\",\n    (\"Mat\", \"zeros\") : \"+zeros:cols:type:\",\n    (\"Mat\", \"ones\") : \"+ones:cols:type:\",\n    (\"Mat\", \"dot\") : \"-dot:\"",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "const_fix",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "const_fix = {}\n# { (class, func) : objc_signature }\nmethod_dict = {\n    (\"Mat\", \"convertTo\") : \"-convertTo:rtype:alpha:beta:\",\n    (\"Mat\", \"setTo\") : \"-setToScalar:mask:\",\n    (\"Mat\", \"zeros\") : \"+zeros:cols:type:\",\n    (\"Mat\", \"ones\") : \"+ones:cols:type:\",\n    (\"Mat\", \"dot\") : \"-dot:\"\n}\nmodules = []",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "method_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "method_dict = {\n    (\"Mat\", \"convertTo\") : \"-convertTo:rtype:alpha:beta:\",\n    (\"Mat\", \"setTo\") : \"-setToScalar:mask:\",\n    (\"Mat\", \"zeros\") : \"+zeros:cols:type:\",\n    (\"Mat\", \"ones\") : \"+ones:cols:type:\",\n    (\"Mat\", \"dot\") : \"-dot:\"\n}\nmodules = []\nclass SkipSymbolException(Exception):\n    def __init__(self, text):",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "modules",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "modules = []\nclass SkipSymbolException(Exception):\n    def __init__(self, text):\n        self.t = text\n    def __str__(self):\n        return self.t\ndef read_contents(fname):\n    with open(fname, 'r') as f:\n        data = f.read()\n    return data",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "T_OBJC_CLASS_HEADER",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "T_OBJC_CLASS_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_class_header.template'))\nT_OBJC_CLASS_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_class_body.template'))\nT_OBJC_MODULE_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_header.template'))\nT_OBJC_MODULE_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_body.template'))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        for ns_ignore in namespace_ignore_list:\n            if self.symbol_id.startswith(ns_ignore + '.'):\n                raise SkipSymbolException('ignored namespace ({}): {}'.format(ns_ignore, self.symbol_id))",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "T_OBJC_CLASS_BODY",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "T_OBJC_CLASS_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_class_body.template'))\nT_OBJC_MODULE_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_header.template'))\nT_OBJC_MODULE_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_body.template'))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        for ns_ignore in namespace_ignore_list:\n            if self.symbol_id.startswith(ns_ignore + '.'):\n                raise SkipSymbolException('ignored namespace ({}): {}'.format(ns_ignore, self.symbol_id))\n        # parse doxygen comments",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "T_OBJC_MODULE_HEADER",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "T_OBJC_MODULE_HEADER = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_header.template'))\nT_OBJC_MODULE_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_body.template'))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        for ns_ignore in namespace_ignore_list:\n            if self.symbol_id.startswith(ns_ignore + '.'):\n                raise SkipSymbolException('ignored namespace ({}): {}'.format(ns_ignore, self.symbol_id))\n        # parse doxygen comments\n        self.params={}",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "T_OBJC_MODULE_BODY",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "description": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "peekOfCode": "T_OBJC_MODULE_BODY = read_contents(os.path.join(SCRIPT_DIR, 'templates/objc_module_body.template'))\nclass GeneralInfo():\n    def __init__(self, type, decl, namespaces):\n        self.symbol_id, self.namespace, self.classpath, self.classname, self.name = self.parseName(decl[0], namespaces)\n        for ns_ignore in namespace_ignore_list:\n            if self.symbol_id.startswith(ns_ignore + '.'):\n                raise SkipSymbolException('ignored namespace ({}): {}'.format(ns_ignore, self.symbol_id))\n        # parse doxygen comments\n        self.params={}\n        self.deprecated = False",
        "detail": "Hw_2.opencv.modules.objc.generator.gen_objc",
        "documentation": {}
    },
    {
        "label": "facedetect_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "peekOfCode": "class facedetect_test(NewOpenCVTests):\n    def test_facedetect(self):\n        cascade_fn = self.repoPath + '/data/haarcascades/haarcascade_frontalface_alt.xml'\n        nested_fn  = self.repoPath + '/data/haarcascades/haarcascade_eye.xml'\n        cascade = cv.CascadeClassifier(cascade_fn)\n        nested = cv.CascadeClassifier(nested_fn)\n        samples = ['samples/data/lena.jpg', 'cv/cascadeandhog/images/mona-lisa.png']\n        faces = []\n        eyes = []\n        testFaces = [",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "documentation": {}
    },
    {
        "label": "detect",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "peekOfCode": "def detect(img, cascade):\n    rects = cascade.detectMultiScale(img, scaleFactor=1.275, minNeighbors=4, minSize=(30, 30),\n                                     flags=cv.CASCADE_SCALE_IMAGE)\n    if len(rects) == 0:\n        return []\n    rects[:,2:] += rects[:,:2]\n    return rects\nfrom tests_common import NewOpenCVTests, intersectionRate\nclass facedetect_test(NewOpenCVTests):\n    def test_facedetect(self):",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_facedetect",
        "documentation": {}
    },
    {
        "label": "aruco_objdetect_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_objdetect_aruco",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_objdetect_aruco",
        "peekOfCode": "class aruco_objdetect_test(NewOpenCVTests):\n    def test_idsAccessibility(self):\n        ids = np.arange(17)\n        rev_ids = ids[::-1]\n        aruco_dict  = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_5X5_250)\n        board = cv.aruco.CharucoBoard((7, 5), 1, 0.5, aruco_dict)\n        np.testing.assert_array_equal(board.getIds().squeeze(), ids)\n        board = cv.aruco.CharucoBoard((7, 5), 1, 0.5, aruco_dict, rev_ids)\n        np.testing.assert_array_equal(board.getIds().squeeze(), rev_ids)\n        board = cv.aruco.CharucoBoard((7, 5), 1, 0.5, aruco_dict, ids)",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_objdetect_aruco",
        "documentation": {}
    },
    {
        "label": "peopledetect_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "peekOfCode": "class peopledetect_test(NewOpenCVTests):\n    def test_peopledetect(self):\n        hog = cv.HOGDescriptor()\n        hog.setSVMDetector( cv.HOGDescriptor_getDefaultPeopleDetector() )\n        dirPath = 'samples/data/'\n        samples = ['basketball1.png', 'basketball2.png']\n        testPeople = [\n        [[23, 76, 164, 477], [440, 22, 637, 478]],\n        [[23, 76, 164, 477], [440, 22, 637, 478]]\n        ]",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "documentation": {}
    },
    {
        "label": "inside",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "peekOfCode": "def inside(r, q):\n    rx, ry, rw, rh = r\n    qx, qy, qw, qh = q\n    return rx > qx and ry > qy and rx + rw < qx + qw and ry + rh < qy + qh\nfrom tests_common import NewOpenCVTests, intersectionRate\nclass peopledetect_test(NewOpenCVTests):\n    def test_peopledetect(self):\n        hog = cv.HOGDescriptor()\n        hog.setSVMDetector( cv.HOGDescriptor_getDefaultPeopleDetector() )\n        dirPath = 'samples/data/'",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_peopledetect",
        "documentation": {}
    },
    {
        "label": "qrcode_detector_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.objdetect.misc.python.test.test_qrcode_detect",
        "description": "Hw_2.opencv.modules.objdetect.misc.python.test.test_qrcode_detect",
        "peekOfCode": "class qrcode_detector_test(NewOpenCVTests):\n    def test_detect(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/link_ocv.jpg'))\n        self.assertFalse(img is None)\n        detector = cv.QRCodeDetector()\n        retval, points = detector.detect(img)\n        self.assertTrue(retval)\n        self.assertEqual(points.shape, (1, 4, 2))\n    def test_detect_and_decode(self):\n        img = cv.imread(os.path.join(self.extraTestDataPath, 'cv/qrcode/link_ocv.jpg'))",
        "detail": "Hw_2.opencv.modules.objdetect.misc.python.test.test_qrcode_detect",
        "documentation": {}
    },
    {
        "label": "get_ocv_version",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.package.extra_modules.misc.version",
        "description": "Hw_2.opencv.modules.python.package.extra_modules.misc.version",
        "peekOfCode": "def get_ocv_version():\n    return getattr(cv2, \"__version__\", \"unavailable\")",
        "detail": "Hw_2.opencv.modules.python.package.extra_modules.misc.version",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.package.setup",
        "description": "Hw_2.opencv.modules.python.package.setup",
        "peekOfCode": "def main():\n    os.chdir(SCRIPT_DIR)\n    package_name = 'opencv'\n    package_version = os.environ.get('OPENCV_VERSION', '4.7.0')  # TODO\n    long_description = 'Open Source Computer Vision Library Python bindings'  # TODO\n    setuptools.setup(\n        name=package_name,\n        version=package_version,\n        url='https://github.com/opencv/opencv',\n        license='Apache 2.0',",
        "detail": "Hw_2.opencv.modules.python.package.setup",
        "documentation": {}
    },
    {
        "label": "FormatStrings",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class FormatStrings:\n    string = 's'\n    unsigned_char = 'b'\n    short_int = 'h'\n    int = 'i'\n    unsigned_int = 'I'\n    long = 'l'\n    unsigned_long = 'k'\n    long_long = 'L'\n    unsigned_long_long = 'K'",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ClassProp",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class ClassProp(object):\n    def __init__(self, decl):\n        self.tp = decl[0].replace(\"*\", \"_ptr\")\n        self.name = decl[1]\n        self.default_value = decl[2]\n        self.readonly = True\n        if \"/RW\" in decl[3]:\n            self.readonly = False\n    @property\n    def export_name(self):",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ClassInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class ClassInfo(object):\n    def __init__(self, name, decl=None, codegen=None):\n        # Scope name can be a module or other class e.g. cv::SimpleBlobDetector::Params\n        scope_name, self.original_name = name.rsplit(\".\", 1)\n        # In case scope refer the outer class exported with different name\n        if codegen:\n            scope_name = codegen.get_export_scope_name(scope_name)\n        self.scope_name = re.sub(r\"^cv\\.?\", \"\", scope_name)\n        self.export_name = self.original_name\n        self.class_id = normalize_class_name(name)",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ArgInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class ArgInfo(object):\n    def __init__(self, atype, name, default_value, modifiers=(),\n                 enclosing_arg=None):\n        # type: (ArgInfo, str, str, str, tuple[str, ...], ArgInfo | None) -> None\n        self.tp = handle_ptr(atype)\n        self.name = name\n        self.defval = default_value\n        self._modifiers = tuple(modifiers)\n        self.isarray = False\n        self.is_smart_ptr = self.tp.startswith('Ptr<')  # FIXIT: handle through modifiers - need to modify parser",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "FuncVariant",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class FuncVariant(object):\n    def __init__(self, namespace, classname, name, decl, isconstructor, known_classes, isphantom=False):\n        self.name = self.wname = name\n        self.isconstructor = isconstructor\n        self.isphantom = isphantom\n        self.docstring = decl[5]\n        self.rettype = decl[4] or handle_ptr(decl[1])\n        if self.rettype == \"void\":\n            self.rettype = \"\"\n        self.args = []",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "FuncInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class FuncInfo(object):\n    def __init__(self, classname, name, cname, isconstructor, namespace, is_static):\n        self.classname = classname\n        self.name = name\n        self.cname = cname\n        self.isconstructor = isconstructor\n        self.namespace = namespace\n        self.is_static = is_static\n        self.variants = []\n    def add_variant(self, decl, known_classes, isphantom=False):",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class Namespace(object):\n    def __init__(self):\n        self.funcs = {}\n        self.consts = {}\nclass PythonWrapperGenerator(object):\n    def __init__(self):\n        self.clear()\n    def clear(self):\n        self.classes = {}\n        self.namespaces = {}",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "PythonWrapperGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "class PythonWrapperGenerator(object):\n    def __init__(self):\n        self.clear()\n    def clear(self):\n        self.classes = {}\n        self.namespaces = {}\n        self.consts = {}\n        self.enums = {}\n        self.code_include = StringIO()\n        self.code_enums = StringIO()",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "normalize_class_name",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "def normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_type_format_string(arg_type_info):\n    if arg_type_info.strict_conversion:\n        return FormatStrings.object\n    else:\n        return arg_type_info.format_str\nclass ClassProp(object):\n    def __init__(self, decl):\n        self.tp = decl[0].replace(\"*\", \"_ptr\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "get_type_format_string",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "def get_type_format_string(arg_type_info):\n    if arg_type_info.strict_conversion:\n        return FormatStrings.object\n    else:\n        return arg_type_info.format_str\nclass ClassProp(object):\n    def __init__(self, decl):\n        self.tp = decl[0].replace(\"*\", \"_ptr\")\n        self.name = decl[1]\n        self.default_value = decl[2]",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "handle_ptr",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "def handle_ptr(tp):\n    if tp.startswith('Ptr_'):\n        tp = 'Ptr<' + \"::\".join(tp.split('_')[1:]) + '>'\n    return tp\nclass ArgInfo(object):\n    def __init__(self, atype, name, default_value, modifiers=(),\n                 enclosing_arg=None):\n        # type: (ArgInfo, str, str, str, tuple[str, ...], ArgInfo | None) -> None\n        self.tp = handle_ptr(atype)\n        self.name = name",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "find_argument_class_info",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "def find_argument_class_info(argument_type, function_namespace,\n                            function_class_name, known_classes):\n    # type: (str, str, str, dict[str, ClassInfo]) -> ClassInfo | None\n    \"\"\"Tries to find corresponding class info for the provided argument type\n    Args:\n        argument_type (str): Function argument type\n        function_namespace (str): Namespace of the function declaration\n        function_class_name (str): Name of the class if function is a method of class\n        known_classes (dict[str, ClassInfo]): Mapping between string class\n            identifier and ClassInfo struct.",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "forbidden_arg_types",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "forbidden_arg_types = [\"void*\"]\nignored_arg_types = [\"RNG*\"]\npass_by_val_types = [\"Point*\", \"Point2f*\", \"Rect*\", \"String*\", \"double*\", \"float*\", \"int*\"]\ngen_template_check_self = Template(\"\"\"\n    ${cname} * self1 = 0;\n    if (!pyopencv_${name}_getp(self, self1))\n        return failmsgp(\"Incorrect type of self (must be '${name}' or its derivative)\");\n    ${pname} _self_ = ${cvt}(self1);\n\"\"\")\ngen_template_call_constructor_prelude = Template(\"\"\"new (&(self->v)) Ptr<$cname>(); // init Ptr with placement new",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ignored_arg_types",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "ignored_arg_types = [\"RNG*\"]\npass_by_val_types = [\"Point*\", \"Point2f*\", \"Rect*\", \"String*\", \"double*\", \"float*\", \"int*\"]\ngen_template_check_self = Template(\"\"\"\n    ${cname} * self1 = 0;\n    if (!pyopencv_${name}_getp(self, self1))\n        return failmsgp(\"Incorrect type of self (must be '${name}' or its derivative)\");\n    ${pname} _self_ = ${cvt}(self1);\n\"\"\")\ngen_template_call_constructor_prelude = Template(\"\"\"new (&(self->v)) Ptr<$cname>(); // init Ptr with placement new\n        if(self) \"\"\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "pass_by_val_types",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "pass_by_val_types = [\"Point*\", \"Point2f*\", \"Rect*\", \"String*\", \"double*\", \"float*\", \"int*\"]\ngen_template_check_self = Template(\"\"\"\n    ${cname} * self1 = 0;\n    if (!pyopencv_${name}_getp(self, self1))\n        return failmsgp(\"Incorrect type of self (must be '${name}' or its derivative)\");\n    ${pname} _self_ = ${cvt}(self1);\n\"\"\")\ngen_template_call_constructor_prelude = Template(\"\"\"new (&(self->v)) Ptr<$cname>(); // init Ptr with placement new\n        if(self) \"\"\")\ngen_template_call_constructor = Template(\"\"\"self->v.reset(new ${cname}${py_args})\"\"\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_check_self",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_check_self = Template(\"\"\"\n    ${cname} * self1 = 0;\n    if (!pyopencv_${name}_getp(self, self1))\n        return failmsgp(\"Incorrect type of self (must be '${name}' or its derivative)\");\n    ${pname} _self_ = ${cvt}(self1);\n\"\"\")\ngen_template_call_constructor_prelude = Template(\"\"\"new (&(self->v)) Ptr<$cname>(); // init Ptr with placement new\n        if(self) \"\"\")\ngen_template_call_constructor = Template(\"\"\"self->v.reset(new ${cname}${py_args})\"\"\")\ngen_template_simple_call_constructor_prelude = Template(\"\"\"if(self) \"\"\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_call_constructor_prelude",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_call_constructor_prelude = Template(\"\"\"new (&(self->v)) Ptr<$cname>(); // init Ptr with placement new\n        if(self) \"\"\")\ngen_template_call_constructor = Template(\"\"\"self->v.reset(new ${cname}${py_args})\"\"\")\ngen_template_simple_call_constructor_prelude = Template(\"\"\"if(self) \"\"\")\ngen_template_simple_call_constructor = Template(\"\"\"new (&(self->v)) ${cname}${py_args}\"\"\")\ngen_template_parse_args = Template(\"\"\"const char* keywords[] = { $kw_list, NULL };\n    if( PyArg_ParseTupleAndKeywords(py_args, kw, \"$fmtspec\", (char**)keywords, $parse_arglist)$code_cvt )\"\"\")\ngen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_call_constructor",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_call_constructor = Template(\"\"\"self->v.reset(new ${cname}${py_args})\"\"\")\ngen_template_simple_call_constructor_prelude = Template(\"\"\"if(self) \"\"\")\ngen_template_simple_call_constructor = Template(\"\"\"new (&(self->v)) ${cname}${py_args}\"\"\")\ngen_template_parse_args = Template(\"\"\"const char* keywords[] = { $kw_list, NULL };\n    if( PyArg_ParseTupleAndKeywords(py_args, kw, \"$fmtspec\", (char**)keywords, $parse_arglist)$code_cvt )\"\"\")\ngen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {\n        ${code_prelude}ERRWRAP2($code_fcall);\n        $code_ret;",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_simple_call_constructor_prelude",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_simple_call_constructor_prelude = Template(\"\"\"if(self) \"\"\")\ngen_template_simple_call_constructor = Template(\"\"\"new (&(self->v)) ${cname}${py_args}\"\"\")\ngen_template_parse_args = Template(\"\"\"const char* keywords[] = { $kw_list, NULL };\n    if( PyArg_ParseTupleAndKeywords(py_args, kw, \"$fmtspec\", (char**)keywords, $parse_arglist)$code_cvt )\"\"\")\ngen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {\n        ${code_prelude}ERRWRAP2($code_fcall);\n        $code_ret;\n    }",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_simple_call_constructor",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_simple_call_constructor = Template(\"\"\"new (&(self->v)) ${cname}${py_args}\"\"\")\ngen_template_parse_args = Template(\"\"\"const char* keywords[] = { $kw_list, NULL };\n    if( PyArg_ParseTupleAndKeywords(py_args, kw, \"$fmtspec\", (char**)keywords, $parse_arglist)$code_cvt )\"\"\")\ngen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {\n        ${code_prelude}ERRWRAP2($code_fcall);\n        $code_ret;\n    }\n\"\"\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_parse_args",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_parse_args = Template(\"\"\"const char* keywords[] = { $kw_list, NULL };\n    if( PyArg_ParseTupleAndKeywords(py_args, kw, \"$fmtspec\", (char**)keywords, $parse_arglist)$code_cvt )\"\"\")\ngen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {\n        ${code_prelude}ERRWRAP2($code_fcall);\n        $code_ret;\n    }\n\"\"\")\ngen_template_mappable = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_func_body",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_func_body = Template(\"\"\"$code_decl\n    $code_parse\n    {\n        ${code_prelude}ERRWRAP2($code_fcall);\n        $code_ret;\n    }\n\"\"\")\ngen_template_mappable = Template(\"\"\"\n    {\n        ${mappable} _src;",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_mappable",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_mappable = Template(\"\"\"\n    {\n        ${mappable} _src;\n        if (pyopencv_to_safe(src, _src, info))\n        {\n            return cv_mappable_to(_src, dst);\n        }\n    }\n\"\"\")\ngen_template_type_decl = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_type_decl",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_type_decl = Template(\"\"\"\n// Converter (${name})\ntemplate<>\nstruct PyOpenCV_Converter< ${cname} >\n{\n    static PyObject* from(const ${cname}& r)\n    {\n        return pyopencv_${name}_Instance(r);\n    }\n    static bool to(PyObject* src, ${cname}& dst, const ArgInfo& info)",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_map_type_cvt",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_map_type_cvt = Template(\"\"\"\ntemplate<> bool pyopencv_to(PyObject* src, ${cname}& dst, const ArgInfo& info);\n\"\"\")\ngen_template_set_prop_from_map = Template(\"\"\"\n    if( PyMapping_HasKeyString(src, (char*)\"$propname\") )\n    {\n        tmp = PyMapping_GetItemString(src, (char*)\"$propname\");\n        ok = tmp && pyopencv_to_safe(tmp, dst.$propname, ArgInfo(\"$propname\", false));\n        Py_DECREF(tmp);\n        if(!ok) return false;",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_set_prop_from_map",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_set_prop_from_map = Template(\"\"\"\n    if( PyMapping_HasKeyString(src, (char*)\"$propname\") )\n    {\n        tmp = PyMapping_GetItemString(src, (char*)\"$propname\");\n        ok = tmp && pyopencv_to_safe(tmp, dst.$propname, ArgInfo(\"$propname\", false));\n        Py_DECREF(tmp);\n        if(!ok) return false;\n    }\"\"\")\ngen_template_type_impl = Template(\"\"\"\n// GetSet (${name})",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_type_impl",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_type_impl = Template(\"\"\"\n// GetSet (${name})\n${getset_code}\n// Methods (${name})\n${methods_code}\n// Tables (${name})\nstatic PyGetSetDef pyopencv_${name}_getseters[] =\n{${getset_inits}\n    {NULL}  /* Sentinel */\n};",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_get_prop",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_get_prop = Template(\"\"\"\nstatic PyObject* pyopencv_${name}_get_${member}(pyopencv_${name}_t* p, void *closure)\n{\n    return pyopencv_from(p->v${access}${member});\n}\n\"\"\")\ngen_template_get_prop_algo = Template(\"\"\"\nstatic PyObject* pyopencv_${name}_get_${member}(pyopencv_${name}_t* p, void *closure)\n{\n    $cname* _self_ = dynamic_cast<$cname*>(p->v.get());",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_get_prop_algo",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_get_prop_algo = Template(\"\"\"\nstatic PyObject* pyopencv_${name}_get_${member}(pyopencv_${name}_t* p, void *closure)\n{\n    $cname* _self_ = dynamic_cast<$cname*>(p->v.get());\n    if (!_self_)\n        return failmsgp(\"Incorrect type of object (must be '${name}' or its derivative)\");\n    return pyopencv_from(_self_${access}${member});\n}\n\"\"\")\ngen_template_set_prop = Template(\"\"\"",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_set_prop",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_set_prop = Template(\"\"\"\nstatic int pyopencv_${name}_set_${member}(pyopencv_${name}_t* p, PyObject *value, void *closure)\n{\n    if (!value)\n    {\n        PyErr_SetString(PyExc_TypeError, \"Cannot delete the ${member} attribute\");\n        return -1;\n    }\n    return pyopencv_to_safe(value, p->v${access}${member}, ArgInfo(\"value\", false)) ? 0 : -1;\n}",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_set_prop_algo",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_set_prop_algo = Template(\"\"\"\nstatic int pyopencv_${name}_set_${member}(pyopencv_${name}_t* p, PyObject *value, void *closure)\n{\n    if (!value)\n    {\n        PyErr_SetString(PyExc_TypeError, \"Cannot delete the ${member} attribute\");\n        return -1;\n    }\n    $cname* _self_ = dynamic_cast<$cname*>(p->v.get());\n    if (!_self_)",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_prop_init",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_prop_init = Template(\"\"\"\n    {(char*)\"${export_member_name}\", (getter)pyopencv_${name}_get_${member}, NULL, (char*)\"${export_member_name}\", NULL},\"\"\")\ngen_template_rw_prop_init = Template(\"\"\"\n    {(char*)\"${export_member_name}\", (getter)pyopencv_${name}_get_${member}, (setter)pyopencv_${name}_set_${member}, (char*)\"${export_member_name}\", NULL},\"\"\")\ngen_template_overloaded_function_call = Template(\"\"\"\n    {\n${variant}\n        pyPopulateArgumentConversionErrors();\n    }\n\"\"\")",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_rw_prop_init",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_rw_prop_init = Template(\"\"\"\n    {(char*)\"${export_member_name}\", (getter)pyopencv_${name}_get_${member}, (setter)pyopencv_${name}_set_${member}, (char*)\"${export_member_name}\", NULL},\"\"\")\ngen_template_overloaded_function_call = Template(\"\"\"\n    {\n${variant}\n        pyPopulateArgumentConversionErrors();\n    }\n\"\"\")\nclass FormatStrings:\n    string = 's'",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "gen_template_overloaded_function_call",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "gen_template_overloaded_function_call = Template(\"\"\"\n    {\n${variant}\n        pyPopulateArgumentConversionErrors();\n    }\n\"\"\")\nclass FormatStrings:\n    string = 's'\n    unsigned_char = 'b'\n    short_int = 'h'",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ArgTypeInfo",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "ArgTypeInfo = namedtuple('ArgTypeInfo',\n                        ['atype', 'format_str', 'default_value',\n                         'strict_conversion'])\n# strict_conversion is False by default\nArgTypeInfo.__new__.__defaults__ = (False,)\nsimple_argtype_mapping = {\n    \"bool\": ArgTypeInfo(\"bool\", FormatStrings.unsigned_char, \"0\", True),\n    \"size_t\": ArgTypeInfo(\"size_t\", FormatStrings.unsigned_long_long, \"0\", True),\n    \"int\": ArgTypeInfo(\"int\", FormatStrings.int, \"0\", True),\n    \"float\": ArgTypeInfo(\"float\", FormatStrings.float, \"0.f\", True),",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "ArgTypeInfo.__new__.__defaults__",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "ArgTypeInfo.__new__.__defaults__ = (False,)\nsimple_argtype_mapping = {\n    \"bool\": ArgTypeInfo(\"bool\", FormatStrings.unsigned_char, \"0\", True),\n    \"size_t\": ArgTypeInfo(\"size_t\", FormatStrings.unsigned_long_long, \"0\", True),\n    \"int\": ArgTypeInfo(\"int\", FormatStrings.int, \"0\", True),\n    \"float\": ArgTypeInfo(\"float\", FormatStrings.float, \"0.f\", True),\n    \"double\": ArgTypeInfo(\"double\", FormatStrings.double, \"0\", True),\n    \"c_string\": ArgTypeInfo(\"char*\", FormatStrings.string, '(char*)\"\"'),\n    \"string\": ArgTypeInfo(\"std::string\", FormatStrings.object, None, True),\n    \"Stream\": ArgTypeInfo(\"Stream\", FormatStrings.object, 'Stream::Null()', True),",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "simple_argtype_mapping",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "simple_argtype_mapping = {\n    \"bool\": ArgTypeInfo(\"bool\", FormatStrings.unsigned_char, \"0\", True),\n    \"size_t\": ArgTypeInfo(\"size_t\", FormatStrings.unsigned_long_long, \"0\", True),\n    \"int\": ArgTypeInfo(\"int\", FormatStrings.int, \"0\", True),\n    \"float\": ArgTypeInfo(\"float\", FormatStrings.float, \"0.f\", True),\n    \"double\": ArgTypeInfo(\"double\", FormatStrings.double, \"0\", True),\n    \"c_string\": ArgTypeInfo(\"char*\", FormatStrings.string, '(char*)\"\"'),\n    \"string\": ArgTypeInfo(\"std::string\", FormatStrings.object, None, True),\n    \"Stream\": ArgTypeInfo(\"Stream\", FormatStrings.object, 'Stream::Null()', True),\n    \"UMat\": ArgTypeInfo(\"UMat\", FormatStrings.object, 'UMat()', True),  # FIXIT: switch to CV_EXPORTS_W_SIMPLE as UMat is already a some kind of smart pointer",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "python_reserved_keywords",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.gen2",
        "description": "Hw_2.opencv.modules.python.src2.gen2",
        "peekOfCode": "python_reserved_keywords = {\n    \"True\", \"None\", \"False\", \"as\", \"assert\", \"def\", \"del\", \"elif\", \"except\", \"exec\",\n    \"finally\", \"from\", \"global\",  \"import\", \"in\", \"is\", \"lambda\", \"nonlocal\",\n    \"pass\", \"print\", \"raise\", \"with\", \"yield\"\n}\ndef normalize_class_name(name):\n    return re.sub(r\"^cv\\.\", \"\", name).replace(\".\", \"_\")\ndef get_type_format_string(arg_type_info):\n    if arg_type_info.strict_conversion:\n        return FormatStrings.object",
        "detail": "Hw_2.opencv.modules.python.src2.gen2",
        "documentation": {}
    },
    {
        "label": "CppHeaderParser",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "description": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "peekOfCode": "class CppHeaderParser(object):\n    def __init__(self, generate_umat_decls=False, generate_gpumat_decls=False):\n        self._generate_umat_decls = generate_umat_decls\n        self._generate_gpumat_decls = generate_gpumat_decls\n        self.BLOCK_TYPE = 0\n        self.BLOCK_NAME = 1\n        self.PROCESS_FLAG = 2\n        self.PUBLIC_SECTION = 3\n        self.CLASS_DECL = 4\n        self.namespaces = set()",
        "detail": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "documentation": {}
    },
    {
        "label": "opencv_hdr_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "description": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "peekOfCode": "opencv_hdr_list = [\n\"../../core/include/opencv2/core.hpp\",\n\"../../core/include/opencv2/core/mat.hpp\",\n\"../../core/include/opencv2/core/ocl.hpp\",\n\"../../flann/include/opencv2/flann/miniflann.hpp\",\n\"../../ml/include/opencv2/ml.hpp\",\n\"../../imgproc/include/opencv2/imgproc.hpp\",\n\"../../calib3d/include/opencv2/calib3d.hpp\",\n\"../../features2d/include/opencv2/features2d.hpp\",\n\"../../video/include/opencv2/video/tracking.hpp\",",
        "detail": "Hw_2.opencv.modules.python.src2.hdr_parser",
        "documentation": {}
    },
    {
        "label": "load_tests",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test",
        "description": "Hw_2.opencv.modules.python.test.test",
        "peekOfCode": "def load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))\n    tests_pattern = os.environ.get('OPENCV_PYTEST_FILTER', 'test_*') + '.py'",
        "detail": "Hw_2.opencv.modules.python.test.test",
        "documentation": {}
    },
    {
        "label": "sys.dont_write_bytecode",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test",
        "description": "Hw_2.opencv.modules.python.test.test",
        "peekOfCode": "sys.dont_write_bytecode = True  # Don't generate .pyc files / __pycache__ directories\nimport os\nimport unittest\n# Python 3 moved urlopen to urllib.requests\ntry:\n    from urllib.request import urlopen\nexcept ImportError:\n    from urllib import urlopen\nfrom tests_common import NewOpenCVTests\nbasedir = os.path.abspath(os.path.dirname(__file__))",
        "detail": "Hw_2.opencv.modules.python.test.test",
        "documentation": {}
    },
    {
        "label": "basedir",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test",
        "description": "Hw_2.opencv.modules.python.test.test",
        "peekOfCode": "basedir = os.path.abspath(os.path.dirname(__file__))\ndef load_tests(loader, tests, pattern):\n    cwd = os.getcwd()\n    config_file = 'opencv_python_tests.cfg'\n    locations = [cwd, basedir]\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            locations += [str(s).strip() for s in f.readlines()]\n    else:\n        print('WARNING: OpenCV tests config file ({}) is missing, running subset of tests'.format(config_file))",
        "detail": "Hw_2.opencv.modules.python.test.test",
        "documentation": {}
    },
    {
        "label": "NewOpenCVTests",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.tests_common",
        "description": "Hw_2.opencv.modules.python.test.tests_common",
        "peekOfCode": "class NewOpenCVTests(unittest.TestCase):\n    # path to local repository folder containing 'samples' folder\n    repoPath = None\n    extraTestDataPath = None\n    # github repository url\n    repoUrl = 'https://raw.github.com/opencv/opencv/4.x'\n    def find_file(self, filename, searchPaths=[], required=True):\n        searchPaths = searchPaths if searchPaths else [self.repoPath, self.extraTestDataPath]\n        for path in searchPaths:\n            if path is not None:",
        "detail": "Hw_2.opencv.modules.python.test.tests_common",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.tests_common",
        "description": "Hw_2.opencv.modules.python.test.tests_common",
        "peekOfCode": "def intersectionRate(s1, s2):\n    x1, y1, x2, y2 = s1\n    s1 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    x1, y1, x2, y2 = s2\n    s2 = np.array([[x1, y1], [x2,y1], [x2, y2], [x1, y2]])\n    area, _intersection = cv.intersectConvexConvex(s1, s2)\n    return 2 * area / (cv.contourArea(s1) + cv.contourArea(s2))\ndef isPointInRect(p, rect):\n    if rect[0] <= p[0] and rect[1] <=p[1] and p[0] <= rect[2] and p[1] <= rect[3]:\n        return True",
        "detail": "Hw_2.opencv.modules.python.test.tests_common",
        "documentation": {}
    },
    {
        "label": "isPointInRect",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.tests_common",
        "description": "Hw_2.opencv.modules.python.test.tests_common",
        "peekOfCode": "def isPointInRect(p, rect):\n    if rect[0] <= p[0] and rect[1] <=p[1] and p[0] <= rect[2] and p[1] <= rect[3]:\n        return True\n    else:\n        return False",
        "detail": "Hw_2.opencv.modules.python.test.tests_common",
        "documentation": {}
    },
    {
        "label": "#sys.OpenCV_LOADER_DEBUG",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.tests_common",
        "description": "Hw_2.opencv.modules.python.test.tests_common",
        "peekOfCode": "#sys.OpenCV_LOADER_DEBUG = True\nimport cv2 as cv\n# Python 3 moved urlopen to urllib.requests\ntry:\n    from urllib.request import urlopen\nexcept ImportError:\n    from urllib import urlopen\nclass NewOpenCVTests(unittest.TestCase):\n    # path to local repository folder containing 'samples' folder\n    repoPath = None",
        "detail": "Hw_2.opencv.modules.python.test.tests_common",
        "documentation": {}
    },
    {
        "label": "algorithm_rw_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_algorithm_rw",
        "description": "Hw_2.opencv.modules.python.test.test_algorithm_rw",
        "peekOfCode": "class algorithm_rw_test(NewOpenCVTests):\n    def test_algorithm_rw(self):\n        fd, fname = tempfile.mkstemp(prefix=\"opencv_python_algorithm_\", suffix=\".yml\")\n        os.close(fd)\n        # some arbitrary non-default parameters\n        gold = cv.AKAZE_create(descriptor_size=1, descriptor_channels=2, nOctaves=3, threshold=4.0)\n        gold.write(cv.FileStorage(fname, cv.FILE_STORAGE_WRITE), \"AKAZE\")\n        fs = cv.FileStorage(fname, cv.FILE_STORAGE_READ)\n        algorithm = cv.AKAZE_create()\n        algorithm.read(fs.getNode(\"AKAZE\"))",
        "detail": "Hw_2.opencv.modules.python.test.test_algorithm_rw",
        "documentation": {}
    },
    {
        "label": "AsyncTest",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_async",
        "description": "Hw_2.opencv.modules.python.test.test_async",
        "peekOfCode": "class AsyncTest(NewOpenCVTests):\n    def test_async_simple(self):\n        m = np.array([[1,2],[3,4],[5,6]])\n        async_result = cv.utils.testAsyncArray(m)\n        self.assertTrue(async_result.valid())\n        ret, result = async_result.get(timeoutNs=10**6)  # 1ms\n        self.assertTrue(ret)\n        self.assertFalse(async_result.valid())\n        self.assertEqual(cv.norm(m, result, cv.NORM_INF), 0)\n    def test_async_exception(self):",
        "detail": "Hw_2.opencv.modules.python.test.test_async",
        "documentation": {}
    },
    {
        "label": "camshift_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_camshift",
        "description": "Hw_2.opencv.modules.python.test.test_camshift",
        "peekOfCode": "class camshift_test(NewOpenCVTests):\n    framesNum = 300\n    frame = None\n    selection = None\n    drag_start = None\n    show_backproj = False\n    track_window = None\n    render = None\n    errors = 0\n    def prepareRender(self):",
        "detail": "Hw_2.opencv.modules.python.test.test_camshift",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_camshift",
        "description": "Hw_2.opencv.modules.python.test.test_camshift",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nfrom tst_scene_render import TestSceneRender\nfrom tests_common import NewOpenCVTests, intersectionRate\nclass camshift_test(NewOpenCVTests):\n    framesNum = 300\n    frame = None",
        "detail": "Hw_2.opencv.modules.python.test.test_camshift",
        "documentation": {}
    },
    {
        "label": "copytomask_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_copytomask",
        "description": "Hw_2.opencv.modules.python.test.test_copytomask",
        "peekOfCode": "class copytomask_test(NewOpenCVTests):\n    def test_copytomask(self):\n        img = self.get_sample('python/images/baboon.png', cv.IMREAD_COLOR)\n        eps = 0.\n        #Create mask using inRange\n        valeurBGRinf = np.array([0,0,100])\n        valeurBGRSup = np.array([70, 70,255])\n        maskRed = cv.inRange(img, valeurBGRinf, valeurBGRSup)\n        #New binding\n        dstcv = np.ndarray(np.array((2, 2, 1))*img.shape, dtype=img.dtype)",
        "detail": "Hw_2.opencv.modules.python.test.test_copytomask",
        "documentation": {}
    },
    {
        "label": "cuda_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_cuda",
        "description": "Hw_2.opencv.modules.python.test.test_cuda",
        "peekOfCode": "class cuda_test(NewOpenCVTests):\n    def setUp(self):\n        super(cuda_test, self).setUp()\n        if not cv.cuda.getCudaEnabledDeviceCount():\n            self.skipTest(\"No CUDA-capable device is detected\")\n    def test_cuda_upload_download(self):\n        npMat = (np.random.random((128, 128, 3)) * 255).astype(np.uint8)\n        cuMat = cv.cuda_GpuMat()\n        cuMat.upload(npMat)\n        self.assertTrue(np.allclose(cuMat.download(), npMat))",
        "detail": "Hw_2.opencv.modules.python.test.test_cuda",
        "documentation": {}
    },
    {
        "label": "dft_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_dft",
        "description": "Hw_2.opencv.modules.python.test.test_dft",
        "peekOfCode": "class dft_test(NewOpenCVTests):\n    def test_dft(self):\n        img = self.get_sample('samples/data/rubberwhale1.png', 0)\n        eps = 0.001\n        #test direct transform\n        refDft = np.fft.fft2(img)\n        refDftShift = np.fft.fftshift(refDft)\n        refMagnitide = np.log(1.0 + np.abs(refDftShift))\n        testDft = cv.dft(np.float32(img),flags = cv.DFT_COMPLEX_OUTPUT)\n        testDftShift = np.fft.fftshift(testDft)",
        "detail": "Hw_2.opencv.modules.python.test.test_dft",
        "documentation": {}
    },
    {
        "label": "Features2D_Tests",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_features2d",
        "description": "Hw_2.opencv.modules.python.test.test_features2d",
        "peekOfCode": "class Features2D_Tests(NewOpenCVTests):\n    def test_issue_13406(self):\n        self.assertEqual(True, hasattr(cv, 'drawKeypoints'))\n        self.assertEqual(True, hasattr(cv, 'DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS'))\n        self.assertEqual(True, hasattr(cv, 'DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS'))\nif __name__ == '__main__':\n    NewOpenCVTests.bootstrap()",
        "detail": "Hw_2.opencv.modules.python.test.test_features2d",
        "documentation": {}
    },
    {
        "label": "MyData",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "description": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "peekOfCode": "class MyData:\n    def __init__(self):\n        self.A = 97\n        self.X = np.pi\n        self.name = 'mydata1234'\n    def write(self, fs, name):\n        fs.startWriteStruct(name, cv.FileNode_MAP|cv.FileNode_FLOW)\n        fs.write('A', self.A)\n        fs.write('X', self.X)\n        fs.write('name', self.name)",
        "detail": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "documentation": {}
    },
    {
        "label": "filestorage_io_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "description": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "peekOfCode": "class filestorage_io_test(NewOpenCVTests):\n    strings_data = ['image1.jpg', 'Awesomeness', '../data/baboon.jpg']\n    R0 = np.eye(3,3)\n    T0 = np.zeros((3,1))\n    def write_data(self, fname):\n        fs = cv.FileStorage(fname, cv.FileStorage_WRITE)\n        R = self.R0\n        T = self.T0\n        m = MyData()\n        fs.write('iterationNr', 100)",
        "detail": "Hw_2.opencv.modules.python.test.test_filestorage_io",
        "documentation": {}
    },
    {
        "label": "fitline_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_fitline",
        "description": "Hw_2.opencv.modules.python.test.test_fitline",
        "peekOfCode": "class fitline_test(NewOpenCVTests):\n    def test_fitline(self):\n        noise = 5\n        n = 200\n        r = 5 / 100.0\n        outn = int(n*r)\n        p0, p1 = (90, 80), (w-90, h-80)\n        line_points = sample_line(p0, p1, n-outn, noise)\n        outliers = np.random.rand(outn, 2) * (w, h)\n        points = np.vstack([line_points, outliers])",
        "detail": "Hw_2.opencv.modules.python.test.test_fitline",
        "documentation": {}
    },
    {
        "label": "toint",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_fitline",
        "description": "Hw_2.opencv.modules.python.test.test_fitline",
        "peekOfCode": "def toint(p):\n    return tuple(map(int, p))\ndef sample_line(p1, p2, n, noise=0.0):\n    np.random.seed(10)\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise\ndist_func_names = ['DIST_L2', 'DIST_L1', 'DIST_L12', 'DIST_FAIR', 'DIST_WELSCH', 'DIST_HUBER']\nclass fitline_test(NewOpenCVTests):\n    def test_fitline(self):",
        "detail": "Hw_2.opencv.modules.python.test.test_fitline",
        "documentation": {}
    },
    {
        "label": "sample_line",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_fitline",
        "description": "Hw_2.opencv.modules.python.test.test_fitline",
        "peekOfCode": "def sample_line(p1, p2, n, noise=0.0):\n    np.random.seed(10)\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise\ndist_func_names = ['DIST_L2', 'DIST_L1', 'DIST_L12', 'DIST_FAIR', 'DIST_WELSCH', 'DIST_HUBER']\nclass fitline_test(NewOpenCVTests):\n    def test_fitline(self):\n        noise = 5\n        n = 200",
        "detail": "Hw_2.opencv.modules.python.test.test_fitline",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_fitline",
        "description": "Hw_2.opencv.modules.python.test.test_fitline",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nimport numpy as np\nimport cv2 as cv\nfrom tests_common import NewOpenCVTests\nw, h = 512, 256\ndef toint(p):\n    return tuple(map(int, p))\ndef sample_line(p1, p2, n, noise=0.0):\n    np.random.seed(10)\n    p1 = np.float32(p1)",
        "detail": "Hw_2.opencv.modules.python.test.test_fitline",
        "documentation": {}
    },
    {
        "label": "dist_func_names",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_fitline",
        "description": "Hw_2.opencv.modules.python.test.test_fitline",
        "peekOfCode": "dist_func_names = ['DIST_L2', 'DIST_L1', 'DIST_L12', 'DIST_FAIR', 'DIST_WELSCH', 'DIST_HUBER']\nclass fitline_test(NewOpenCVTests):\n    def test_fitline(self):\n        noise = 5\n        n = 200\n        r = 5 / 100.0\n        outn = int(n*r)\n        p0, p1 = (90, 80), (w-90, h-80)\n        line_points = sample_line(p0, p1, n-outn, noise)\n        outliers = np.random.rand(outn, 2) * (w, h)",
        "detail": "Hw_2.opencv.modules.python.test.test_fitline",
        "documentation": {}
    },
    {
        "label": "get_cache_dir_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_fs_cache_dir",
        "description": "Hw_2.opencv.modules.python.test.test_fs_cache_dir",
        "peekOfCode": "class get_cache_dir_test(NewOpenCVTests):\n    def test_get_cache_dir(self):\n        #New binding\n        path = cv.utils.fs.getCacheDirectoryForDownloads()\n        self.assertTrue(os.path.exists(path))\n        self.assertTrue(os.path.isdir(path))\n    def get_cache_dir_imread_interop(self, ext):\n        path = cv.utils.fs.getCacheDirectoryForDownloads()\n        gold_image = np.ones((16, 16, 3), np.uint8)\n        read_from_file = np.zeros((16, 16, 3), np.uint8)",
        "detail": "Hw_2.opencv.modules.python.test.test_fs_cache_dir",
        "documentation": {}
    },
    {
        "label": "gaussian_mix_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "description": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "peekOfCode": "class gaussian_mix_test(NewOpenCVTests):\n    def test_gaussian_mix(self):\n        np.random.seed(10)\n        cluster_n = 5\n        img_size = 512\n        points, ref_distrs = make_gaussians(cluster_n, img_size)\n        em = cv.ml.EM_create()\n        em.setClustersNumber(cluster_n)\n        em.setCovarianceMatrixType(cv.ml.EM_COV_MAT_GENERIC)\n        em.trainEM(points)",
        "detail": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "documentation": {}
    },
    {
        "label": "make_gaussians",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "description": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "peekOfCode": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)\n        points.append( pts )",
        "detail": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "description": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nfrom numpy import random\nimport cv2 as cv\ndef make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _ in xrange(cluster_n):",
        "detail": "Hw_2.opencv.modules.python.test.test_gaussian_mix",
        "documentation": {}
    },
    {
        "label": "grabcut_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_grabcut",
        "description": "Hw_2.opencv.modules.python.test.test_grabcut",
        "peekOfCode": "class grabcut_test(NewOpenCVTests):\n    def verify(self, mask, exp):\n        maxDiffRatio = 0.02\n        expArea = np.count_nonzero(exp)\n        nonIntersectArea = np.count_nonzero(mask != exp)\n        curRatio = float(nonIntersectArea) / expArea\n        return curRatio < maxDiffRatio\n    def scaleMask(self, mask):\n        return np.where((mask==cv.GC_FGD) + (mask==cv.GC_PR_FGD),255,0).astype('uint8')\n    def test_grabcut(self):",
        "detail": "Hw_2.opencv.modules.python.test.test_grabcut",
        "documentation": {}
    },
    {
        "label": "houghcircles_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "description": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "peekOfCode": "class houghcircles_test(NewOpenCVTests):\n    def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n        img = cv.medianBlur(img, 5)\n        circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)[0]\n        testCircles = [[38, 181, 17.6],\n        [99.7, 166, 13.12],\n        [142.7, 160, 13.52],",
        "detail": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "documentation": {}
    },
    {
        "label": "circleApproximation",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "description": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "peekOfCode": "def circleApproximation(circle):\n    nPoints = 30\n    dPhi = 2*pi / nPoints\n    contour = []\n    for i in range(nPoints):\n        contour.append(([circle[0] + circle[2]*cos(i*dPhi),\n            circle[1] + circle[2]*sin(i*dPhi)]))\n    return np.array(contour).astype(int)\ndef convContoursIntersectiponRate(c1, c2):\n    s1 = cv.contourArea(c1)",
        "detail": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "documentation": {}
    },
    {
        "label": "convContoursIntersectiponRate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "description": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "peekOfCode": "def convContoursIntersectiponRate(c1, c2):\n    s1 = cv.contourArea(c1)\n    s2 = cv.contourArea(c2)\n    s, _ = cv.intersectConvexConvex(c1, c2)\n    return 2*s/(s1+s2)\nclass houghcircles_test(NewOpenCVTests):\n    def test_houghcircles(self):\n        fn = \"samples/data/board.jpg\"\n        src = self.get_sample(fn, 1)\n        img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.modules.python.test.test_houghcircles",
        "documentation": {}
    },
    {
        "label": "houghlines_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_houghlines",
        "description": "Hw_2.opencv.modules.python.test.test_houghlines",
        "peekOfCode": "class houghlines_test(NewOpenCVTests):\n    def test_houghlines(self):\n        fn = \"/samples/data/pic1.png\"\n        src = self.get_sample(fn)\n        dst = cv.Canny(src, 50, 200)\n        lines = cv.HoughLinesP(dst, 1, math.pi/180.0, 40, np.array([]), 50, 10)[:,0,:]\n        eps = 5\n        testLines = [\n            #rect1\n             [ 232,  25, 43, 25],",
        "detail": "Hw_2.opencv.modules.python.test.test_houghlines",
        "documentation": {}
    },
    {
        "label": "linesDiff",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_houghlines",
        "description": "Hw_2.opencv.modules.python.test.test_houghlines",
        "peekOfCode": "def linesDiff(line1, line2):\n    norm1 = cv.norm(line1 - line2, cv.NORM_L2)\n    line3 = line1[2:4] + line1[0:2]\n    norm2 = cv.norm(line3 - line2, cv.NORM_L2)\n    return min(norm1, norm2)\nclass houghlines_test(NewOpenCVTests):\n    def test_houghlines(self):\n        fn = \"/samples/data/pic1.png\"\n        src = self.get_sample(fn)\n        dst = cv.Canny(src, 50, 200)",
        "detail": "Hw_2.opencv.modules.python.test.test_houghlines",
        "documentation": {}
    },
    {
        "label": "kmeans_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_kmeans",
        "description": "Hw_2.opencv.modules.python.test.test_kmeans",
        "peekOfCode": "class kmeans_test(NewOpenCVTests):\n    def test_kmeans(self):\n        np.random.seed(10)\n        cluster_n = 5\n        img_size = 512\n        points, _, clusterSizes = make_gaussians(cluster_n, img_size)\n        term_crit = (cv.TERM_CRITERIA_EPS, 30, 0.1)\n        _ret, labels, centers = cv.kmeans(points, cluster_n, None, term_crit, 10, 0)\n        self.assertEqual(len(centers), cluster_n)\n        offset = 0",
        "detail": "Hw_2.opencv.modules.python.test.test_kmeans",
        "documentation": {}
    },
    {
        "label": "make_gaussians",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_kmeans",
        "description": "Hw_2.opencv.modules.python.test.test_kmeans",
        "peekOfCode": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    sizes = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)",
        "detail": "Hw_2.opencv.modules.python.test.test_kmeans",
        "documentation": {}
    },
    {
        "label": "getMainLabelConfidence",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_kmeans",
        "description": "Hw_2.opencv.modules.python.test.test_kmeans",
        "peekOfCode": "def getMainLabelConfidence(labels, nLabels):\n    n = len(labels)\n    labelsDict = dict.fromkeys(range(nLabels), 0)\n    labelsConfDict = dict.fromkeys(range(nLabels))\n    for i in range(n):\n        labelsDict[labels[i][0]] += 1\n    for i in range(nLabels):\n        labelsConfDict[i] = float(labelsDict[i]) / n\n    return max(labelsConfDict.values())\nclass kmeans_test(NewOpenCVTests):",
        "detail": "Hw_2.opencv.modules.python.test.test_kmeans",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_kmeans",
        "description": "Hw_2.opencv.modules.python.test.test_kmeans",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nfrom tests_common import NewOpenCVTests\ndef make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    sizes = []\n    for _ in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size",
        "detail": "Hw_2.opencv.modules.python.test.test_kmeans",
        "documentation": {}
    },
    {
        "label": "Hackathon244Tests",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_legacy",
        "description": "Hw_2.opencv.modules.python.test.test_legacy",
        "peekOfCode": "class Hackathon244Tests(NewOpenCVTests):\n    def test_int_array(self):\n        a = np.array([-1, 2, -3, 4, -5])\n        absa0 = np.abs(a)\n        self.assertTrue(cv.norm(a, cv.NORM_L1) == 15)\n        absa1 = cv.absdiff(a, 0)\n        self.assertEqual(cv.norm(absa1, absa0, cv.NORM_INF), 0)\n    def test_imencode(self):\n        a = np.zeros((480, 640), dtype=np.uint8)\n        flag, ajpg = cv.imencode(\"img_q90.jpg\", a, [cv.IMWRITE_JPEG_QUALITY, 90])",
        "detail": "Hw_2.opencv.modules.python.test.test_legacy",
        "documentation": {}
    },
    {
        "label": "Bindings",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "class Bindings(NewOpenCVTests):\n    def test_inheritance(self):\n        bm = cv.StereoBM_create()\n        bm.getPreFilterCap()  # from StereoBM\n        bm.getBlockSize()  # from SteroMatcher\n        boost = cv.ml.Boost_create()\n        boost.getBoostType()  # from ml::Boost\n        boost.getMaxDepth()  # from ml::DTrees\n        boost.isClassifier()  # from ml::StatModel\n    def test_raiseGeneralException(self):",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "Arguments",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "class Arguments(NewOpenCVTests):\n    def _try_to_convert(self, conversion, value):\n        try:\n            result = conversion(value).lower()\n        except Exception as e:\n            self.fail(\n                '{} \"{}\" is risen for conversion {} of type {}'.format(\n                    type(e).__name__, e, value, type(value).__name__\n                )\n            )",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "CanUsePurePythonModuleFunction",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "class CanUsePurePythonModuleFunction(NewOpenCVTests):\n    def test_can_get_ocv_version(self):\n        import sys\n        if sys.version_info[0] < 3:\n            raise unittest.SkipTest('Python 2.x is not supported')\n        self.assertEqual(cv.misc.get_ocv_version(), cv.__version__,\n                         \"Can't get package version using Python misc module\")\n    def test_native_method_can_be_patched(self):\n        import sys\n        if sys.version_info[0] < 3:",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "SamplesFindFile",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "class SamplesFindFile(NewOpenCVTests):\n    def test_ExistedFile(self):\n        res = cv.samples.findFile('lena.jpg', False)\n        self.assertNotEqual(res, '')\n    def test_MissingFile(self):\n        res = cv.samples.findFile('non_existed.file', False)\n        self.assertEqual(res, '')\n    def test_MissingFileException(self):\n        try:\n            _res = cv.samples.findFile('non_existed.file', True)",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "is_numeric",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "def is_numeric(dtype):\n    return np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating)\ndef get_limits(dtype):\n    if not is_numeric(dtype):\n        return None, None\n    if np.issubdtype(dtype, np.integer):\n        info = np.iinfo(dtype)\n    else:\n        info = np.finfo(dtype)\n    return info.min, info.max",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "get_limits",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "def get_limits(dtype):\n    if not is_numeric(dtype):\n        return None, None\n    if np.issubdtype(dtype, np.integer):\n        info = np.iinfo(dtype)\n    else:\n        info = np.finfo(dtype)\n    return info.min, info.max\ndef get_conversion_error_msg(value, expected, actual):\n    return 'Conversion \"{}\" of type \"{}\" failed\\nExpected: \"{}\" vs Actual \"{}\"'.format(",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "get_conversion_error_msg",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "def get_conversion_error_msg(value, expected, actual):\n    return 'Conversion \"{}\" of type \"{}\" failed\\nExpected: \"{}\" vs Actual \"{}\"'.format(\n        value, type(value).__name__, expected, actual\n    )\ndef get_no_exception_msg(value):\n    return 'Exception is not risen for {} of type {}'.format(value, type(value).__name__)\nclass Bindings(NewOpenCVTests):\n    def test_inheritance(self):\n        bm = cv.StereoBM_create()\n        bm.getPreFilterCap()  # from StereoBM",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "get_no_exception_msg",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_misc",
        "description": "Hw_2.opencv.modules.python.test.test_misc",
        "peekOfCode": "def get_no_exception_msg(value):\n    return 'Exception is not risen for {} of type {}'.format(value, type(value).__name__)\nclass Bindings(NewOpenCVTests):\n    def test_inheritance(self):\n        bm = cv.StereoBM_create()\n        bm.getPreFilterCap()  # from StereoBM\n        bm.getBlockSize()  # from SteroMatcher\n        boost = cv.ml.Boost_create()\n        boost.getBoostType()  # from ml::Boost\n        boost.getMaxDepth()  # from ml::DTrees",
        "detail": "Hw_2.opencv.modules.python.test.test_misc",
        "documentation": {}
    },
    {
        "label": "morphology_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_morphology",
        "description": "Hw_2.opencv.modules.python.test.test_morphology",
        "peekOfCode": "class morphology_test(NewOpenCVTests):\n    def test_morphology(self):\n        fn = 'samples/data/rubberwhale1.png'\n        img = self.get_sample(fn)\n        modes = ['erode/dilate', 'open/close', 'blackhat/tophat', 'gradient']\n        str_modes = ['ellipse', 'rect', 'cross']\n        referenceHashes = { modes[0]: '071a526425b79e45b4d0d71ef51b0562', modes[1] : '071a526425b79e45b4d0d71ef51b0562',\n            modes[2] : '427e89f581b7df1b60a831b1ed4c8618', modes[3] : '0dd8ad251088a63d0dd022bcdc57361c'}\n        def update(cur_mode):\n            cur_str_mode = str_modes[0]",
        "detail": "Hw_2.opencv.modules.python.test.test_morphology",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_morphology",
        "description": "Hw_2.opencv.modules.python.test.test_morphology",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nimport numpy as np\nimport cv2 as cv\nfrom tests_common import NewOpenCVTests\nclass morphology_test(NewOpenCVTests):\n    def test_morphology(self):\n        fn = 'samples/data/rubberwhale1.png'\n        img = self.get_sample(fn)\n        modes = ['erode/dilate', 'open/close', 'blackhat/tophat', 'gradient']\n        str_modes = ['ellipse', 'rect', 'cross']",
        "detail": "Hw_2.opencv.modules.python.test.test_morphology",
        "documentation": {}
    },
    {
        "label": "mser_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_mser",
        "description": "Hw_2.opencv.modules.python.test.test_mser",
        "peekOfCode": "class mser_test(NewOpenCVTests):\n    def test_mser(self):\n        img = self.get_sample('cv/mser/puzzle.png', 0)\n        smallImg = [\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n         [255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255, 255, 255, 255, 255, 255,   0,   0,   0,   0, 255, 255, 255, 255],",
        "detail": "Hw_2.opencv.modules.python.test.test_mser",
        "documentation": {}
    },
    {
        "label": "norm_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "class norm_test(NewOpenCVTests):\n    def test_norm_for_one_array(self):\n        np.random.seed(123)\n        for norm_type, norm in norm_type_under_test.items():\n            element_types = get_element_types(norm_type)\n            for shape, element_type in product(shapes, element_types):\n                array = generate_vector(shape, element_type)\n                expected = norm(array)\n                actual = cv.norm(array, norm_type)\n                self.assertAlmostEqual(",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_inf",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_inf(x, y=None):\n    def norm(vec):\n        return np.linalg.norm(vec.flatten(), np.inf)\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))\ndef norm_l1(x, y=None):\n    def norm(vec):\n        return np.linalg.norm(vec.flatten(), 1)\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_l1",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_l1(x, y=None):\n    def norm(vec):\n        return np.linalg.norm(vec.flatten(), 1)\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))\ndef norm_l2(x, y=None):\n    def norm(vec):\n        return np.linalg.norm(vec.flatten())\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_l2",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_l2(x, y=None):\n    def norm(vec):\n        return np.linalg.norm(vec.flatten())\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))\ndef norm_l2sqr(x, y=None):\n    def norm(vec):\n        return np.square(vec).sum()\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_l2sqr",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_l2sqr(x, y=None):\n    def norm(vec):\n        return np.square(vec).sum()\n    x = x.astype(np.float64)\n    return norm(x) if y is None else norm(x - y.astype(np.float64))\ndef norm_hamming(x, y=None):\n    def norm(vec):\n        return sum(bin(i).count('1') for i in vec.flatten())\n    return norm(x) if y is None else norm(np.bitwise_xor(x, y))\ndef norm_hamming2(x, y=None):",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_hamming",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_hamming(x, y=None):\n    def norm(vec):\n        return sum(bin(i).count('1') for i in vec.flatten())\n    return norm(x) if y is None else norm(np.bitwise_xor(x, y))\ndef norm_hamming2(x, y=None):\n    def norm(vec):\n        def element_norm(element):\n            binary_str = bin(element).split('b')[-1]\n            if len(binary_str) % 2 == 1:\n                binary_str = '0' + binary_str",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_hamming2",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def norm_hamming2(x, y=None):\n    def norm(vec):\n        def element_norm(element):\n            binary_str = bin(element).split('b')[-1]\n            if len(binary_str) % 2 == 1:\n                binary_str = '0' + binary_str\n            gen = filter(lambda p: p != '00',\n                         (binary_str[i:i+2]\n                          for i in range(0, len(binary_str), 2)))\n            return sum(1 for _ in gen)",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "get_element_types",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def get_element_types(norm_type):\n    if norm_type in (cv.NORM_HAMMING, cv.NORM_HAMMING2):\n        return (np.uint8,)\n    else:\n        return (np.uint8, np.int8, np.uint16, np.int16, np.int32, np.float32,\n                np.float64)\ndef generate_vector(shape, dtype):\n    if np.issubdtype(dtype, np.integer):\n        return np.random.randint(0, 100, shape).astype(dtype)\n    else:",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "generate_vector",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "def generate_vector(shape, dtype):\n    if np.issubdtype(dtype, np.integer):\n        return np.random.randint(0, 100, shape).astype(dtype)\n    else:\n        return np.random.normal(10., 12.5, shape).astype(dtype)\nshapes = (1, 2, 3, 5, 7, 16, (1, 1), (2, 2), (3, 5), (1, 7))\nclass norm_test(NewOpenCVTests):\n    def test_norm_for_one_array(self):\n        np.random.seed(123)\n        for norm_type, norm in norm_type_under_test.items():",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_type_under_test",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "norm_type_under_test = {\n    cv.NORM_INF: norm_inf,\n    cv.NORM_L1: norm_l1,\n    cv.NORM_L2: norm_l2,\n    cv.NORM_L2SQR: norm_l2sqr,\n    cv.NORM_HAMMING: norm_hamming,\n    cv.NORM_HAMMING2: norm_hamming2\n}\nnorm_name = {\n    cv.NORM_INF: 'inf',",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "norm_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "norm_name = {\n    cv.NORM_INF: 'inf',\n    cv.NORM_L1: 'L1',\n    cv.NORM_L2: 'L2',\n    cv.NORM_L2SQR: 'L2SQR',\n    cv.NORM_HAMMING: 'Hamming',\n    cv.NORM_HAMMING2: 'Hamming2'\n}\ndef get_element_types(norm_type):\n    if norm_type in (cv.NORM_HAMMING, cv.NORM_HAMMING2):",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "shapes",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_norm",
        "description": "Hw_2.opencv.modules.python.test.test_norm",
        "peekOfCode": "shapes = (1, 2, 3, 5, 7, 16, (1, 1), (2, 2), (3, 5), (1, 7))\nclass norm_test(NewOpenCVTests):\n    def test_norm_for_one_array(self):\n        np.random.seed(123)\n        for norm_type, norm in norm_type_under_test.items():\n            element_types = get_element_types(norm_type)\n            for shape, element_type in product(shapes, element_types):\n                array = generate_vector(shape, element_type)\n                expected = norm(array)\n                actual = cv.norm(array, norm_type)",
        "detail": "Hw_2.opencv.modules.python.test.test_norm",
        "documentation": {}
    },
    {
        "label": "persistence_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_persistence",
        "description": "Hw_2.opencv.modules.python.test.test_persistence",
        "peekOfCode": "class persistence_test(NewOpenCVTests):\n    def test_yml_rw(self):\n        fd, fname = tempfile.mkstemp(prefix=\"opencv_python_persistence_\", suffix=\".yml\")\n        os.close(fd)\n        # Writing ...\n        expected = np.array([[[0, 1, 2, 3, 4]]])\n        expected_str = (\"Hello\", \"World\", \"!\")\n        fs = cv.FileStorage(fname, cv.FILE_STORAGE_WRITE)\n        fs.write(\"test\", expected)\n        fs.write(\"strings\", expected_str)",
        "detail": "Hw_2.opencv.modules.python.test.test_persistence",
        "documentation": {}
    },
    {
        "label": "squares_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "class squares_test(NewOpenCVTests):\n    def test_squares(self):\n        img = self.get_sample('samples/data/pic1.png')\n        squares = find_squares(img)\n        testSquares = [\n        [[43, 25],\n        [43, 129],\n        [232, 129],\n        [232, 25]],\n        [[252, 87],",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "angle_cos",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "def angle_cos(p0, p1, p2):\n    d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')\n    return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )\ndef find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)\n    squares = []\n    for gray in cv.split(img):\n        for thrs in xrange(0, 255, 26):\n            if thrs == 0:\n                bin = cv.Canny(gray, 0, 50, apertureSize=5)",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "find_squares",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "def find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)\n    squares = []\n    for gray in cv.split(img):\n        for thrs in xrange(0, 255, 26):\n            if thrs == 0:\n                bin = cv.Canny(gray, 0, 50, apertureSize=5)\n                bin = cv.dilate(bin, None)\n            else:\n                _retval, bin = cv.threshold(gray, thrs, 255, cv.THRESH_BINARY)",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "intersectionRate",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "def intersectionRate(s1, s2):\n    area, _intersection = cv.intersectConvexConvex(np.array(s1), np.array(s2))\n    return 2 * area / (cv.contourArea(np.array(s1)) + cv.contourArea(np.array(s2)))\ndef filterSquares(squares, square):\n    for i in range(len(squares)):\n        if intersectionRate(squares[i], square) > 0.95:\n            return False\n    return True\nfrom tests_common import NewOpenCVTests\nclass squares_test(NewOpenCVTests):",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "filterSquares",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "def filterSquares(squares, square):\n    for i in range(len(squares)):\n        if intersectionRate(squares[i], square) > 0.95:\n            return False\n    return True\nfrom tests_common import NewOpenCVTests\nclass squares_test(NewOpenCVTests):\n    def test_squares(self):\n        img = self.get_sample('samples/data/pic1.png')\n        squares = find_squares(img)",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.test_squares",
        "description": "Hw_2.opencv.modules.python.test.test_squares",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\ndef angle_cos(p0, p1, p2):\n    d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')\n    return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )\ndef find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)",
        "detail": "Hw_2.opencv.modules.python.test.test_squares",
        "documentation": {}
    },
    {
        "label": "texture_flow_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_texture_flow",
        "description": "Hw_2.opencv.modules.python.test.test_texture_flow",
        "peekOfCode": "class texture_flow_test(NewOpenCVTests):\n    def test_texture_flow(self):\n        img = self.get_sample('samples/data/chessboard.png')\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        h, w = img.shape[:2]\n        eigen = cv.cornerEigenValsAndVecs(gray, 5, 3)\n        eigen = eigen.reshape(h, w, 3, 2)  # [[e1, e2], v1, v2]\n        flow = eigen[:,:,2]\n        d = 300\n        eps = d / 30",
        "detail": "Hw_2.opencv.modules.python.test.test_texture_flow",
        "documentation": {}
    },
    {
        "label": "UMat",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_umat",
        "description": "Hw_2.opencv.modules.python.test.test_umat",
        "peekOfCode": "class UMat(NewOpenCVTests):\n    def test_umat_construct(self):\n        data = np.random.random([512, 512])\n        # UMat constructors\n        data_um = cv.UMat(data)  # from ndarray\n        data_sub_um = cv.UMat(data_um, (128, 256), (128, 256))  # from UMat\n        data_dst_um = cv.UMat(128, 128, cv.CV_64F)  # from size/type\n        # test continuous and submatrix flags\n        assert data_um.isContinuous() and not data_um.isSubmatrix()\n        assert not data_sub_um.isContinuous() and data_sub_um.isSubmatrix()",
        "detail": "Hw_2.opencv.modules.python.test.test_umat",
        "documentation": {}
    },
    {
        "label": "load_exposure_seq",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.python.test.test_umat",
        "description": "Hw_2.opencv.modules.python.test.test_umat",
        "peekOfCode": "def load_exposure_seq(path):\n    images = []\n    times = []\n    with open(os.path.join(path, 'list.txt'), 'r') as list_file:\n        for line in list_file.readlines():\n            name, time = line.split()\n            images.append(cv.imread(os.path.join(path, name)))\n            times.append(1. / float(time))\n    return images, times\nclass UMat(NewOpenCVTests):",
        "detail": "Hw_2.opencv.modules.python.test.test_umat",
        "documentation": {}
    },
    {
        "label": "watershed_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.test_watershed",
        "description": "Hw_2.opencv.modules.python.test.test_watershed",
        "peekOfCode": "class watershed_test(NewOpenCVTests):\n    def test_watershed(self):\n        img = self.get_sample('cv/inpaint/orig.png')\n        markers = self.get_sample('cv/watershed/wshed_exp.png', 0)\n        refSegments = self.get_sample('cv/watershed/wshed_segments.png')\n        if img is None or markers is None:\n            self.assertEqual(0, 1, 'Missing test data')\n        colors = np.int32( list(np.ndindex(3, 3, 3)) ) * 122\n        cv.watershed(img, np.int32(markers))\n        segments = colors[np.maximum(markers, 0)]",
        "detail": "Hw_2.opencv.modules.python.test.test_watershed",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "description": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "peekOfCode": "class TestSceneRender():\n    def __init__(self, bgImg = None, fgImg = None, deformation = False, noise = 0.0, speed = 0.25, **params):\n        self.time = 0.0\n        self.timeStep = 1.0 / 30.0\n        self.foreground = fgImg\n        self.deformation = deformation\n        self.noise = noise\n        self.speed = speed\n        if bgImg is not None:\n            self.sceneBg = bgImg.copy()",
        "detail": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "documentation": {}
    },
    {
        "label": "defaultSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "description": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "peekOfCode": "defaultSize = 512\nclass TestSceneRender():\n    def __init__(self, bgImg = None, fgImg = None, deformation = False, noise = 0.0, speed = 0.25, **params):\n        self.time = 0.0\n        self.timeStep = 1.0 / 30.0\n        self.foreground = fgImg\n        self.deformation = deformation\n        self.noise = noise\n        self.speed = speed\n        if bgImg is not None:",
        "detail": "Hw_2.opencv.modules.python.test.tst_scene_render",
        "documentation": {}
    },
    {
        "label": "stitching_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_test(NewOpenCVTests):\n    def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        (_result, pano) = stitcher.stitch((img1, img2))\n        #cv.imshow(\"pano\", pano)\n        #cv.waitKey()\n        self.assertAlmostEqual(pano.shape[0], 685, delta=100, msg=\"rows: %r\" % list(pano.shape))\n        self.assertAlmostEqual(pano.shape[1], 1025, delta=100, msg=\"cols: %r\" % list(pano.shape))",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_detail_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_detail_test(NewOpenCVTests):\n    def test_simple(self):\n        img = self.get_sample('stitching/a1.png')\n        finder= cv.ORB.create()\n        imgFea = cv.detail.computeImageFeatures2(finder,img)\n        self.assertIsNotNone(imgFea)\n        # Added Test for PR #21180\n        self.assertIsNotNone(imgFea.keypoints)\n        matcher = cv.detail_BestOf2NearestMatcher(False, 0.3)\n        self.assertIsNotNone(matcher)",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_compose_panorama_test_no_args",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_compose_panorama_test_no_args(NewOpenCVTests):\n    def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama()\n        assert result == 0\nclass stitching_compose_panorama_args(NewOpenCVTests):\n    def test_simple(self):",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_compose_panorama_args",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_compose_panorama_args(NewOpenCVTests):\n    def test_simple(self):\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        stitcher = cv.Stitcher.create(cv.Stitcher_PANORAMA)\n        stitcher.estimateTransform((img1, img2))\n        result, _ = stitcher.composePanorama((img1, img2))\n        assert result == 0\nclass stitching_matches_info_test(NewOpenCVTests):\n    def test_simple(self):",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_matches_info_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_matches_info_test(NewOpenCVTests):\n    def test_simple(self):\n        finder = cv.ORB.create()\n        img1 = self.get_sample('stitching/a1.png')\n        img2 = self.get_sample('stitching/a2.png')\n        img_feat1 = cv.detail.computeImageFeatures2(finder, img1)\n        img_feat2 = cv.detail.computeImageFeatures2(finder, img2)\n        matcher = cv.detail.BestOf2NearestMatcher_create()\n        matches_info = matcher.apply(img_feat1, img_feat2)\n        self.assertIsNotNone(matches_info.matches)",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_range_matcher_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_range_matcher_test(NewOpenCVTests):\n    def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        orb = cv.ORB_create()\n        features = [cv.detail.computeImageFeatures2(orb, img) for img in images]\n        matcher = cv.detail_BestOf2NearestRangeMatcher(range_width=1)",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "stitching_seam_finder_graph_cuts",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "description": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "peekOfCode": "class stitching_seam_finder_graph_cuts(NewOpenCVTests):\n    def test_simple(self):\n        images = [\n            self.get_sample('stitching/a1.png'),\n            self.get_sample('stitching/a2.png'),\n            self.get_sample('stitching/a3.png')\n        ]\n        images = [cv.resize(img, [100, 100]) for img in images]\n        finder = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\n        masks = [cv.UMat(255 * np.ones((img.shape[0], img.shape[1]), np.uint8)) for img in images]",
        "detail": "Hw_2.opencv.modules.stitching.misc.python.test.test_stitching",
        "documentation": {}
    },
    {
        "label": "keyselector",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "def keyselector(a):\n    if cvsize_re.match(a):\n        size = [int(d) for d in a.split('x')]\n        return size[0] * size[1]\n    elif cvtype_re.match(a):\n        if a.startswith(\"CV_\"):\n            a = a[3:]\n        depth = 7\n        if a[0] == '8':\n            depth = (0, 1) [a[1] == 'S']",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "getValueParams",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "def getValueParams(test):\n    param = test.get(\"value_param\")\n    if not param:\n        return []\n    if param.startswith(\"(\"):\n        param = param[1:]\n    if param.endswith(\")\"):\n        param = param[:-1]\n    args = []\n    prev_pos = 0",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "nextPermutation",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "def nextPermutation(indexes, lists, x, y):\n    idx = len(indexes)-1\n    while idx >= 0:\n        while idx == x or idx == y:\n            idx -= 1\n        if idx < 0:\n            return False\n        v = indexes[idx] + 1\n        if v < len(lists[idx]):\n            indexes[idx] = v;",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "getTestWideName",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "def getTestWideName(sname, indexes, lists, x, y):\n    name = sname + \"::(\"\n    for i in range(len(indexes)):\n        if i > 0:\n            name += \", \"\n        if i == x:\n            name += \"X\"\n        elif i == y:\n            name += \"Y\"\n        else:",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "getTest",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "def getTest(stests, x, y, row, col):\n    for pair in stests:\n        if pair[1][x] == row and pair[1][y] == col:\n            return pair[0]\n    return None\nif __name__ == \"__main__\":\n    parser = OptionParser()\n    parser.add_option(\"-o\", \"--output\", dest=\"format\", help=\"output results in text format (can be 'txt', 'html' or 'auto' - default)\", metavar=\"FMT\", default=\"auto\")\n    parser.add_option(\"-u\", \"--units\", dest=\"units\", help=\"units for output values (s, ms (default), us, ns or ticks)\", metavar=\"UNITS\", default=\"ms\")\n    parser.add_option(\"-m\", \"--metric\", dest=\"metric\", help=\"output metric\", metavar=\"NAME\", default=\"gmean\")",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "cvsize_re",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "cvsize_re = re.compile(\"^\\d+x\\d+$\")\ncvtype_re = re.compile(\"^(CV_)(8U|8S|16U|16S|32S|32F|64F)(C\\d{1,3})?$\")\ndef keyselector(a):\n    if cvsize_re.match(a):\n        size = [int(d) for d in a.split('x')]\n        return size[0] * size[1]\n    elif cvtype_re.match(a):\n        if a.startswith(\"CV_\"):\n            a = a[3:]\n        depth = 7",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "cvtype_re",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "cvtype_re = re.compile(\"^(CV_)(8U|8S|16U|16S|32S|32F|64F)(C\\d{1,3})?$\")\ndef keyselector(a):\n    if cvsize_re.match(a):\n        size = [int(d) for d in a.split('x')]\n        return size[0] * size[1]\n    elif cvtype_re.match(a):\n        if a.startswith(\"CV_\"):\n            a = a[3:]\n        depth = 7\n        if a[0] == '8':",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "convert",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "convert = lambda text: int(text) if text.isdigit() else text\nalphanum_keyselector = lambda key: [ convert(c) for c in re.split('([0-9]+)', str(keyselector(key))) ]\ndef getValueParams(test):\n    param = test.get(\"value_param\")\n    if not param:\n        return []\n    if param.startswith(\"(\"):\n        param = param[1:]\n    if param.endswith(\")\"):\n        param = param[:-1]",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "alphanum_keyselector",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.chart",
        "description": "Hw_2.opencv.modules.ts.misc.chart",
        "peekOfCode": "alphanum_keyselector = lambda key: [ convert(c) for c in re.split('([0-9]+)', str(keyselector(key))) ]\ndef getValueParams(test):\n    param = test.get(\"value_param\")\n    if not param:\n        return []\n    if param.startswith(\"(\"):\n        param = param[1:]\n    if param.endswith(\")\"):\n        param = param[:-1]\n    args = []",
        "detail": "Hw_2.opencv.modules.ts.misc.chart",
        "documentation": {}
    },
    {
        "label": "dummyColorizer",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "class dummyColorizer(object):\n    def __init__(self, stream):\n        self.stream = stream\n    def write(self, *text, **attrs):\n        if text:\n            self.stream.write(\" \".join([str(t) for t in text]))\nclass asciiSeqColorizer(object):\n    RESET_SEQ = \"\\033[0m\"\n    #BOLD_SEQ = \"\\033[1m\"\n    ITALIC_SEQ = \"\\033[3m\"",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "asciiSeqColorizer",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "class asciiSeqColorizer(object):\n    RESET_SEQ = \"\\033[0m\"\n    #BOLD_SEQ = \"\\033[1m\"\n    ITALIC_SEQ = \"\\033[3m\"\n    UNDERLINE_SEQ = \"\\033[4m\"\n    STRIKEOUT_SEQ = \"\\033[9m\"\n    COLOR_SEQ0 = \"\\033[00;%dm\" #dark\n    COLOR_SEQ1 = \"\\033[01;%dm\" #bold and light\n    def __init__(self, stream):\n        self.stream = stream",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "RGB2LAB",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def RGB2LAB(r,g,b):\n    if max(r,g,b):\n        r /= 255.\n        g /= 255.\n        b /= 255.\n    X = (0.412453 * r + 0.357580 * g + 0.180423 * b) / 0.950456\n    Y = (0.212671 * r + 0.715160 * g + 0.072169 * b)\n    Z = (0.019334 * r + 0.119193 * g + 0.950227 * b) / 1.088754\n    #[X * 0.950456]   [0.412453 0.357580 0.180423]   [R]\n    #[Y           ] = [0.212671 0.715160 0.072169] * [G]",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "colorDistance",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def colorDistance(r1,g1,b1 = None, r2 = None, g2 = None,b2 = None):\n    if type(r1) == tuple and type(g1) == tuple and b1 is None and r2 is None and g2 is None and b2 is None:\n        (l1,a1,b1) = RGB2LAB(*r1)\n        (l2,a2,b2) = RGB2LAB(*g1)\n    else:\n        (l1,a1,b1) = RGB2LAB(r1,g1,b1)\n        (l2,a2,b2) = RGB2LAB(r2,g2,b2)\n    #CIE94\n    dl = l1-l2\n    C1 = math.sqrt(a1*a1 + b1*b1)",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "parseHexColor",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def parseHexColor(col):\n    if len(col) != 4 and len(col) != 7 and not col.startswith(\"#\"):\n        return (0,0,0)\n    if len(col) == 4:\n        r = col[1]*2\n        g = col[2]*2\n        b = col[3]*2\n    else:\n        r = col[1:3]\n        g = col[3:5]",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "getColor",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def getColor(col):\n    if isinstance(col, str):\n        if col.lower() in webcolors:\n            return parseHexColor(webcolors[col.lower()])\n        else:\n            return parseHexColor(col)\n    else:\n        return col\ndef getNearestConsoleColor(col):\n    color = getColor(col)",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "getNearestConsoleColor",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def getNearestConsoleColor(col):\n    color = getColor(col)\n    minidx = 0\n    mindist = colorDistance(color, getColor(consoleColors[0]))\n    for i in range(len(consoleColors)):\n        dist = colorDistance(color, getColor(consoleColors[i]))\n        if dist < mindist:\n            mindist = dist\n            minidx = i\n    return minidx",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "getColorizer",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "def getColorizer(stream):\n    if stream.isatty():\n        if os.name == \"nt\":\n            return winConsoleColorizer(stream)\n        else:\n            return asciiSeqColorizer(stream)\n    else:\n        return dummyColorizer(stream)",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "webcolors",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.color",
        "description": "Hw_2.opencv.modules.ts.misc.color",
        "peekOfCode": "webcolors = {\n\"indianred\": \"#cd5c5c\",\n\"lightcoral\": \"#f08080\",\n\"salmon\": \"#fa8072\",\n\"darksalmon\": \"#e9967a\",\n\"lightsalmon\": \"#ffa07a\",\n\"red\": \"#ff0000\",\n\"crimson\": \"#dc143c\",\n\"firebrick\": \"#b22222\",\n\"darkred\": \"#8b0000\",",
        "detail": "Hw_2.opencv.modules.ts.misc.color",
        "documentation": {}
    },
    {
        "label": "epilog",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.run",
        "description": "Hw_2.opencv.modules.ts.misc.run",
        "peekOfCode": "epilog = '''\nNOTE:\nAdditional options starting with \"--gtest_\" and \"--perf_\" will be passed directly to the test executables.\n'''\nif __name__ == \"__main__\":\n    # log.basicConfig(format='[%(levelname)s] %(message)s', level = log.DEBUG)\n    # log.basicConfig(format='[%(levelname)s] %(message)s', level = log.INFO)\n    parser = argparse.ArgumentParser(\n        description='OpenCV test runner script',\n        epilog=epilog,",
        "detail": "Hw_2.opencv.modules.ts.misc.run",
        "documentation": {}
    },
    {
        "label": "ApkInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "class ApkInfo:\n    def __init__(self):\n        self.pkg_name = None\n        self.pkg_target = None\n        self.pkg_runner = None\n    def forcePackage(self, package):\n        if package:\n            if package.startswith(\".\"):\n                self.pkg_target += package\n            else:",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "Tool",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "class Tool:\n    def __init__(self):\n        self.cmd = []\n    def run(self, args=[], silent=False):\n        cmd = self.cmd[:]\n        cmd.extend(args)\n        return execute(self.cmd + args, silent)\nclass Adb(Tool):\n    def __init__(self, sdk_dir):\n        Tool.__init__(self)",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "Adb",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "class Adb(Tool):\n    def __init__(self, sdk_dir):\n        Tool.__init__(self)\n        exe_path = os.path.join(sdk_dir, exe(\"platform-tools/adb\"))\n        if not os.path.isfile(exe_path) or not os.access(exe_path, os.X_OK):\n            exe_path = None\n        # fix adb tool location\n        if not exe_path:\n            exe_path = \"adb\"\n        self.cmd = [exe_path]",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "Aapt",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "class Aapt(Tool):\n    def __init__(self, sdk_dir):\n        Tool.__init__(self)\n        aapt_fn = exe(\"aapt\")\n        aapt = None\n        for r, ds, fs in os.walk(os.path.join(sdk_dir, 'build-tools')):\n            if aapt_fn in fs:\n                aapt = os.path.join(r, aapt_fn)\n                break\n        if not aapt:",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "AndroidTestSuite",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "class AndroidTestSuite(TestSuite):\n    def __init__(self, options, cache, id, android_env={}):\n        TestSuite.__init__(self, options, cache, id)\n        sdk_dir = options.android_sdk or os.environ.get(\"ANDROID_SDK\", False) or os.path.dirname(os.path.dirname(self.cache.android_executable))\n        log.debug(\"Detecting Android tools in directory: %s\", sdk_dir)\n        self.adb = Adb(sdk_dir)\n        self.aapt = Aapt(sdk_dir)\n        self.env = android_env\n    def isTest(self, fullpath):\n        if os.path.isfile(fullpath):",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "exe",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_android",
        "description": "Hw_2.opencv.modules.ts.misc.run_android",
        "peekOfCode": "def exe(program):\n    return program + \".exe\" if hostos == 'nt' else program\nclass ApkInfo:\n    def __init__(self):\n        self.pkg_name = None\n        self.pkg_target = None\n        self.pkg_runner = None\n    def forcePackage(self, package):\n        if package:\n            if package.startswith(\".\"):",
        "detail": "Hw_2.opencv.modules.ts.misc.run_android",
        "documentation": {}
    },
    {
        "label": "longTestFilter",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_long",
        "description": "Hw_2.opencv.modules.ts.misc.run_long",
        "peekOfCode": "def longTestFilter(data, module=None):\n    res = ['*', '-'] + [v for m, v, _time in data if module is None or m == module]\n    return '--gtest_filter={}'.format(':'.join(res))\n# Parse one xml file, filter out tests which took less than 'timeLimit' seconds\n# Returns tuple: ( <module_name>, [ (<module_name>, <test_name>, <test_time>), ... ] )\ndef parseOneFile(filename, timeLimit):\n    tree = ET.parse(filename)\n    root = tree.getroot()\n    def guess(s, delims):\n        for delim in delims:",
        "detail": "Hw_2.opencv.modules.ts.misc.run_long",
        "documentation": {}
    },
    {
        "label": "parseOneFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_long",
        "description": "Hw_2.opencv.modules.ts.misc.run_long",
        "peekOfCode": "def parseOneFile(filename, timeLimit):\n    tree = ET.parse(filename)\n    root = tree.getroot()\n    def guess(s, delims):\n        for delim in delims:\n            tmp = s.partition(delim)\n            if len(tmp[1]) != 0:\n                return tmp[0]\n        return None\n    module = guess(filename, ['_posix_', '_nt_', '__']) or root.get('cv_module_name')",
        "detail": "Hw_2.opencv.modules.ts.misc.run_long",
        "documentation": {}
    },
    {
        "label": "LONG_TESTS_DEBUG_VALGRIND",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_long",
        "description": "Hw_2.opencv.modules.ts.misc.run_long",
        "peekOfCode": "LONG_TESTS_DEBUG_VALGRIND = [\n    ('calib3d', 'Calib3d_InitUndistortRectifyMap.accuracy', 2017.22),\n    ('dnn', 'Reproducibility*', 1000),  # large DNN models\n    ('dnn', '*RCNN*', 1000),  # very large DNN models\n    ('dnn', '*RFCN*', 1000),  # very large DNN models\n    ('dnn', '*EAST*', 1000),  # very large DNN models\n    ('dnn', '*VGG16*', 1000),  # very large DNN models\n    ('dnn', '*ZFNet*', 1000),  # very large DNN models\n    ('dnn', '*ResNet101_DUC_HDC*', 1000),  # very large DNN models\n    ('dnn', '*LResNet100E_IR*', 1000),  # very large DNN models",
        "detail": "Hw_2.opencv.modules.ts.misc.run_long",
        "documentation": {}
    },
    {
        "label": "TestSuite",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_suite",
        "description": "Hw_2.opencv.modules.ts.misc.run_suite",
        "peekOfCode": "class TestSuite(object):\n    def __init__(self, options, cache, id):\n        self.options = options\n        self.cache = cache\n        self.nameprefix = \"opencv_\" + self.options.mode + \"_\"\n        self.tests = self.cache.gatherTests(self.nameprefix + \"*\", self.isTest)\n        self.id = id\n    def getOS(self):\n        return getPlatformVersion() or self.cache.getOS()\n    def getLogName(self, app):",
        "detail": "Hw_2.opencv.modules.ts.misc.run_suite",
        "documentation": {}
    },
    {
        "label": "Err",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "class Err(Exception):\n    def __init__(self, msg, *args):\n        self.msg = msg % args\ndef execute(cmd, silent=False, cwd=\".\", env=None):\n    try:\n        log.debug(\"Run: %s\", cmd)\n        if env is not None:\n            for k in env:\n                log.debug(\"    Environ: %s=%s\", k, env[k])\n            new_env = os.environ.copy()",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "CMakeCache",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "class CMakeCache:\n    def __init__(self, cfg=None):\n        self.setDefaultAttrs()\n        self.main_modules = []\n        if cfg:\n            self.build_type = cfg\n    def setDummy(self, path):\n        self.tests_dir = os.path.normpath(path)\n    def read(self, path, fname):\n        rx = re.compile(r'^OPENCV_MODULE_opencv_(\\w+)_LOCATION:INTERNAL=(.*)$')",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "TempEnvDir",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "class TempEnvDir:\n    def __init__(self, envname, prefix):\n        self.envname = envname\n        self.prefix = prefix\n        self.saved_name = None\n        self.new_name = None\n    def init(self):\n        self.saved_name = os.environ.get(self.envname)\n        self.new_name = tempfile.mkdtemp(prefix=self.prefix, dir=self.saved_name or None)\n        os.environ[self.envname] = self.new_name",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "initLogger",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "def initLogger():\n    logger = logging.getLogger(\"run.py\")\n    logger.setLevel(logging.DEBUG)\n    ch = logging.StreamHandler(sys.stderr)\n    ch.setFormatter(logging.Formatter(\"%(message)s\"))\n    logger.addHandler(ch)\n    return logger\nlog = initLogger()\nhostos = os.name  # 'nt', 'posix'\nclass Err(Exception):",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "def execute(cmd, silent=False, cwd=\".\", env=None):\n    try:\n        log.debug(\"Run: %s\", cmd)\n        if env is not None:\n            for k in env:\n                log.debug(\"    Environ: %s=%s\", k, env[k])\n            new_env = os.environ.copy()\n            new_env.update(env)\n            env = new_env\n        if sys.platform == 'darwin':  # https://github.com/opencv/opencv/issues/14351",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "isColorEnabled",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "def isColorEnabled(args):\n    usercolor = [a for a in args if a.startswith(\"--gtest_color=\")]\n    return len(usercolor) == 0 and sys.stdout.isatty() and hostos != \"nt\"\ndef getPlatformVersion():\n    mv = platform.mac_ver()\n    if mv[0]:\n        return \"Darwin\" + mv[0]\n    else:\n        wv = platform.win32_ver()\n        if wv[0]:",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "getPlatformVersion",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "def getPlatformVersion():\n    mv = platform.mac_ver()\n    if mv[0]:\n        return \"Darwin\" + mv[0]\n    else:\n        wv = platform.win32_ver()\n        if wv[0]:\n            return \"Windows\" + wv[0]\n        else:\n            lv = platform.linux_distribution()",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "log",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "log = initLogger()\nhostos = os.name  # 'nt', 'posix'\nclass Err(Exception):\n    def __init__(self, msg, *args):\n        self.msg = msg % args\ndef execute(cmd, silent=False, cwd=\".\", env=None):\n    try:\n        log.debug(\"Run: %s\", cmd)\n        if env is not None:\n            for k in env:",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "hostos",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "hostos = os.name  # 'nt', 'posix'\nclass Err(Exception):\n    def __init__(self, msg, *args):\n        self.msg = msg % args\ndef execute(cmd, silent=False, cwd=\".\", env=None):\n    try:\n        log.debug(\"Run: %s\", cmd)\n        if env is not None:\n            for k in env:\n                log.debug(\"    Environ: %s=%s\", k, env[k])",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "parse_patterns",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.run_utils",
        "description": "Hw_2.opencv.modules.ts.misc.run_utils",
        "peekOfCode": "parse_patterns = (\n    {'name': \"cmake_home\",               'default': None,       'pattern': re.compile(r\"^CMAKE_HOME_DIRECTORY:\\w+=(.+)$\")},\n    {'name': \"opencv_home\",              'default': None,       'pattern': re.compile(r\"^OpenCV_SOURCE_DIR:\\w+=(.+)$\")},\n    {'name': \"opencv_build\",             'default': None,       'pattern': re.compile(r\"^OpenCV_BINARY_DIR:\\w+=(.+)$\")},\n    {'name': \"tests_dir\",                'default': None,       'pattern': re.compile(r\"^EXECUTABLE_OUTPUT_PATH:\\w+=(.+)$\")},\n    {'name': \"build_type\",               'default': \"Release\",  'pattern': re.compile(r\"^CMAKE_BUILD_TYPE:\\w+=(.*)$\")},\n    {'name': \"android_abi\",              'default': None,       'pattern': re.compile(r\"^ANDROID_ABI:\\w+=(.*)$\")},\n    {'name': \"android_executable\",       'default': None,       'pattern': re.compile(r\"^ANDROID_EXECUTABLE:\\w+=(.*android.*)$\")},\n    {'name': \"ant_executable\",           'default': None,       'pattern': re.compile(r\"^ANT_EXECUTABLE:\\w+=(.*ant.*)$\")},\n    {'name': \"java_test_dir\",            'default': None,       'pattern': re.compile(r\"^OPENCV_JAVA_TEST_DIR:\\w+=(.*)$\")},",
        "detail": "Hw_2.opencv.modules.ts.misc.run_utils",
        "documentation": {}
    },
    {
        "label": "getSetName",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "def getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None\n    if short and prefix:\n        return prefix\n    name = tset[0].replace(\".xml\",\"\").replace(\"_\", \"\\n\")\n    if prefix:\n        return prefix + \"\\n\" + (\"-\"*int(len(max(prefix.split(\"\\n\"), key=len))*1.5)) + \"\\n\" + name",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "numeric_re",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "numeric_re = re.compile(\"(\\d+)\")\ncvtype_re = re.compile(\"(8U|8S|16U|16S|32S|32F|64F)C(\\d{1,3})\")\ncvtypes = { '8U': 0, '8S': 1, '16U': 2, '16S': 3, '32S': 4, '32F': 5, '64F': 6 }\nconvert = lambda text: int(text) if text.isdigit() else text\nkeyselector = lambda a: cvtype_re.sub(lambda match: \" \" + str(cvtypes.get(match.group(1), 7) + (int(match.group(2))-1) * 8) + \" \", a)\nalphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "cvtype_re",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "cvtype_re = re.compile(\"(8U|8S|16U|16S|32S|32F|64F)C(\\d{1,3})\")\ncvtypes = { '8U': 0, '8S': 1, '16U': 2, '16S': 3, '32S': 4, '32F': 5, '64F': 6 }\nconvert = lambda text: int(text) if text.isdigit() else text\nkeyselector = lambda a: cvtype_re.sub(lambda match: \" \" + str(cvtypes.get(match.group(1), 7) + (int(match.group(2))-1) * 8) + \" \", a)\nalphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "cvtypes",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "cvtypes = { '8U': 0, '8S': 1, '16U': 2, '16S': 3, '32S': 4, '32F': 5, '64F': 6 }\nconvert = lambda text: int(text) if text.isdigit() else text\nkeyselector = lambda a: cvtype_re.sub(lambda match: \" \" + str(cvtypes.get(match.group(1), 7) + (int(match.group(2))-1) * 8) + \" \", a)\nalphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None\n    if short and prefix:",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "convert",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "convert = lambda text: int(text) if text.isdigit() else text\nkeyselector = lambda a: cvtype_re.sub(lambda match: \" \" + str(cvtypes.get(match.group(1), 7) + (int(match.group(2))-1) * 8) + \" \", a)\nalphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None\n    if short and prefix:\n        return prefix",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "keyselector",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "keyselector = lambda a: cvtype_re.sub(lambda match: \" \" + str(cvtypes.get(match.group(1), 7) + (int(match.group(2))-1) * 8) + \" \", a)\nalphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None\n    if short and prefix:\n        return prefix\n    name = tset[0].replace(\".xml\",\"\").replace(\"_\", \"\\n\")",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "alphanum_keyselector",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.summary",
        "description": "Hw_2.opencv.modules.ts.misc.summary",
        "peekOfCode": "alphanum_keyselector = lambda key: [ convert(c) for c in numeric_re.split(keyselector(key)) ]\ndef getSetName(tset, idx, columns, short = True):\n    if columns and len(columns) > idx:\n        prefix = columns[idx]\n    else:\n        prefix = None\n    if short and prefix:\n        return prefix\n    name = tset[0].replace(\".xml\",\"\").replace(\"_\", \"\\n\")\n    if prefix:",
        "detail": "Hw_2.opencv.modules.ts.misc.summary",
        "documentation": {}
    },
    {
        "label": "tblCell",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "class tblCell(object):\n    def __init__(self, text, value = None, props = None):\n        self.text = text\n        self.value = value\n        self.props = props\nclass tblColumn(object):\n    def __init__(self, caption, title = None, props = None):\n        self.text = caption\n        self.title = title\n        self.props = props",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "tblColumn",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "class tblColumn(object):\n    def __init__(self, caption, title = None, props = None):\n        self.text = caption\n        self.title = title\n        self.props = props\nclass tblRow(object):\n    def __init__(self, colsNum, props = None):\n        self.cells = [None] * colsNum\n        self.props = props\ndef htmlEncode(str):",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "tblRow",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "class tblRow(object):\n    def __init__(self, colsNum, props = None):\n        self.cells = [None] * colsNum\n        self.props = props\ndef htmlEncode(str):\n    return '<br/>'.join([escape(s) for s in str])\nclass table(object):\n    def_align = \"left\"\n    def_valign = \"middle\"\n    def_color = None",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "class table(object):\n    def_align = \"left\"\n    def_valign = \"middle\"\n    def_color = None\n    def_colspan = 1\n    def_rowspan = 1\n    def_bold = False\n    def_italic = False\n    def_text=\"-\"\n    def __init__(self, caption = None, format=None):",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "htmlEncode",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def htmlEncode(str):\n    return '<br/>'.join([escape(s) for s in str])\nclass table(object):\n    def_align = \"left\"\n    def_valign = \"middle\"\n    def_color = None\n    def_colspan = 1\n    def_rowspan = 1\n    def_bold = False\n    def_italic = False",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "htmlPrintHeader",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def htmlPrintHeader(out, title = None):\n    if title:\n        titletag = \"<title>%s</title>\\n\" % htmlEncode([str(title)])\n    else:\n        titletag = \"\"\n    out.write(\"\"\"<!DOCTYPE HTML>\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=us-ascii\">\n%s<style type=\"text/css\">",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "htmlPrintFooter",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def htmlPrintFooter(out):\n    out.write(\"</body>\\n</html>\")\ndef getStdoutFilename():\n    try:\n        if os.name == \"nt\":\n            import msvcrt, ctypes\n            handle = msvcrt.get_osfhandle(sys.stdout.fileno())\n            size = ctypes.c_ulong(1024)\n            nameBuffer = ctypes.create_string_buffer(size.value)\n            ctypes.windll.kernel32.GetFinalPathNameByHandleA(handle, nameBuffer, size, 4)",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "getStdoutFilename",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def getStdoutFilename():\n    try:\n        if os.name == \"nt\":\n            import msvcrt, ctypes\n            handle = msvcrt.get_osfhandle(sys.stdout.fileno())\n            size = ctypes.c_ulong(1024)\n            nameBuffer = ctypes.create_string_buffer(size.value)\n            ctypes.windll.kernel32.GetFinalPathNameByHandleA(handle, nameBuffer, size, 4)\n            return nameBuffer.value\n        else:",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "detectHtmlOutputType",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def detectHtmlOutputType(requestedType):\n    if requestedType in ['txt', 'markdown']:\n        return False\n    elif requestedType in [\"html\", \"moinwiki\"]:\n        return True\n    else:\n        if sys.stdout.isatty():\n            return False\n        else:\n            outname = getStdoutFilename()",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "getRelativeVal",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def getRelativeVal(test, test0, metric):\n    if not test or not test0:\n        return None\n    val0 = test0.get(metric, \"s\")\n    if not val0:\n        return None\n    val =  test.get(metric, \"s\")\n    if not val or val == 0:\n        return None\n    return float(val0)/val",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "getCycleReduction",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def getCycleReduction(test, test0, metric):\n    if not test or not test0:\n        return None\n    val0 = test0.get(metric, \"s\")\n    if not val0 or val0 == 0:\n        return None\n    val =  test.get(metric, \"s\")\n    if not val:\n        return None\n    return (1.0-float(val)/val0)*100",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "getScore",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def getScore(test, test0, metric):\n    if not test or not test0:\n        return None\n    m0 = float(test.get(\"gmean\", None))\n    m1 = float(test0.get(\"gmean\", None))\n    if m0 == 0 or m1 == 0:\n        return None\n    s0 = float(test.get(\"gstddev\", None))\n    s1 = float(test0.get(\"gstddev\", None))\n    s = math.sqrt(s0*s0 + s1*s1)",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "formatValue",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "def formatValue(val, metric, units = None):\n    if val is None:\n        return \"-\"\n    if metric.endswith(\"%\"):\n        return \"%.2f\" % val\n    if metric.endswith(\"$\"):\n        return \"%.2f%%\" % val\n    if metric.endswith(\"S\"):\n        if val > 3.5:\n            return \"SLOWER\"",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "metrix_table",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "description": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "peekOfCode": "metrix_table = \\\n{\n    \"name\": (\"Name of Test\", lambda test,test0,units: str(test)),\n    \"samples\": (\"Number of\\ncollected samples\", lambda test,test0,units: test.get(\"samples\", units)),\n    \"outliers\": (\"Number of\\noutliers\", lambda test,test0,units: test.get(\"outliers\", units)),\n    \"gmean\": (\"Geometric mean\", lambda test,test0,units: test.get(\"gmean\", units)),\n    \"mean\": (\"Mean\", lambda test,test0,units: test.get(\"mean\", units)),\n    \"min\": (\"Min\", lambda test,test0,units: test.get(\"min\", units)),\n    \"median\": (\"Median\", lambda test,test0,units: test.get(\"median\", units)),\n    \"stddev\": (\"Standard deviation\", lambda test,test0,units: test.get(\"stddev\", units)),",
        "detail": "Hw_2.opencv.modules.ts.misc.table_formatter",
        "documentation": {}
    },
    {
        "label": "TestInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "description": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "peekOfCode": "class TestInfo(object):\n    def __init__(self, xmlnode):\n        self.fixture = xmlnode.getAttribute(\"classname\")\n        self.name = xmlnode.getAttribute(\"name\")\n        self.value_param = xmlnode.getAttribute(\"value_param\")\n        self.type_param = xmlnode.getAttribute(\"type_param\")\n        custom_status = xmlnode.getAttribute(\"custom_status\")\n        failures = xmlnode.getElementsByTagName(\"failure\")\n        if len(custom_status) > 0:\n            self.status = custom_status",
        "detail": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "documentation": {}
    },
    {
        "label": "TestRunInfo",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "description": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "peekOfCode": "class TestRunInfo(object):\n    def __init__(self, properties, tests):\n        self.properties = properties\n        self.tests = tests\n    def __len__(self):\n        return len(self.tests)\n    def __getitem__(self, key):\n        return self.tests[key]\ndef parseLogFile(filename):\n    log = parse(filename)",
        "detail": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "documentation": {}
    },
    {
        "label": "parseLogFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "description": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "peekOfCode": "def parseLogFile(filename):\n    log = parse(filename)\n    properties = {\n        attr_name[3:]: attr_value\n        for (attr_name, attr_value) in log.documentElement.attributes.items()\n        if attr_name.startswith('cv_')\n    }\n    tests = list(map(TestInfo, log.getElementsByTagName(\"testcase\")))\n    return TestRunInfo(properties, tests)\nif __name__ == \"__main__\":",
        "detail": "Hw_2.opencv.modules.ts.misc.testlog_parser",
        "documentation": {}
    },
    {
        "label": "Trace",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "class Trace:\n    def __init__(self, filename=None):\n        self.tasks = {}\n        self.tasks_list = []\n        self.locations = {}\n        self.threads_stack = {}\n        self.pending_files = deque()\n        if filename:\n            self.load(filename)\n    class TraceTask:",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "tryNum",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "def tryNum(s):\n    if s.startswith('0x'):\n        try:\n            return int(s, 16)\n        except ValueError:\n            pass\n    try:\n        return int(s)\n    except ValueError:\n        pass",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "formatTimestamp",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "def formatTimestamp(t):\n    return \"%.3f\" % (t * 1e-6)\ntry:\n    from statistics import median\nexcept ImportError:\n    def median(lst):\n        sortedLst = sorted(lst)\n        lstLen = len(lst)\n        index = (lstLen - 1) // 2\n        if (lstLen % 2):",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "getCXXFunctionName",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "def getCXXFunctionName(spec):\n    def dropParams(spec):\n        pos = len(spec) - 1\n        depth = 0\n        while pos >= 0:\n            if spec[pos] == ')':\n                depth = depth + 1\n            elif spec[pos] == '(':\n                depth = depth - 1\n                if depth == 0:",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "REGION_FLAG_IMPL_MASK",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "REGION_FLAG_IMPL_MASK = 15 << 16\nREGION_FLAG_IMPL_IPP = 1 << 16\nREGION_FLAG_IMPL_OPENCL = 2 << 16\nDEBUG = False\nif DEBUG:\n    dprint = print\n    dpprint = pprint\nelse:\n    def dprint(args, **kwargs):\n        pass",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "REGION_FLAG_IMPL_IPP",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "REGION_FLAG_IMPL_IPP = 1 << 16\nREGION_FLAG_IMPL_OPENCL = 2 << 16\nDEBUG = False\nif DEBUG:\n    dprint = print\n    dpprint = pprint\nelse:\n    def dprint(args, **kwargs):\n        pass\n    def dpprint(args, **kwargs):",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "REGION_FLAG_IMPL_OPENCL",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "REGION_FLAG_IMPL_OPENCL = 2 << 16\nDEBUG = False\nif DEBUG:\n    dprint = print\n    dpprint = pprint\nelse:\n    def dprint(args, **kwargs):\n        pass\n    def dpprint(args, **kwargs):\n        pass",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "DEBUG",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "DEBUG = False\nif DEBUG:\n    dprint = print\n    dpprint = pprint\nelse:\n    def dprint(args, **kwargs):\n        pass\n    def dpprint(args, **kwargs):\n        pass\ndef tryNum(s):",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "stack_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "description": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "peekOfCode": "stack_size = 10\nclass Trace:\n    def __init__(self, filename=None):\n        self.tasks = {}\n        self.tasks_list = []\n        self.locations = {}\n        self.threads_stack = {}\n        self.pending_files = deque()\n        if filename:\n            self.load(filename)",
        "detail": "Hw_2.opencv.modules.ts.misc.trace_profiler",
        "documentation": {}
    },
    {
        "label": "Collector",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "class Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched\n        self.tests = {}\n        self.extra_configurations = set()\n    # Format a sorted sequence of pairs as if it was a dictionary.\n    # We can't just use a dictionary instead, since we want to preserve the sorted order of the keys.\n    @staticmethod",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "make_match_func",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "def make_match_func(matchers):\n    def match_func(properties):\n        for matcher in matchers:\n            if all(properties.get(name) == value\n                   for (name, value) in matcher['properties'].iteritems()):\n                return matcher['name']\n        return None\n    return match_func\ndef main():\n    arg_parser = ArgumentParser(description='Build an XLS performance report.')",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "def main():\n    arg_parser = ArgumentParser(description='Build an XLS performance report.')\n    arg_parser.add_argument('sheet_dirs', nargs='+', metavar='DIR', help='directory containing perf test logs')\n    arg_parser.add_argument('-o', '--output', metavar='XLS', default='report.xls', help='name of output file')\n    arg_parser.add_argument('-c', '--config', metavar='CONF', help='global configuration file')\n    arg_parser.add_argument('--include-unmatched', action='store_true',\n        help='include results from XML files that were not recognized by configuration matchers')\n    arg_parser.add_argument('--show-times-per-pixel', action='store_true',\n        help='for tests that have an image size parameter, show per-pixel time, as well as total time')\n    args = arg_parser.parse_args()",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "re_image_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "re_image_size = re.compile(r'^ \\d+ x \\d+$', re.VERBOSE)\nre_data_type = re.compile(r'^ (?: 8 | 16 | 32 | 64 ) [USF] C [1234] $', re.VERBOSE)\ntime_style = xlwt.easyxf(num_format_str='#0.00')\nno_time_style = xlwt.easyxf('pattern: pattern solid, fore_color gray25')\nfailed_style = xlwt.easyxf('pattern: pattern solid, fore_color red')\nnoimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "re_data_type",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "re_data_type = re.compile(r'^ (?: 8 | 16 | 32 | 64 ) [USF] C [1234] $', re.VERBOSE)\ntime_style = xlwt.easyxf(num_format_str='#0.00')\nno_time_style = xlwt.easyxf('pattern: pattern solid, fore_color gray25')\nfailed_style = xlwt.easyxf('pattern: pattern solid, fore_color red')\nnoimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "time_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "time_style = xlwt.easyxf(num_format_str='#0.00')\nno_time_style = xlwt.easyxf('pattern: pattern solid, fore_color gray25')\nfailed_style = xlwt.easyxf('pattern: pattern solid, fore_color red')\nnoimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "no_time_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "no_time_style = xlwt.easyxf('pattern: pattern solid, fore_color gray25')\nfailed_style = xlwt.easyxf('pattern: pattern solid, fore_color red')\nnoimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "failed_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "failed_style = xlwt.easyxf('pattern: pattern solid, fore_color red')\nnoimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "noimpl_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "noimpl_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nstyle_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "style_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "style_dict = {\"failed\": failed_style, \"noimpl\":noimpl_style}\nspeedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "speedup_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "speedup_style = time_style\ngood_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "good_speedup_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "good_speedup_style = xlwt.easyxf('font: color green', num_format_str='#0.00')\nbad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "bad_speedup_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "bad_speedup_style = xlwt.easyxf('font: color red', num_format_str='#0.00')\nno_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "no_speedup_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "no_speedup_style = no_time_style\nerror_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched\n        self.tests = {}",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "error_speedup_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "error_speedup_style = xlwt.easyxf('pattern: pattern solid, fore_color orange')\nheader_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched\n        self.tests = {}\n        self.extra_configurations = set()",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "header_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "header_style = xlwt.easyxf('font: bold true; alignment: horizontal centre, vertical top, wrap True')\nsubheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched\n        self.tests = {}\n        self.extra_configurations = set()\n    # Format a sorted sequence of pairs as if it was a dictionary.",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "subheader_style",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.ts.misc.xls-report",
        "description": "Hw_2.opencv.modules.ts.misc.xls-report",
        "peekOfCode": "subheader_style = xlwt.easyxf('alignment: horizontal centre, vertical top')\nclass Collector(object):\n    def __init__(self, config_match_func, include_unmatched):\n        self.__config_cache = {}\n        self.config_match_func = config_match_func\n        self.include_unmatched = include_unmatched\n        self.tests = {}\n        self.extra_configurations = set()\n    # Format a sorted sequence of pairs as if it was a dictionary.\n    # We can't just use a dictionary instead, since we want to preserve the sorted order of the keys.",
        "detail": "Hw_2.opencv.modules.ts.misc.xls-report",
        "documentation": {}
    },
    {
        "label": "lk_homography_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "peekOfCode": "class lk_homography_test(NewOpenCVTests):\n    render = None\n    framesCounter = 0\n    frame = frame0 = None\n    p0 = None\n    p1 = None\n    gray0 = gray1 = None\n    numFeaturesInRectOnStart = 0\n    def test_lk_homography(self):\n        self.render = TestSceneRender(self.get_sample('samples/data/graf1.png'),",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "documentation": {}
    },
    {
        "label": "checkedTrace",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "peekOfCode": "def checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n    d = abs(p0-p0r).reshape(-1, 2).max(-1)\n    status = d < back_threshold\n    return p1, status\nclass lk_homography_test(NewOpenCVTests):\n    render = None\n    framesCounter = 0\n    frame = frame0 = None",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "documentation": {}
    },
    {
        "label": "lk_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "peekOfCode": "lk_params = dict( winSize  = (19, 19),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\nfeature_params = dict( maxCorners = 1000,\n                       qualityLevel = 0.01,\n                       minDistance = 8,\n                       blockSize = 19 )\ndef checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "documentation": {}
    },
    {
        "label": "feature_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "peekOfCode": "feature_params = dict( maxCorners = 1000,\n                       qualityLevel = 0.01,\n                       minDistance = 8,\n                       blockSize = 19 )\ndef checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n    d = abs(p0-p0r).reshape(-1, 2).max(-1)\n    status = d < back_threshold\n    return p1, status",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_homography",
        "documentation": {}
    },
    {
        "label": "lk_track_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "peekOfCode": "class lk_track_test(NewOpenCVTests):\n    track_len = 10\n    detect_interval = 5\n    tracks = []\n    frame_idx = 0\n    render = None\n    def test_lk_track(self):\n        self.render = TestSceneRender(self.get_sample('samples/data/graf1.png'), self.get_sample('samples/data/box.png'))\n        self.runTracker()\n    def runTracker(self):",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "documentation": {}
    },
    {
        "label": "getRectFromPoints",
        "kind": 2,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "peekOfCode": "def getRectFromPoints(points):\n    distances = []\n    for point in points:\n        distances.append(cv.norm(point, cv.NORM_L2))\n    x0, y0 = points[np.argmin(distances)]\n    x1, y1 = points[np.argmax(distances)]\n    return np.array([x0, y0, x1, y1])\nclass lk_track_test(NewOpenCVTests):\n    track_len = 10\n    detect_interval = 5",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "documentation": {}
    },
    {
        "label": "lk_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "peekOfCode": "lk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\nfeature_params = dict( maxCorners = 500,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\ndef getRectFromPoints(points):\n    distances = []\n    for point in points:",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "documentation": {}
    },
    {
        "label": "feature_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "peekOfCode": "feature_params = dict( maxCorners = 500,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\ndef getRectFromPoints(points):\n    distances = []\n    for point in points:\n        distances.append(cv.norm(point, cv.NORM_L2))\n    x0, y0 = points[np.argmin(distances)]\n    x1, y1 = points[np.argmax(distances)]",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_lk_track",
        "documentation": {}
    },
    {
        "label": "tracking_test",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.video.misc.python.test.test_tracking",
        "description": "Hw_2.opencv.modules.video.misc.python.test.test_tracking",
        "peekOfCode": "class tracking_test(NewOpenCVTests):\n    def test_createTracker(self):\n        t = cv.TrackerMIL_create()\n        try:\n            t = cv.TrackerGOTURN_create()\n        except cv.error as e:\n            pass  # may fail due to missing DL model files\nif __name__ == '__main__':\n    NewOpenCVTests.bootstrap()",
        "detail": "Hw_2.opencv.modules.video.misc.python.test.test_tracking",
        "documentation": {}
    },
    {
        "label": "Bindings",
        "kind": 6,
        "importPath": "Hw_2.opencv.modules.videoio.misc.python.test.test_videoio",
        "description": "Hw_2.opencv.modules.videoio.misc.python.test.test_videoio",
        "peekOfCode": "class Bindings(NewOpenCVTests):\n    def check_name(self, name):\n        #print(name)\n        self.assertFalse(name == None)\n        self.assertFalse(name == \"\")\n    def test_registry(self):\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_ANY));\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_FFMPEG))\n        self.check_name(cv.videoio_registry.getBackendName(cv.CAP_OPENCV_MJPEG))\n        backends = cv.videoio_registry.getBackends()",
        "detail": "Hw_2.opencv.modules.videoio.misc.python.test.test_videoio",
        "documentation": {}
    },
    {
        "label": "TestCmakeBuild",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "description": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "peekOfCode": "class TestCmakeBuild(unittest.TestCase):\n    def __init__(self, libset, abi, cmake_vars, opencv_cmake_path, workdir, *args, **kwargs):\n        unittest.TestCase.__init__(self, *args, **kwargs)\n        self.libset = libset\n        self.abi = abi\n        self.cmake_vars = cmake_vars\n        self.opencv_cmake_path = opencv_cmake_path\n        self.workdir = workdir\n        self.srcdir = os.path.join(self.workdir, \"src\")\n        self.bindir = os.path.join(self.workdir, \"build\")",
        "detail": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "documentation": {}
    },
    {
        "label": "suite",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "description": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "peekOfCode": "def suite(workdir, opencv_cmake_path):\n    abis = {\n        \"armeabi-v7a\": { \"ANDROID_ABI\": \"armeabi-v7a\", \"ANDROID_TOOLCHAIN\": \"clang\", \"ANDROID_STL\": \"c++_shared\", 'ANDROID_NATIVE_API_LEVEL': \"21\" },\n        \"arm64-v8a\": { \"ANDROID_ABI\": \"arm64-v8a\", \"ANDROID_TOOLCHAIN\": \"clang\", \"ANDROID_STL\": \"c++_shared\", 'ANDROID_NATIVE_API_LEVEL': \"21\" },\n        \"x86\": { \"ANDROID_ABI\": \"x86\", \"ANDROID_TOOLCHAIN\": \"clang\", \"ANDROID_STL\": \"c++_shared\", 'ANDROID_NATIVE_API_LEVEL': \"21\" },\n        \"x86_64\": { \"ANDROID_ABI\": \"x86_64\", \"ANDROID_TOOLCHAIN\": \"clang\", \"ANDROID_STL\": \"c++_shared\", 'ANDROID_NATIVE_API_LEVEL': \"21\" },\n    }\n    suite = unittest.TestSuite()\n    for libset in [\"\", \"opencv_java\"]:\n        for abi, cmake_vars in abis.items():",
        "detail": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "documentation": {}
    },
    {
        "label": "CPP_TEMPLATE",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "description": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "peekOfCode": "CPP_TEMPLATE = '''\\\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\nusing namespace cv;\nconst char* message = \"Hello Android!\";\nint main(int argc, char* argv[])\n{\n  (void)argc; (void)argv;\n  printf(\"%s\\\\n\", message);",
        "detail": "Hw_2.opencv.platforms.android.build-tests.test_cmake_build",
        "documentation": {}
    },
    {
        "label": "Fail",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "class Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        retcode = subprocess.call(cmd, shell=shell)",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "ABI",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "class ABI:\n    def __init__(self, platform_id, name, toolchain, ndk_api_level = None, cmake_vars = dict()):\n        self.platform_id = platform_id # platform code to add to apk version (for cmake)\n        self.name = name # general name (official Android ABI identifier)\n        self.toolchain = toolchain # toolchain identifier (for cmake)\n        self.cmake_vars = dict(\n            ANDROID_STL=\"gnustl_static\",\n            ANDROID_ABI=self.name,\n            ANDROID_PLATFORM_ID=platform_id,\n        )",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "Builder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "class Builder:\n    def __init__(self, workdir, opencvdir, config):\n        self.workdir = check_dir(workdir, create=True)\n        self.opencvdir = check_dir(opencvdir)\n        self.config = config\n        self.libdest = check_dir(os.path.join(self.workdir, \"o4a\"), create=True, clean=True)\n        self.resultdest = check_dir(os.path.join(self.workdir, 'OpenCV-android-sdk'), create=True, clean=True)\n        self.docdest = check_dir(os.path.join(self.workdir, 'OpenCV-android-sdk', 'sdk', 'java', 'javadoc'), create=True, clean=True)\n        self.extra_packs = []\n        self.opencv_version = determine_opencv_version(os.path.join(self.opencvdir, \"modules\", \"core\", \"include\", \"opencv2\", \"core\", \"version.hpp\"))",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def execute(cmd, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        retcode = subprocess.call(cmd, shell=shell)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)\n    except OSError as e:",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "rm_one",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def rm_one(d):\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)\ndef check_dir(d, create=False, clean=False):",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "check_dir",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def check_dir(d, create=False, clean=False):\n    d = os.path.abspath(d)\n    log.info(\"Check dir %s (create: %s, clean: %s)\", d, create, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for x in glob.glob(os.path.join(d, \"*\")):\n                rm_one(x)\n    else:",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "check_executable",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def check_executable(cmd):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        result = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n        if not isinstance(result, str):\n            result = result.decode(\"utf-8\")\n        log.debug(\"Result: %s\" % (result+'\\n').split('\\n')[0])\n        return True\n    except Exception as e:\n        log.debug('Failed: %s' % e)",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "determine_opencv_version",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def determine_opencv_version(version_hpp_path):\n    # version in 2.4 - CV_VERSION_EPOCH.CV_VERSION_MAJOR.CV_VERSION_MINOR.CV_VERSION_REVISION\n    # version in master - CV_VERSION_MAJOR.CV_VERSION_MINOR.CV_VERSION_REVISION-CV_VERSION_STATUS\n    with open(version_hpp_path, \"rt\") as f:\n        data = f.read()\n        major = re.search(r'^#define\\W+CV_VERSION_MAJOR\\W+(\\d+)$', data, re.MULTILINE).group(1)\n        minor = re.search(r'^#define\\W+CV_VERSION_MINOR\\W+(\\d+)$', data, re.MULTILINE).group(1)\n        revision = re.search(r'^#define\\W+CV_VERSION_REVISION\\W+(\\d+)$', data, re.MULTILINE).group(1)\n        version_status = re.search(r'^#define\\W+CV_VERSION_STATUS\\W+\"([^\"]*)\"$', data, re.MULTILINE).group(1)\n        return \"%(major)s.%(minor)s.%(revision)s%(version_status)s\" % locals()",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "move_smart",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def move_smart(src, dst):\n    def move_recurse(subdir):\n        s = os.path.join(src, subdir)\n        d = os.path.join(dst, subdir)\n        if os.path.exists(d):\n            if os.path.isdir(d):\n                for item in os.listdir(s):\n                    move_recurse(os.path.join(subdir, item))\n            elif os.path.isfile(s):\n                shutil.move(s, d)",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "copytree_smart",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def copytree_smart(src, dst):\n    def copy_recurse(subdir):\n        s = os.path.join(src, subdir)\n        d = os.path.join(dst, subdir)\n        if os.path.exists(d):\n            if os.path.isdir(d):\n                for item in os.listdir(s):\n                    copy_recurse(os.path.join(subdir, item))\n            elif os.path.isfile(s):\n                shutil.copy2(s, d)",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "get_highest_version",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def get_highest_version(subdirs):\n    return max(subdirs, key=lambda dir: [int(comp) for comp in os.path.split(dir)[-1].split('.')])\n#===================================================================================================\nclass ABI:\n    def __init__(self, platform_id, name, toolchain, ndk_api_level = None, cmake_vars = dict()):\n        self.platform_id = platform_id # platform code to add to apk version (for cmake)\n        self.name = name # general name (official Android ABI identifier)\n        self.toolchain = toolchain # toolchain identifier (for cmake)\n        self.cmake_vars = dict(\n            ANDROID_STL=\"gnustl_static\",",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "get_ndk_dir",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "def get_ndk_dir():\n    # look to see if Android NDK is installed\n    android_sdk_ndk = os.path.join(os.environ[\"ANDROID_SDK\"], 'ndk')\n    android_sdk_ndk_bundle = os.path.join(os.environ[\"ANDROID_SDK\"], 'ndk-bundle')\n    if os.path.exists(android_sdk_ndk):\n        ndk_subdirs = [f for f in os.listdir(android_sdk_ndk) if os.path.exists(os.path.join(android_sdk_ndk, f, 'package.xml'))]\n        if len(ndk_subdirs) > 0:\n            # there could be more than one - get the most recent\n            ndk_from_sdk = os.path.join(android_sdk_ndk, get_highest_version(ndk_subdirs))\n            log.info(\"Using NDK (side-by-side) from Android SDK: %s\", ndk_from_sdk)",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "SCRIPT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.build_sdk",
        "description": "Hw_2.opencv.platforms.android.build_sdk",
        "peekOfCode": "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nclass Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))",
        "detail": "Hw_2.opencv.platforms.android.build_sdk",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-10.config",
        "description": "Hw_2.opencv.platforms.android.ndk-10.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", \"arm-linux-androideabi-4.8\", cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"1\", \"armeabi\",     \"arm-linux-androideabi-4.8\"),\n    ABI(\"3\", \"arm64-v8a\",   \"aarch64-linux-android-4.9\"),\n    ABI(\"5\", \"x86_64\",      \"x86_64-4.9\"),\n    ABI(\"4\", \"x86\",         \"x86-4.8\"),\n    ABI(\"7\", \"mips64\",      \"mips64el-linux-android-4.9\"),\n    ABI(\"6\", \"mips\",        \"mipsel-linux-android-4.8\")\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-10.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-16.config",
        "description": "Hw_2.opencv.platforms.android.ndk-16.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", \"arm-linux-androideabi-4.9\", cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"1\", \"armeabi\",     \"arm-linux-androideabi-4.9\", cmake_vars=dict(WITH_TBB='OFF')),\n    ABI(\"3\", \"arm64-v8a\",   \"aarch64-linux-android-4.9\"),\n    ABI(\"5\", \"x86_64\",      \"x86_64-4.9\"),\n    ABI(\"4\", \"x86\",         \"x86-4.9\"),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-16.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-17.config",
        "description": "Hw_2.opencv.platforms.android.ndk-17.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"3\", \"arm64-v8a\",   None),\n    ABI(\"5\", \"x86_64\",      None),\n    ABI(\"4\", \"x86\",         None),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-17.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-18-api-level-21.config",
        "description": "Hw_2.opencv.platforms.android.ndk-18-api-level-21.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, 21, cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"3\", \"arm64-v8a\",   None, 21),\n    ABI(\"5\", \"x86_64\",      None, 21),\n    ABI(\"4\", \"x86\",         None, 21),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-18-api-level-21.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-18-api-level-24.config",
        "description": "Hw_2.opencv.platforms.android.ndk-18-api-level-24.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, 24, cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"3\", \"arm64-v8a\",   None, 24),\n    ABI(\"5\", \"x86_64\",      None, 24),\n    ABI(\"4\", \"x86\",         None, 24),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-18-api-level-24.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-18.config",
        "description": "Hw_2.opencv.platforms.android.ndk-18.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON')),\n    ABI(\"3\", \"arm64-v8a\",   None),\n    ABI(\"5\", \"x86_64\",      None),\n    ABI(\"4\", \"x86\",         None),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-18.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-22.config",
        "description": "Hw_2.opencv.platforms.android.ndk-22.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, cmake_vars=dict(ANDROID_ABI='armeabi-v7a with NEON', ANDROID_GRADLE_PLUGIN_VERSION='4.1.2', GRADLE_VERSION='6.5', KOTLIN_PLUGIN_VERSION='1.5.10')),\n    ABI(\"3\", \"arm64-v8a\",   None, cmake_vars=dict(ANDROID_GRADLE_PLUGIN_VERSION='4.1.2', GRADLE_VERSION='6.5', KOTLIN_PLUGIN_VERSION='1.5.10')),\n    ABI(\"5\", \"x86_64\",      None, cmake_vars=dict(ANDROID_GRADLE_PLUGIN_VERSION='4.1.2', GRADLE_VERSION='6.5', KOTLIN_PLUGIN_VERSION='1.5.10')),\n    ABI(\"4\", \"x86\",         None, cmake_vars=dict(ANDROID_GRADLE_PLUGIN_VERSION='4.1.2', GRADLE_VERSION='6.5', KOTLIN_PLUGIN_VERSION='1.5.10')),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-22.config",
        "documentation": {}
    },
    {
        "label": "ANDROID_NATIVE_API_LEVEL",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-25.config",
        "description": "Hw_2.opencv.platforms.android.ndk-25.config",
        "peekOfCode": "ANDROID_NATIVE_API_LEVEL = int(os.environ.get('ANDROID_NATIVE_API_LEVEL', 32))\ncmake_common_vars = {\n    # Docs: https://source.android.com/docs/setup/about/build-numbers\n    # Docs: https://developer.android.com/studio/publish/versioning\n    'ANDROID_COMPILE_SDK_VERSION': os.environ.get('ANDROID_COMPILE_SDK_VERSION', 32),\n    'ANDROID_TARGET_SDK_VERSION': os.environ.get('ANDROID_TARGET_SDK_VERSION', 32),\n    'ANDROID_MIN_SDK_VERSION': os.environ.get('ANDROID_MIN_SDK_VERSION', ANDROID_NATIVE_API_LEVEL),\n    # Docs: https://developer.android.com/studio/releases/gradle-plugin\n    'ANDROID_GRADLE_PLUGIN_VERSION': '7.3.1',\n    'GRADLE_VERSION': '7.5.1',",
        "detail": "Hw_2.opencv.platforms.android.ndk-25.config",
        "documentation": {}
    },
    {
        "label": "cmake_common_vars",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-25.config",
        "description": "Hw_2.opencv.platforms.android.ndk-25.config",
        "peekOfCode": "cmake_common_vars = {\n    # Docs: https://source.android.com/docs/setup/about/build-numbers\n    # Docs: https://developer.android.com/studio/publish/versioning\n    'ANDROID_COMPILE_SDK_VERSION': os.environ.get('ANDROID_COMPILE_SDK_VERSION', 32),\n    'ANDROID_TARGET_SDK_VERSION': os.environ.get('ANDROID_TARGET_SDK_VERSION', 32),\n    'ANDROID_MIN_SDK_VERSION': os.environ.get('ANDROID_MIN_SDK_VERSION', ANDROID_NATIVE_API_LEVEL),\n    # Docs: https://developer.android.com/studio/releases/gradle-plugin\n    'ANDROID_GRADLE_PLUGIN_VERSION': '7.3.1',\n    'GRADLE_VERSION': '7.5.1',\n    'KOTLIN_PLUGIN_VERSION': '1.5.20',",
        "detail": "Hw_2.opencv.platforms.android.ndk-25.config",
        "documentation": {}
    },
    {
        "label": "ABIs",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.android.ndk-25.config",
        "description": "Hw_2.opencv.platforms.android.ndk-25.config",
        "peekOfCode": "ABIs = [\n    ABI(\"2\", \"armeabi-v7a\", None, ndk_api_level=ANDROID_NATIVE_API_LEVEL, cmake_vars=cmake_common_vars),\n    ABI(\"3\", \"arm64-v8a\",   None, ndk_api_level=ANDROID_NATIVE_API_LEVEL, cmake_vars=cmake_common_vars),\n    ABI(\"5\", \"x86_64\",      None, ndk_api_level=ANDROID_NATIVE_API_LEVEL, cmake_vars=cmake_common_vars),\n    ABI(\"4\", \"x86\",         None, ndk_api_level=ANDROID_NATIVE_API_LEVEL, cmake_vars=cmake_common_vars),\n]",
        "detail": "Hw_2.opencv.platforms.android.ndk-25.config",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)\ndef print_header(text):\n    print(\"=\"*60)\n    print(text)\n    print(\"=\"*60)",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_header",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def print_header(text):\n    print(\"=\"*60)\n    print(text)\n    print(\"=\"*60)\ndef print_error(text):\n    print(\"=\"*60, file=sys.stderr)\n    print(\"ERROR: %s\" % text, file=sys.stderr)\n    print(\"=\"*60, file=sys.stderr)\ndef get_xcode_major():\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "print_error",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def print_error(text):\n    print(\"=\"*60, file=sys.stderr)\n    print(\"ERROR: %s\" % text, file=sys.stderr)\n    print(\"=\"*60, file=sys.stderr)\ndef get_xcode_major():\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')\n    m = re.match(r'Xcode\\s+(\\d+)\\..*', ret, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    else:",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_major",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def get_xcode_major():\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')\n    m = re.match(r'Xcode\\s+(\\d+)\\..*', ret, flags=re.IGNORECASE)\n    if m:\n        return int(m.group(1))\n    else:\n        raise Exception(\"Failed to parse Xcode version\")\ndef get_xcode_version():\n    \"\"\"\n    Returns the major and minor version of the current Xcode",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_version",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def get_xcode_version():\n    \"\"\"\n    Returns the major and minor version of the current Xcode\n    command line tools as a tuple of (major, minor)\n    \"\"\"\n    ret = check_output([\"xcodebuild\", \"-version\"]).decode('utf-8')\n    m = re.match(r'Xcode\\s+(\\d+)\\.(\\d+)', ret, flags=re.IGNORECASE)\n    if m:\n        return (int(m.group(1)), int(m.group(2)))\n    else:",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_xcode_setting",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def get_xcode_setting(var, projectdir):\n    ret = check_output([\"xcodebuild\", \"-showBuildSettings\"], cwd = projectdir).decode('utf-8')\n    m = re.search(\"\\s\" + var + \" = (.*)\", ret)\n    if m:\n        return m.group(1)\n    else:\n        raise Exception(\"Failed to parse Xcode settings\")\ndef get_cmake_version():\n    \"\"\"\n    Returns the major and minor version of the current CMake",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "get_cmake_version",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "description": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "peekOfCode": "def get_cmake_version():\n    \"\"\"\n    Returns the major and minor version of the current CMake\n    command line tools as a tuple of (major, minor, revision)\n    \"\"\"\n    ret = check_output([\"cmake\", \"--version\"]).decode('utf-8')\n    m = re.match(r'cmake\\sversion\\s+(\\d+)\\.(\\d+).(\\d+)', ret, flags=re.IGNORECASE)\n    if m:\n        return (int(m.group(1)), int(m.group(2)), int(m.group(3)))\n    else:",
        "detail": "Hw_2.opencv.platforms.apple.cv_build_utils",
        "documentation": {}
    },
    {
        "label": "DocBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.build_docs",
        "description": "Hw_2.opencv.platforms.ios.build_docs",
        "peekOfCode": "class DocBuilder:\n    def __init__(self, script_dir, framework_dir, output_dir, framework_header, framework_name, arch, target):\n        self.script_dir = script_dir\n        self.framework_dir = framework_dir\n        self.output_dir = output_dir\n        self.framework_header = framework_header\n        self.framework_name = framework_name\n        self.arch = arch\n        self.target = target\n    def _build(self):",
        "detail": "Hw_2.opencv.platforms.ios.build_docs",
        "documentation": {}
    },
    {
        "label": "iOSDocBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.build_docs",
        "description": "Hw_2.opencv.platforms.ios.build_docs",
        "peekOfCode": "class iOSDocBuilder(DocBuilder):\n    def getToolchain(self):\n        return None\nif __name__ == \"__main__\":\n    script_dir = os.path.abspath(os.path.dirname(sys.argv[0]))\n    parser = argparse.ArgumentParser(description='The script builds OpenCV docs for iOS.')\n    parser.add_argument('framework_dir', metavar='FRAMEWORK_DIR', help='folder where framework build files are located')\n    parser.add_argument('--output_dir', default=None, help='folder where docs will be built (default is \"../doc_build\" relative to framework_dir)')\n    parser.add_argument('--framework_header', default=None, help='umbrella header for OpenCV framework (default is \"../../../lib/Release/{framework_name}.framework/Headers/{framework_name}.h\")')\n    parser.add_argument('--framework_name', default='opencv2', help='Name of OpenCV framework (default: opencv2, will change to OpenCV in future version)')",
        "detail": "Hw_2.opencv.platforms.ios.build_docs",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.ios.build_docs",
        "description": "Hw_2.opencv.platforms.ios.build_docs",
        "peekOfCode": "def execute(cmd, cwd = None, output = None):\n    if not output:\n        print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n        print('Executing: ' + ' '.join(cmd))\n        retcode = check_call(cmd, cwd = cwd)\n        if retcode != 0:\n            raise Exception(\"Child returned:\", retcode)\n    else:\n        with open(output, \"a\") as f:\n            f.flush()",
        "detail": "Hw_2.opencv.platforms.ios.build_docs",
        "documentation": {}
    },
    {
        "label": "Builder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.build_framework",
        "description": "Hw_2.opencv.platforms.ios.build_framework",
        "peekOfCode": "class Builder:\n    def __init__(self, opencv, contrib, dynamic, bitcodedisabled, exclude, disable, enablenonfree, targets, debug, debug_info, framework_name, run_tests, build_docs, swiftdisabled):\n        self.opencv = os.path.abspath(opencv)\n        self.contrib = None\n        if contrib:\n            modpath = os.path.join(contrib, \"modules\")\n            if os.path.isdir(modpath):\n                self.contrib = os.path.abspath(modpath)\n            else:\n                print(\"Note: contrib repository is bad - modules subfolder not found\", file=sys.stderr)",
        "detail": "Hw_2.opencv.platforms.ios.build_framework",
        "documentation": {}
    },
    {
        "label": "iOSBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.build_framework",
        "description": "Hw_2.opencv.platforms.ios.build_framework",
        "peekOfCode": "class iOSBuilder(Builder):\n    def getToolchain(self, arch, target):\n        toolchain = os.path.join(self.opencv, \"platforms\", \"ios\", \"cmake\", \"Toolchains\", \"Toolchain-%s_Xcode.cmake\" % target)\n        return toolchain\n    def getCMakeArgs(self, arch, target):\n        args = Builder.getCMakeArgs(self, arch, target)\n        args = args + [\n            '-DIOS_ARCH=%s' % arch\n        ]\n        return args",
        "detail": "Hw_2.opencv.platforms.ios.build_framework",
        "documentation": {}
    },
    {
        "label": "TestRunner",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.run_tests",
        "description": "Hw_2.opencv.platforms.ios.run_tests",
        "peekOfCode": "class TestRunner:\n    def __init__(self, script_dir, tests_dir, build_dir, framework_dir, framework_name, arch, target, platform):\n        self.script_dir = script_dir\n        self.tests_dir = tests_dir\n        self.build_dir = build_dir\n        self.framework_dir = framework_dir\n        self.framework_name = framework_name\n        self.arch = arch\n        self.target = target\n        self.platform = platform",
        "detail": "Hw_2.opencv.platforms.ios.run_tests",
        "documentation": {}
    },
    {
        "label": "iOSTestRunner",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.ios.run_tests",
        "description": "Hw_2.opencv.platforms.ios.run_tests",
        "peekOfCode": "class iOSTestRunner(TestRunner):\n    def getToolchain(self):\n        toolchain = os.path.join(self.script_dir, \"cmake\", \"Toolchains\", \"Toolchain-%s_Xcode.cmake\" % self.target)\n        return toolchain\n    def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            \"-DIOS_ARCH=%s\" % self.arch,\n            \"-DIPHONEOS_DEPLOYMENT_TARGET=%s\" % os.environ['IPHONEOS_DEPLOYMENT_TARGET'],\n        ]",
        "detail": "Hw_2.opencv.platforms.ios.run_tests",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.ios.run_tests",
        "description": "Hw_2.opencv.platforms.ios.run_tests",
        "peekOfCode": "def execute(cmd, cwd = None):\n    print(\"Executing: %s in %s\" % (cmd, cwd), file=sys.stderr)\n    print('Executing: ' + ' '.join(cmd))\n    retcode = check_call(cmd, cwd = cwd)\n    if retcode != 0:\n        raise Exception(\"Child returned:\", retcode)\nclass TestRunner:\n    def __init__(self, script_dir, tests_dir, build_dir, framework_dir, framework_name, arch, target, platform):\n        self.script_dir = script_dir\n        self.tests_dir = tests_dir",
        "detail": "Hw_2.opencv.platforms.ios.run_tests",
        "documentation": {}
    },
    {
        "label": "Fail",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "class Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, shell=False):\n    try:\n        log.info(\"Executing: %s\" % cmd)\n        env = os.environ.copy()\n        env['VERBOSE'] = '1'",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "Builder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "class Builder:\n    def __init__(self, options):\n        self.options = options\n        self.build_dir = check_dir(options.build_dir, create=True)\n        self.opencv_dir = check_dir(options.opencv_dir)\n        print('-----------------------------------------------------------')\n        print('options.opencv_dir:', options.opencv_dir)\n        self.emscripten_dir = check_dir(options.emscripten_dir)\n    def get_toolchain_file(self):\n        return os.path.join(self.emscripten_dir, \"cmake\", \"Modules\", \"Platform\", \"Emscripten.cmake\")",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "def execute(cmd, shell=False):\n    try:\n        log.info(\"Executing: %s\" % cmd)\n        env = os.environ.copy()\n        env['VERBOSE'] = '1'\n        retcode = subprocess.call(cmd, shell=shell, env=env)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:\n            raise Fail(\"Child returned: %s\" % retcode)",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "rm_one",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "def rm_one(d):\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)\ndef check_dir(d, create=False, clean=False):",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "check_dir",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "def check_dir(d, create=False, clean=False):\n    d = os.path.abspath(d)\n    log.info(\"Check dir %s (create: %s, clean: %s)\", d, create, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for x in glob.glob(os.path.join(d, \"*\")):\n                rm_one(x)\n    else:",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "check_file",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "def check_file(d):\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isfile(d):\n            return True\n        else:\n            return False\n    return False\ndef find_file(name, path):\n    for root, dirs, files in os.walk(path):",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "find_file",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "def find_file(name, path):\n    for root, dirs, files in os.walk(path):\n        if name in files:\n            return os.path.join(root, name)\nclass Builder:\n    def __init__(self, options):\n        self.options = options\n        self.build_dir = check_dir(options.build_dir, create=True)\n        self.opencv_dir = check_dir(options.opencv_dir)\n        print('-----------------------------------------------------------')",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "SCRIPT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.build_js",
        "description": "Hw_2.opencv.platforms.js.build_js",
        "peekOfCode": "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nclass Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, shell=False):\n    try:\n        log.info(\"Executing: %s\" % cmd)\n        env = os.environ.copy()",
        "detail": "Hw_2.opencv.platforms.js.build_js",
        "documentation": {}
    },
    {
        "label": "core",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "core = {\n    '': [\n        'absdiff', 'add', 'addWeighted', 'bitwise_and', 'bitwise_not', 'bitwise_or', 'bitwise_xor', 'cartToPolar',\n        'compare', 'convertScaleAbs', 'copyMakeBorder', 'countNonZero', 'determinant', 'dft', 'divide', 'eigen',\n        'exp', 'flip', 'getOptimalDFTSize','gemm', 'hconcat', 'inRange', 'invert', 'kmeans', 'log', 'magnitude',\n        'max', 'mean', 'meanStdDev', 'merge', 'min', 'minMaxLoc', 'mixChannels', 'multiply', 'norm', 'normalize',\n        'perspectiveTransform', 'polarToCart', 'pow', 'randn', 'randu', 'reduce', 'repeat', 'rotate', 'setIdentity', 'setRNGSeed',\n        'solve', 'solvePoly', 'split', 'sqrt', 'subtract', 'trace', 'transform', 'transpose', 'vconcat',\n        'setLogLevel', 'getLogLevel',\n    ],",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "imgproc",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "imgproc = {\n    '': [\n        'Canny',\n        'GaussianBlur',\n        'Laplacian',\n        'HoughLines',\n        'HoughLinesP',\n        'HoughCircles',\n        'Scharr',\n        'Sobel',",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "objdetect",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "objdetect = {'': ['groupRectangles'],\n             'HOGDescriptor': ['load', 'HOGDescriptor', 'getDefaultPeopleDetector', 'getDaimlerPeopleDetector', 'setSVMDetector', 'detectMultiScale'],\n             'CascadeClassifier': ['load', 'detectMultiScale2', 'CascadeClassifier', 'detectMultiScale3', 'empty', 'detectMultiScale'],\n             'QRCodeDetector': ['QRCodeDetector', 'decode', 'decodeCurved', 'detect', 'detectAndDecode', 'detectMulti', 'setEpsX', 'setEpsY'],\n             'ArucoDetector': ['getPredefinedDictionary', 'detectMarkers', 'refineDetectedMarkers', 'getDictionary', 'setDictionary', 'getDetectorParameters', 'setDetectorParameters', 'getRefineParameters', 'setRefineParameters'],\n             'GridBoard': ['create','generateImage', 'getGridSize', 'getMarkerLength', 'getMarkerSeparation'],\n             'CharucoBoard': ['create', 'generateImage', 'getChessboardCorners', 'getNearestMarkerCorners', 'checkCharucoCornersCollinear']\n}\nvideo = {\n    '': [",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "video",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "video = {\n    '': [\n        'CamShift',\n        'calcOpticalFlowFarneback',\n        'calcOpticalFlowPyrLK',\n        'createBackgroundSubtractorMOG2',\n        'findTransformECC',\n        'meanShift',\n    ],\n    'BackgroundSubtractorMOG2': ['BackgroundSubtractorMOG2', 'apply'],",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "dnn",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "dnn = {'dnn_Net': ['setInput', 'forward', 'setPreferableBackend'],\n       '': ['readNetFromCaffe', 'readNetFromTensorflow', 'readNetFromTorch', 'readNetFromDarknet',\n            'readNetFromONNX', 'readNetFromTFLite', 'readNet', 'blobFromImage']}\nfeatures2d = {'Feature2D': ['detect', 'compute', 'detectAndCompute', 'descriptorSize', 'descriptorType', 'defaultNorm', 'empty', 'getDefaultName'],\n              'BRISK': ['create', 'getDefaultName'],\n              'ORB': ['create', 'setMaxFeatures', 'setScaleFactor', 'setNLevels', 'setEdgeThreshold', 'setFastThreshold', 'setFirstLevel', 'setWTA_K', 'setScoreType', 'setPatchSize', 'getFastThreshold', 'getDefaultName'],\n              'MSER': ['create', 'detectRegions', 'setDelta', 'getDelta', 'setMinArea', 'getMinArea', 'setMaxArea', 'getMaxArea', 'setPass2Only', 'getPass2Only', 'getDefaultName'],\n              'FastFeatureDetector': ['create', 'setThreshold', 'getThreshold', 'setNonmaxSuppression', 'getNonmaxSuppression', 'setType', 'getType', 'getDefaultName'],\n              'AgastFeatureDetector': ['create', 'setThreshold', 'getThreshold', 'setNonmaxSuppression', 'getNonmaxSuppression', 'setType', 'getType', 'getDefaultName'],\n              'GFTTDetector': ['create', 'setMaxFeatures', 'getMaxFeatures', 'setQualityLevel', 'getQualityLevel', 'setMinDistance', 'getMinDistance', 'setBlockSize', 'getBlockSize', 'setHarrisDetector', 'getHarrisDetector', 'setK', 'getK', 'getDefaultName'],",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "features2d",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "features2d = {'Feature2D': ['detect', 'compute', 'detectAndCompute', 'descriptorSize', 'descriptorType', 'defaultNorm', 'empty', 'getDefaultName'],\n              'BRISK': ['create', 'getDefaultName'],\n              'ORB': ['create', 'setMaxFeatures', 'setScaleFactor', 'setNLevels', 'setEdgeThreshold', 'setFastThreshold', 'setFirstLevel', 'setWTA_K', 'setScoreType', 'setPatchSize', 'getFastThreshold', 'getDefaultName'],\n              'MSER': ['create', 'detectRegions', 'setDelta', 'getDelta', 'setMinArea', 'getMinArea', 'setMaxArea', 'getMaxArea', 'setPass2Only', 'getPass2Only', 'getDefaultName'],\n              'FastFeatureDetector': ['create', 'setThreshold', 'getThreshold', 'setNonmaxSuppression', 'getNonmaxSuppression', 'setType', 'getType', 'getDefaultName'],\n              'AgastFeatureDetector': ['create', 'setThreshold', 'getThreshold', 'setNonmaxSuppression', 'getNonmaxSuppression', 'setType', 'getType', 'getDefaultName'],\n              'GFTTDetector': ['create', 'setMaxFeatures', 'getMaxFeatures', 'setQualityLevel', 'getQualityLevel', 'setMinDistance', 'getMinDistance', 'setBlockSize', 'getBlockSize', 'setHarrisDetector', 'getHarrisDetector', 'setK', 'getK', 'getDefaultName'],\n              # 'SimpleBlobDetector': ['create'],\n              'KAZE': ['create', 'setExtended', 'getExtended', 'setUpright', 'getUpright', 'setThreshold', 'getThreshold', 'setNOctaves', 'getNOctaves', 'setNOctaveLayers', 'getNOctaveLayers', 'setDiffusivity', 'getDiffusivity', 'getDefaultName'],\n              'AKAZE': ['create', 'setDescriptorType', 'getDescriptorType', 'setDescriptorSize', 'getDescriptorSize', 'setDescriptorChannels', 'getDescriptorChannels', 'setThreshold', 'getThreshold', 'setNOctaves', 'getNOctaves', 'setNOctaveLayers', 'getNOctaveLayers', 'setDiffusivity', 'getDiffusivity', 'getDefaultName'],",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "photo",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "photo = {'': ['createAlignMTB', 'createCalibrateDebevec', 'createCalibrateRobertson', \\\n              'createMergeDebevec', 'createMergeMertens', 'createMergeRobertson', \\\n              'createTonemapDrago', 'createTonemapMantiuk', 'createTonemapReinhard', 'inpaint'],\n        'CalibrateCRF': ['process'],\n        'AlignMTB' : ['calculateShift', 'shiftMat', 'computeBitmaps', 'getMaxBits', 'setMaxBits', \\\n                      'getExcludeRange', 'setExcludeRange', 'getCut', 'setCut'],\n        'CalibrateDebevec' : ['getLambda', 'setLambda', 'getSamples', 'setSamples', 'getRandom', 'setRandom'],\n        'CalibrateRobertson' : ['getMaxIter', 'setMaxIter', 'getThreshold', 'setThreshold', 'getRadiance'],\n        'MergeExposures' : ['process'],\n        'MergeDebevec' : ['process'],",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "calib3d",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "calib3d = {\n    '': [\n        'findHomography',\n        'calibrateCameraExtended',\n        'drawFrameAxes',\n        'estimateAffine2D',\n        'getDefaultNewCameraMatrix',\n        'initUndistortRectifyMap',\n        'Rodrigues',\n        'solvePnP',",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "white_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.js.opencv_js.config",
        "description": "Hw_2.opencv.platforms.js.opencv_js.config",
        "peekOfCode": "white_list = makeWhiteList([core, imgproc, objdetect, video, dnn, features2d, photo, calib3d])\n# namespace_prefix_override['dnn'] = ''  # compatibility stuff (enabled by default)\n# namespace_prefix_override['aruco'] = ''  # compatibility stuff (enabled by default)",
        "detail": "Hw_2.opencv.platforms.js.opencv_js.config",
        "documentation": {}
    },
    {
        "label": "OSXDocBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.osx.build_docs",
        "description": "Hw_2.opencv.platforms.osx.build_docs",
        "peekOfCode": "class OSXDocBuilder(DocBuilder):\n    def getToolchain(self):\n        return None\nif __name__ == \"__main__\":\n    script_dir = os.path.abspath(os.path.dirname(sys.argv[0]))\n    parser = argparse.ArgumentParser(description='The script builds OpenCV docs for macOS.')\n    parser.add_argument('framework_dir', metavar='FRAMEWORK_DIR', help='folder where framework build files are located')\n    parser.add_argument('--output_dir', default=None, help='folder where docs will be built (default is \"../doc_build\" relative to framework_dir)')\n    parser.add_argument('--framework_header', default=None, help='umbrella header for OpenCV framework (default is \"../../../lib/Release/{framework_name}.framework/Headers/{framework_name}.h\")')\n    parser.add_argument('--framework_name', default='opencv2', help='Name of OpenCV framework (default: opencv2, will change to OpenCV in future version)')",
        "detail": "Hw_2.opencv.platforms.osx.build_docs",
        "documentation": {}
    },
    {
        "label": "OSXBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.osx.build_framework",
        "description": "Hw_2.opencv.platforms.osx.build_framework",
        "peekOfCode": "class OSXBuilder(Builder):\n    def checkCMakeVersion(self):\n        assert get_cmake_version() >= (3, 17), \"CMake 3.17 or later is required. Current version is {}\".format(get_cmake_version())\n    def getObjcTarget(self, target):\n        # Obj-C generation target\n        if target == \"Catalyst\":\n            return 'ios'\n        else:\n            return 'osx'\n    def getToolchain(self, arch, target):",
        "detail": "Hw_2.opencv.platforms.osx.build_framework",
        "documentation": {}
    },
    {
        "label": "OSXTestRunner",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.osx.run_tests",
        "description": "Hw_2.opencv.platforms.osx.run_tests",
        "peekOfCode": "class OSXTestRunner(TestRunner):\n    def getToolchain(self):\n        return None\n    def getCMakeArgs(self):\n        args = TestRunner.getCMakeArgs(self)\n        args = args + [\n            '-DMACOSX_DEPLOYMENT_TARGET=%s' % os.environ['MACOSX_DEPLOYMENT_TARGET']\n        ]\n        return args\nif __name__ == \"__main__\":",
        "detail": "Hw_2.opencv.platforms.osx.run_tests",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_nn_builder')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builderd.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builderd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.srcdir / 'ngraph/LICENSE', sysroot_license_dir / 'ngraph-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_legacy')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.srcdir / 'ngraph/LICENSE', sysroot_license_dir / 'ngraph-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_legacy')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cldnn_global_custom_kernels')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_nn_builder.lib', sysroot_ie_lib_dir / 'inference_engine_nn_builder.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.srcdir / 'ngraph/LICENSE', sysroot_license_dir / 'ngraph-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.3.0.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\ncopy_dll('inference_engine_legacy')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.srcdir / 'ngraph/LICENSE', sysroot_license_dir / 'ngraph-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2020.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\ncopy_dll('inference_engine_legacy')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.srcdir / 'ngraph/LICENSE', sysroot_license_dir / 'ngraph-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "os.environ['CI_BUILD_NUMBER']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "peekOfCode": "os.environ['CI_BUILD_NUMBER'] = '2021.2.0-opencv_winpack_dldt'\ncmake_vars['ENABLE_V10_SERIALIZE'] = 'ON'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "documentation": {}
    },
    {
        "label": "cmake_vars['ENABLE_V10_SERIALIZE']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "peekOfCode": "cmake_vars['ENABLE_V10_SERIALIZE'] = 'ON'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.build.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\ncopy_dll('inference_engine_legacy')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(self.build_dir / 'install' / 'lib' / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "os.environ['CI_BUILD_NUMBER']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "peekOfCode": "os.environ['CI_BUILD_NUMBER'] = '2021.3.0-opencv_winpack_dldt'\ncmake_vars['ENABLE_V10_SERIALIZE'] = 'ON'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "documentation": {}
    },
    {
        "label": "cmake_vars['ENABLE_V10_SERIALIZE']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "peekOfCode": "cmake_vars['ENABLE_V10_SERIALIZE'] = 'ON'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.build.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\ncopy_dll('inference_engine_ir_v7_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.3.sysroot.config",
        "documentation": {}
    },
    {
        "label": "os.environ['CI_BUILD_NUMBER']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.build.config",
        "peekOfCode": "os.environ['CI_BUILD_NUMBER'] = '2021.4.0-opencv_winpack_dldt'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.build.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\n#copy_dll('inference_engine_ir_v7_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.sysroot.config",
        "documentation": {}
    },
    {
        "label": "os.environ['CI_BUILD_NUMBER']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.build.config",
        "peekOfCode": "os.environ['CI_BUILD_NUMBER'] = '2021.4.1-opencv_winpack_dldt'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.build.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\n#copy_dll('inference_engine_ir_v7_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.1.sysroot.config",
        "documentation": {}
    },
    {
        "label": "os.environ['CI_BUILD_NUMBER']",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.build.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.build.config",
        "peekOfCode": "os.environ['CI_BUILD_NUMBER'] = '2021.4.2-opencv_winpack_dldt'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.build.config",
        "documentation": {}
    },
    {
        "label": "copy_bin",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "def copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "copy_dll",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "def copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')\n#copy_dll('inference_engine_ir_v7_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "sysroot_bin_dir = prepare_dir(self.sysrootdir / 'bin')\ncopytree(self.build_dir / 'install', self.sysrootdir / 'ngraph')\n#rm_one(self.sysrootdir / 'ngraph' / 'lib' / 'ngraph.dll')\nbuild_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_config",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "build_config = 'Release' if not self.config.build_debug else 'Debug'\nbuild_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "build_bin_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "build_bin_dir = self.build_dir / 'bin' / 'intel64' / build_config\ndef copy_bin(name):\n    global build_bin_dir, sysroot_bin_dir\n    copytree(build_bin_dir / name, sysroot_bin_dir / name)\ndll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "dll_suffix",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "dll_suffix = 'd' if self.config.build_debug else ''\ndef copy_dll(name):\n    global copy_bin, dll_suffix\n    copy_bin(name + dll_suffix + '.dll')\n    copy_bin(name + dll_suffix + '.pdb')\ncopy_bin('cache.json')\ncopy_dll('clDNNPlugin')\ncopy_dll('HeteroPlugin')\ncopy_dll('inference_engine')\ncopy_dll('inference_engine_ir_reader')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "sysroot_ie_dir = prepare_dir(self.sysrootdir / 'deployment_tools' / 'inference_engine')\nsysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_ie_lib_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "sysroot_ie_lib_dir = prepare_dir(sysroot_ie_dir / 'lib' / 'intel64')\ncopytree(self.srcdir / 'inference-engine' / 'include', sysroot_ie_dir / 'include')\nif not self.config.build_debug:\n    copytree(build_bin_dir / 'ngraph.lib', sysroot_ie_lib_dir / 'ngraph.lib')\n    copytree(build_bin_dir / 'inference_engine.lib', sysroot_ie_lib_dir / 'inference_engine.lib')\n    copytree(build_bin_dir / 'inference_engine_ir_reader.lib', sysroot_ie_lib_dir / 'inference_engine_ir_reader.lib')\n    copytree(build_bin_dir / 'inference_engine_legacy.lib', sysroot_ie_lib_dir / 'inference_engine_legacy.lib')\nelse:\n    copytree(build_bin_dir / 'ngraphd.lib', sysroot_ie_lib_dir / 'ngraphd.lib')\n    copytree(build_bin_dir / 'inference_engined.lib', sysroot_ie_lib_dir / 'inference_engined.lib')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "sysroot_license_dir",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "description": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "peekOfCode": "sysroot_license_dir = prepare_dir(self.sysrootdir / 'etc' / 'licenses')\ncopytree(self.srcdir / 'LICENSE', sysroot_license_dir / 'dldt-LICENSE')\ncopytree(self.sysrootdir / 'tbb/LICENSE', sysroot_license_dir / 'tbb-LICENSE')",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.2021.4.2.sysroot.config",
        "documentation": {}
    },
    {
        "label": "Fail",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "class Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "BuilderDLDT",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "class BuilderDLDT:\n    def __init__(self, config):\n        self.config = config\n        cpath = self.config.dldt_config\n        log.info('DLDT build configuration: %s', cpath)\n        if not os.path.exists(cpath):\n            cpath = os.path.join(SCRIPT_DIR, cpath)\n            if not os.path.exists(cpath):\n                raise Fail('Config \"%s\" is missing' % cpath)\n        self.cpath = Path(cpath)",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "Builder",
        "kind": 6,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "class Builder:\n    def __init__(self, config):\n        self.config = config\n        build_dir_name = 'opencv_build' if not self.config.build_debug else 'opencv_build_debug'\n        self.build_dir = prepare_dir(Path(self.config.output_dir) / build_dir_name, clean=self.config.clean_opencv)\n        self.package_dir = prepare_dir(Path(self.config.output_dir) / 'package/opencv', clean=True)\n        self.install_dir = prepare_dir(self.package_dir / 'build')\n        self.src_dir = check_dir(self.config.opencv_dir)\n    def build(self, builderDLDT):\n        self.cmake_path = 'cmake'",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "execute",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))\n        if cwd:\n            log.info(\"    in: %s\" % cwd)\n        retcode = subprocess.call(cmd, shell=shell, cwd=str(cwd) if cwd else None)\n        if retcode < 0:\n            raise Fail(\"Child was terminated by signal: %s\" % -retcode)\n        elif retcode > 0:",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "check_executable",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def check_executable(cmd):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        result = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n        if not isinstance(result, str):\n            result = result.decode(\"utf-8\")\n        log.debug(\"Result: %s\" % (result + '\\n').split('\\n')[0])\n        return True\n    except OSError as e:\n        log.debug('Failed: %s' % e)",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "rm_one",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def rm_one(d):\n    d = str(d)  # Python 3.5 may not handle Path\n    d = os.path.abspath(d)\n    if os.path.exists(d):\n        if os.path.isdir(d):\n            log.info(\"Removing dir: %s\", d)\n            shutil.rmtree(d)\n        elif os.path.isfile(d):\n            log.info(\"Removing file: %s\", d)\n            os.remove(d)",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "prepare_dir",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def prepare_dir(d, clean=False):\n    d = str(d)  # Python 3.5 may not handle Path\n    d = os.path.abspath(d)\n    log.info(\"Preparing directory: '%s' (clean: %r)\", d, clean)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n        if clean:\n            for item in os.listdir(d):\n                rm_one(os.path.join(d, item))",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "check_dir",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def check_dir(d):\n    d = str(d)  # Python 3.5 may not handle Path\n    d = os.path.abspath(d)\n    log.info(\"Check directory: '%s'\", d)\n    if os.path.exists(d):\n        if not os.path.isdir(d):\n            raise Fail(\"Not a directory: %s\" % d)\n    else:\n        raise Fail(\"The directory is missing: %s\" % d)\n    return Path(d)",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "copytree",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def copytree(src, dst, exclude=None):\n    log.debug('copytree(%s, %s)', src, dst)\n    src = str(src)  # Python 3.5 may not handle Path\n    dst = str(dst)  # Python 3.5 may not handle Path\n    if os.path.isfile(src):\n        shutil.copy2(src, dst)\n        return\n    def copy_recurse(subdir):\n        if exclude and subdir in exclude:\n            log.debug('  skip: %s', subdir)",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "git_checkout",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def git_checkout(dst, url, branch, revision, clone_extra_args, noFetch=False):\n    assert isinstance(dst, Path)\n    log.info(\"Git checkout: '%s' (%s @ %s)\", dst, url, revision)\n    if noFetch:\n        pass\n    elif not os.path.exists(str(dst / '.git')):\n        execute(cmd=['git', 'clone'] +\n                (['-b', branch] if branch else []) +\n                clone_extra_args + [url, '.'], cwd=dst)\n    else:",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "git_apply_patch",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def git_apply_patch(src_dir, patch_file):\n    src_dir = str(src_dir)  # Python 3.5 may not handle Path\n    patch_file = str(patch_file)  # Python 3.5 may not handle Path\n    assert os.path.exists(patch_file), patch_file\n    execute(cmd=['git', 'apply', '--3way', '-v', '--ignore-space-change', str(patch_file)], cwd=src_dir)\n    execute(cmd=['git', '--no-pager', 'diff', 'HEAD'], cwd=src_dir)\n    os.environ['GIT_AUTHOR_NAME'] = os.environ['GIT_COMMITTER_NAME']='build'\n    os.environ['GIT_AUTHOR_EMAIL'] = os.environ['GIT_COMMITTER_EMAIL']='build@opencv.org'\n    execute(cmd=['git', 'commit', '-am', 'apply opencv patch'], cwd=src_dir)\n#===================================================================================================",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "def main():\n    dldt_src_url = 'https://github.com/openvinotoolkit/openvino'\n    dldt_src_commit = '2021.4.2'\n    dldt_config = None\n    dldt_release = None\n    build_cache_dir_default = os.environ.get('BUILD_CACHE_DIR', '.build_cache')\n    build_subst_drive = os.environ.get('BUILD_SUBST_DRIVE', None)\n    parser = argparse.ArgumentParser(\n            description='Build OpenCV Windows package with Inference Engine (DLDT)',\n    )",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "SCRIPT_DIR",
        "kind": 5,
        "importPath": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "description": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "peekOfCode": "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nclass Fail(Exception):\n    def __init__(self, text=None):\n        self.t = text\n    def __str__(self):\n        return \"ERROR\" if self.t is None else self.t\ndef execute(cmd, cwd=None, shell=False):\n    try:\n        log.debug(\"Executing: %s\" % cmd)\n        log.info('Executing: ' + ' '.join(cmd))",
        "detail": "Hw_2.opencv.platforms.winpack_dldt.build_package",
        "documentation": {}
    },
    {
        "label": "ClsAccEvaluation",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_accuracy_evaluator",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_accuracy_evaluator",
        "peekOfCode": "class ClsAccEvaluation:\n    log = sys.stdout\n    img_classes = {}\n    batch_size = 0\n    def __init__(self, log_path, img_classes_file, batch_size):\n        self.log = open(log_path, 'w')\n        self.img_classes = self.read_classes(img_classes_file)\n        self.batch_size = batch_size\n        # collect the accuracies for both models\n        self.general_quality_metric = []",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_accuracy_evaluator",
        "documentation": {}
    },
    {
        "label": "DataFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "peekOfCode": "class DataFetch(object):\n    imgs_dir = ''\n    frame_size = 0\n    bgr_to_rgb = False\n    __metaclass__ = ABCMeta\n    @abstractmethod\n    def preprocess(self, img):\n        pass\n    @staticmethod\n    def reshape_img(img):",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "documentation": {}
    },
    {
        "label": "PyTorchPreprocessedFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "peekOfCode": "class PyTorchPreprocessedFetch(DataFetch):\n    def __init__(self, pytorch_cls_config, preprocess_input=None):\n        self.imgs_dir = pytorch_cls_config.img_root_dir\n        self.frame_size = pytorch_cls_config.frame_size\n        self.bgr_to_rgb = pytorch_cls_config.bgr_to_rgb\n        self.preprocess_input = preprocess_input\n    def preprocess(self, img):\n        img = cv2.resize(img, (PYTORCH_RSZ_WIDTH, PYTORCH_RSZ_HEIGHT))\n        img = self.center_crop(img)\n        if self.preprocess_input:",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "documentation": {}
    },
    {
        "label": "TFPreprocessedFetch",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "peekOfCode": "class TFPreprocessedFetch(DataFetch):\n    def __init__(self, tf_cls_config, preprocess_input):\n        self.imgs_dir = tf_cls_config.img_root_dir\n        self.frame_size = tf_cls_config.frame_size\n        self.bgr_to_rgb = tf_cls_config.bgr_to_rgb\n        self.preprocess_input = preprocess_input\n    def preprocess(self, img):\n        img = self.initial_preprocess(img)\n        return self.preprocess_input(img)",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.evaluation.classification.cls_data_fetcher",
        "documentation": {}
    },
    {
        "label": "BASE_IMG_SCALE_FACTOR",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "BASE_IMG_SCALE_FACTOR = 1 / 255.0\nPYTORCH_RSZ_HEIGHT = 256\nPYTORCH_RSZ_WIDTH = 256\npytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": True,\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "PYTORCH_RSZ_HEIGHT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "PYTORCH_RSZ_HEIGHT = 256\nPYTORCH_RSZ_WIDTH = 256\npytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": True,\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),\n    \"rsz_width\": str(PYTORCH_RSZ_WIDTH)",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "PYTORCH_RSZ_WIDTH",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "PYTORCH_RSZ_WIDTH = 256\npytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": True,\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),\n    \"rsz_width\": str(PYTORCH_RSZ_WIDTH)\n}",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "pytorch_resize_input_blob",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "pytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": True,\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),\n    \"rsz_width\": str(PYTORCH_RSZ_WIDTH)\n}\npytorch_input_blob = {",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "pytorch_input_blob",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "pytorch_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": True\n}\ntf_input_blob = {\n    \"scale\": str(1 / 127.5),\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "tf_input_blob",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "tf_input_blob = {\n    \"scale\": str(1 / 127.5),\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],\n    \"std\": [],\n    \"crop\": \"True\",\n    \"rgb\": True\n}\ntf_model_blob_caffe_mode = {\n    \"mean\": [\"103.939\", \"116.779\", \"123.68\"],\n    \"scale\": \"1.0\",",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "tf_model_blob_caffe_mode",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "peekOfCode": "tf_model_blob_caffe_mode = {\n    \"mean\": [\"103.939\", \"116.779\", \"123.68\"],\n    \"scale\": \"1.0\",\n    \"std\": [],\n    \"crop\": \"True\",\n    \"rgb\": False\n}",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.default_preprocess_config",
        "documentation": {}
    },
    {
        "label": "CommonConfig",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "peekOfCode": "class CommonConfig:\n    output_data_root_dir: str = \"dnn_model_runner/dnn_conversion\"\n    logs_dir: str = os.path.join(output_data_root_dir, \"logs\")\n    log_file_path: str = os.path.join(logs_dir, \"{}_log.txt\")\n@dataclass\nclass TestClsConfig:\n    batch_size: int = 1\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "documentation": {}
    },
    {
        "label": "TestClsConfig",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "peekOfCode": "class TestClsConfig:\n    batch_size: int = 1\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching\n    img_cls_file: str = \"./val.txt\"\n    bgr_to_rgb: bool = True\n@dataclass\nclass TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "documentation": {}
    },
    {
        "label": "TestClsModuleConfig",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "peekOfCode": "class TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"\n    test_module_name: str = \"classification\"\n    test_module_path: str = \"classification.py\"\n    input_img: str = os.path.join(cls_test_data_dir, \"squirrel_cls.jpg\")\n    model: str = \"\"\n    frame_height: str = str(TestClsConfig.frame_size)\n    frame_width: str = str(TestClsConfig.frame_size)\n    scale: str = \"1.0\"\n    mean: List[str] = field(default_factory=lambda: [\"0.0\", \"0.0\", \"0.0\"])",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.configs.test_config",
        "documentation": {}
    },
    {
        "label": "ClsModelTestPipeline",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.cls_model_test_pipeline",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.cls_model_test_pipeline",
        "peekOfCode": "class ClsModelTestPipeline(ModelTestPipeline):\n    def __init__(\n            self,\n            network_model,\n            model_processor,\n            dnn_model_processor,\n            data_fetcher,\n            img_processor=None,\n            cls_args_parser=None,\n            default_input_blob_preproc=None",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.cls_model_test_pipeline",
        "documentation": {}
    },
    {
        "label": "ModelTestPipeline",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.model_test_pipeline",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.model_test_pipeline",
        "peekOfCode": "class ModelTestPipeline:\n    def __init__(\n            self,\n            network_model,\n            model_processor,\n            dnn_model_processor\n    ):\n        self._net_model = network_model\n        self._model_processor = model_processor\n        self._dnn_model_processor = dnn_model_processor",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.test.model_test_pipeline",
        "documentation": {}
    },
    {
        "label": "AbstractModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "peekOfCode": "class AbstractModel(ABC):\n    @abstractmethod\n    def get_prepared_models(self):\n        pass\nclass Framework(object):\n    in_blob_name = ''\n    out_blob_name = ''\n    __metaclass__ = ABCMeta\n    @abstractmethod\n    def get_name(self):",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "documentation": {}
    },
    {
        "label": "Framework",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "peekOfCode": "class Framework(object):\n    in_blob_name = ''\n    out_blob_name = ''\n    __metaclass__ = ABCMeta\n    @abstractmethod\n    def get_name(self):\n        pass\n    @abstractmethod\n    def get_output(self, input_blob):\n        pass",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.abstract_model",
        "documentation": {}
    },
    {
        "label": "read_rgb_img",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "peekOfCode": "def read_rgb_img(img_file, is_bgr_to_rgb=True):\n    img = cv2.imread(img_file, cv2.IMREAD_COLOR)\n    if is_bgr_to_rgb:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\ndef get_pytorch_preprocess(img):\n    img = img.astype(np.float32)\n    img *= BASE_IMG_SCALE_FACTOR\n    img -= [0.485, 0.456, 0.406]\n    img /= [0.229, 0.224, 0.225]",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "documentation": {}
    },
    {
        "label": "get_pytorch_preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "peekOfCode": "def get_pytorch_preprocess(img):\n    img = img.astype(np.float32)\n    img *= BASE_IMG_SCALE_FACTOR\n    img -= [0.485, 0.456, 0.406]\n    img /= [0.229, 0.224, 0.225]\n    return img",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.img_utils",
        "documentation": {}
    },
    {
        "label": "get_full_model_path",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def get_full_model_path(lib_name, model_full_name):\n    model_path = MODEL_PATH_ROOT.format(lib_name)\n    return {\n        \"path\": model_path,\n        \"full_path\": os.path.join(model_path, model_full_name)\n    }\ndef plot_acc(data_list, experiment_name):\n    plt.figure(figsize=[8, 6])\n    plt.plot(data_list[:, 0], \"r\", linewidth=2.5, label=\"Original Model\")\n    plt.plot(data_list[:, 1], \"b\", linewidth=2.5, label=\"Converted DNN Model\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "plot_acc",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def plot_acc(data_list, experiment_name):\n    plt.figure(figsize=[8, 6])\n    plt.plot(data_list[:, 0], \"r\", linewidth=2.5, label=\"Original Model\")\n    plt.plot(data_list[:, 1], \"b\", linewidth=2.5, label=\"Converted DNN Model\")\n    plt.xlabel(\"Iterations \", fontsize=15)\n    plt.ylabel(\"Time (ms)\", fontsize=15)\n    plt.title(experiment_name, fontsize=15)\n    plt.legend()\n    full_path_to_fig = os.path.join(CommonConfig().output_data_root_dir, experiment_name + \".png\")\n    plt.savefig(full_path_to_fig, bbox_inches=\"tight\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "get_final_summary_info",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def get_final_summary_info(general_quality_metric, general_inference_time, metric_name):\n    general_quality_metric = np.array(general_quality_metric)\n    general_inference_time = np.array(general_inference_time)\n    summary_line = \"===== End of processing. General results:\\n\"\n    \"\\t* mean {} for the original model: {}\\t\"\n    \"\\t* mean time (min) for the original model inferences: {}\\n\"\n    \"\\t* mean {} for the DNN model: {}\\t\"\n    \"\\t* mean time (min) for the DNN model inferences: {}\\n\".format(\n        metric_name, np.mean(general_quality_metric[:, 0]),\n        np.mean(general_inference_time[:, 0]) / 60000,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "set_common_reproducibility",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def set_common_reproducibility():\n    random.seed(SEED_VAL)\n    np.random.seed(SEED_VAL)\ndef set_pytorch_env():\n    set_common_reproducibility()\n    torch.manual_seed(SEED_VAL)\n    torch.set_printoptions(precision=10)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(SEED_VAL)\n        torch.backends.cudnn_benchmark_enabled = False",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "set_pytorch_env",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def set_pytorch_env():\n    set_common_reproducibility()\n    torch.manual_seed(SEED_VAL)\n    torch.set_printoptions(precision=10)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(SEED_VAL)\n        torch.backends.cudnn_benchmark_enabled = False\n        torch.backends.cudnn.deterministic = True\ndef set_tf_env(is_use_gpu=True):\n    set_common_reproducibility()",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "set_tf_env",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def set_tf_env(is_use_gpu=True):\n    set_common_reproducibility()\n    tf.random.set_seed(SEED_VAL)\n    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n    if tf.config.list_physical_devices(\"GPU\") and is_use_gpu:\n        gpu_devices = tf.config.list_physical_devices(\"GPU\")\n        tf.config.experimental.set_visible_devices(gpu_devices[0], \"GPU\")\n        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n        os.environ[\"TF_USE_CUDNN\"] = \"1\"\n    else:",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "str_bool",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def str_bool(input_val):\n    if input_val.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif input_val.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value was expected')\ndef get_formatted_model_list(model_list):\n    note_line = 'Please, choose the model from the below list:\\n'\n    spaces_to_set = ' ' * (len(note_line) - 2)",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "get_formatted_model_list",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def get_formatted_model_list(model_list):\n    note_line = 'Please, choose the model from the below list:\\n'\n    spaces_to_set = ' ' * (len(note_line) - 2)\n    return note_line + ''.join([spaces_to_set, '{} \\n'] * len(model_list)).format(*model_list)\ndef model_str(model_list):\n    def type_model_list(input_val):\n        if input_val.lower() in model_list:\n            return input_val.lower()\n        else:\n            raise argparse.ArgumentTypeError(",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "model_str",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def model_str(model_list):\n    def type_model_list(input_val):\n        if input_val.lower() in model_list:\n            return input_val.lower()\n        else:\n            raise argparse.ArgumentTypeError(\n                'The model is currently unavailable for test.\\n' +\n                get_formatted_model_list(model_list)\n            )\n    return type_model_list",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "get_test_module",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def get_test_module(test_module_name, test_module_path):\n    module_spec = importlib.util.spec_from_file_location(test_module_name, test_module_path)\n    test_module = importlib.util.module_from_spec(module_spec)\n    module_spec.loader.exec_module(test_module)\n    module_spec.loader.exec_module(test_module)\n    return test_module\ndef create_parser():\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\n        \"--test\",",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "create_parser",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def create_parser():\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\n        \"--test\",\n        type=str_bool,\n        help=\"Define whether you'd like to run the model with OpenCV for testing.\",\n        default=False\n    ),\n    parser.add_argument(\n        \"--default_img_preprocess\",",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "create_extended_parser",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "def create_extended_parser(model_list):\n    parser = create_parser()\n    parser.add_argument(\n        \"--model_name\",\n        type=model_str(model_list=model_list),\n        help=\"\\nDefine the model name to test.\\n\" +\n             get_formatted_model_list(model_list),\n        required=True\n    )\n    return parser",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "SEED_VAL",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "SEED_VAL = 42\nDNN_LIB = \"DNN\"\n# common path for model savings\nMODEL_PATH_ROOT = os.path.join(CommonConfig().output_data_root_dir, \"{}/models\")\ndef get_full_model_path(lib_name, model_full_name):\n    model_path = MODEL_PATH_ROOT.format(lib_name)\n    return {\n        \"path\": model_path,\n        \"full_path\": os.path.join(model_path, model_full_name)\n    }",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "DNN_LIB",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "DNN_LIB = \"DNN\"\n# common path for model savings\nMODEL_PATH_ROOT = os.path.join(CommonConfig().output_data_root_dir, \"{}/models\")\ndef get_full_model_path(lib_name, model_full_name):\n    model_path = MODEL_PATH_ROOT.format(lib_name)\n    return {\n        \"path\": model_path,\n        \"full_path\": os.path.join(model_path, model_full_name)\n    }\ndef plot_acc(data_list, experiment_name):",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "MODEL_PATH_ROOT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "peekOfCode": "MODEL_PATH_ROOT = os.path.join(CommonConfig().output_data_root_dir, \"{}/models\")\ndef get_full_model_path(lib_name, model_full_name):\n    model_path = MODEL_PATH_ROOT.format(lib_name)\n    return {\n        \"path\": model_path,\n        \"full_path\": os.path.join(model_path, model_full_name)\n    }\ndef plot_acc(data_list, experiment_name):\n    plt.figure(figsize=[8, 6])\n    plt.plot(data_list[:, 0], \"r\", linewidth=2.5, label=\"Original Model\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.common.utils",
        "documentation": {}
    },
    {
        "label": "get_color_map_list",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "peekOfCode": "def get_color_map_list(num_classes):\n    \"\"\"\n    Returns the color map for visualizing the segmentation mask,\n    which can support arbitrary number of classes.\n    Args:\n        num_classes (int): Number of classes.\n    Returns:\n        (list). The color map.\n    \"\"\"\n    num_classes += 1",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "peekOfCode": "def visualize(image, result, save_dir=None, weight=0.6):\n    \"\"\"\n    Convert predict result to color image, and save added image.\n    Args:\n        image (str): The path of origin image.\n        result (np.ndarray): The predict result of image.\n        save_dir (str): The directory for saving visual image. Default: None.\n        weight (float): The image weight of visual image, and the result weight is (1 - weight). Default: 0.6\n    Returns:\n        vis_result (np.ndarray): If `save_dir` is None, return the visualized result.",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "peekOfCode": "def preprocess(image_path):\n    ''' preprocess input image file to np.ndarray\n    Args:\n        image_path(str): Path of input image file\n    Returns:\n        ProcessedImage(numpy.ndarray): A numpy.ndarray\n                variable which shape is (1, 3, 192, 192)\n    '''\n    transforms = T.Compose([\n        T.Resize((192, 192)),",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_humanseg",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "peekOfCode": "def preprocess(image_path):\n    ''' preprocess input image file to np.ndarray\n    Args:\n        image_path(str): Path of input image file\n    Returns:\n        ProcessedImage(numpy.ndarray): A numpy.ndarray\n                variable which shape is (1, 3, 224, 224)\n    '''\n    transforms = T.Compose([\n        T.Resize((256, 256)),",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "documentation": {}
    },
    {
        "label": "export_onnx_resnet50",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "peekOfCode": "def export_onnx_resnet50(save_path):\n    ''' export PaddlePaddle model to ONNX format\n    Args:\n        save_path(str): Path to save exported ONNX model\n    Returns:\n        None\n    '''\n    model = hub.Module(name=\"resnet50_vd_imagenet_ssld\")\n    input_spec = paddle.static.InputSpec(\n        [1, 3, 224, 224], \"float32\", \"image\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.paddlepaddle.paddle_resnet50",
        "documentation": {}
    },
    {
        "label": "PyTorchClsModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "peekOfCode": "class PyTorchClsModel(PyTorchModelPreparer):\n    def __init__(self, height, width, model_name, original_model):\n        super(PyTorchClsModel, self).__init__(height, width, model_name, original_model)\ndef main():\n    set_pytorch_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name\n    cls_model = PyTorchClsModel(\n        height=TestClsConfig().frame_size,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "peekOfCode": "def main():\n    set_pytorch_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name\n    cls_model = PyTorchClsModel(\n        height=TestClsConfig().frame_size,\n        width=TestClsConfig().frame_size,\n        model_name=model_name,\n        original_model=model_dict[model_name](pretrained=True)",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "model_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "peekOfCode": "model_dict = {\n    \"alexnet\": models.alexnet,\n    \"vgg11\": models.vgg11,\n    \"vgg13\": models.vgg13,\n    \"vgg16\": models.vgg16,\n    \"vgg19\": models.vgg19,\n    \"resnet18\": models.resnet18,\n    \"resnet34\": models.resnet34,\n    \"resnet50\": models.resnet50,\n    \"resnet101\": models.resnet101,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "get_pytorch_onnx_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def get_pytorch_onnx_model(original_model):\n    # define the directory for further converted model save\n    onnx_model_path = \"models\"\n    # define the name of further converted model\n    onnx_model_name = \"resnet50.onnx\"\n    # create directory for further converted model\n    os.makedirs(onnx_model_path, exist_ok=True)\n    # get full path to the converted model\n    full_model_path = os.path.join(onnx_model_path, onnx_model_name)\n    # generate model input",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "get_preprocessed_img",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def get_preprocessed_img(img_path):\n    # read the image\n    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    input_img = input_img.astype(np.float32)\n    input_img = cv2.resize(input_img, (256, 256))\n    # define preprocess parameters\n    mean = np.array([0.485, 0.456, 0.406]) * 255.0\n    scale = 1 / 255.0\n    std = [0.229, 0.224, 0.225]\n    # prepare input blob to fit the model input:",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "get_imagenet_labels",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def get_imagenet_labels(labels_path):\n    with open(labels_path) as f:\n        imagenet_labels = [line.strip() for line in f.readlines()]\n    return imagenet_labels\ndef get_opencv_dnn_prediction(opencv_net, preproc_img, imagenet_labels):\n    # set OpenCV DNN input\n    opencv_net.setInput(preproc_img)\n    # OpenCV DNN inference\n    out = opencv_net.forward()\n    print(\"OpenCV DNN prediction: \\n\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "get_opencv_dnn_prediction",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def get_opencv_dnn_prediction(opencv_net, preproc_img, imagenet_labels):\n    # set OpenCV DNN input\n    opencv_net.setInput(preproc_img)\n    # OpenCV DNN inference\n    out = opencv_net.forward()\n    print(\"OpenCV DNN prediction: \\n\")\n    print(\"* shape: \", out.shape)\n    # get the predicted class ID\n    imagenet_class_id = np.argmax(out)\n    # get confidence",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "get_pytorch_dnn_prediction",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def get_pytorch_dnn_prediction(original_net, preproc_img, imagenet_labels):\n    original_net.eval()\n    preproc_img = torch.FloatTensor(preproc_img)\n    # inference\n    with torch.no_grad():\n        out = original_net(preproc_img)\n    print(\"\\nPyTorch model prediction: \\n\")\n    print(\"* shape: \", out.shape)\n    # get the predicted class ID\n    imagenet_class_id = torch.argmax(out, axis=1).item()",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "peekOfCode": "def main():\n    # initialize PyTorch ResNet-50 model\n    original_model = models.resnet50(pretrained=True)\n    # get the path to the converted into ONNX PyTorch model\n    full_model_path = get_pytorch_onnx_model(original_model)\n    # read converted .onnx model with OpenCV API\n    opencv_net = cv2.dnn.readNetFromONNX(full_model_path)\n    print(\"OpenCV model was successfully read. Layer IDs: \\n\", opencv_net.getLayerNames())\n    # get preprocessed image\n    input_img = get_preprocessed_img(\"../data/squirrel_cls.jpg\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50",
        "documentation": {}
    },
    {
        "label": "get_pytorch_onnx_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "peekOfCode": "def get_pytorch_onnx_model(original_model):\n    # define the directory for further converted model save\n    onnx_model_path = \"models\"\n    # define the name of further converted model\n    onnx_model_name = \"resnet50.onnx\"\n    # create directory for further converted model\n    os.makedirs(onnx_model_path, exist_ok=True)\n    # get full path to the converted model\n    full_model_path = os.path.join(onnx_model_path, onnx_model_name)\n    # generate model input",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "peekOfCode": "def main():\n    # initialize PyTorch ResNet-50 model\n    original_model = models.resnet50(pretrained=True)\n    # get the path to the converted into ONNX PyTorch model\n    full_model_path = get_pytorch_onnx_model(original_model)\n    print(\"PyTorch ResNet-50 model was successfully converted: \", full_model_path)\nif __name__ == \"__main__\":\n    main()",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx",
        "documentation": {}
    },
    {
        "label": "PyTorchModelPreparer",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "peekOfCode": "class PyTorchModelPreparer(AbstractModel):\n    def __init__(\n            self,\n            height,\n            width,\n            model_name=\"default\",\n            original_model=object,\n            batch_size=1,\n            default_input_name=\"input\",\n            default_output_name=\"output\"",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "documentation": {}
    },
    {
        "label": "PyTorchModelProcessor",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "peekOfCode": "class PyTorchModelProcessor(Framework):\n    def __init__(self, prepared_model, model_name):\n        self._prepared_model = prepared_model\n        self._name = model_name\n    def get_output(self, input_blob):\n        tensor = torch.FloatTensor(input_blob)\n        self._prepared_model.eval()\n        with torch.no_grad():\n            model_out = self._prepared_model(tensor)\n        # segmentation case",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "documentation": {}
    },
    {
        "label": "PyTorchDnnModelProcessor",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "peekOfCode": "class PyTorchDnnModelProcessor(Framework):\n    def __init__(self, prepared_dnn_model, model_name):\n        self._prepared_dnn_model = prepared_dnn_model\n        self._name = model_name\n    def get_output(self, input_blob):\n        self._prepared_dnn_model.setInput(input_blob, '')\n        return self._prepared_dnn_model.forward()\n    def get_name(self):\n        return self._name",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "documentation": {}
    },
    {
        "label": "CURRENT_LIB",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "peekOfCode": "CURRENT_LIB = \"PyTorch\"\nMODEL_FORMAT = \".onnx\"\nclass PyTorchModelPreparer(AbstractModel):\n    def __init__(\n            self,\n            height,\n            width,\n            model_name=\"default\",\n            original_model=object,\n            batch_size=1,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "documentation": {}
    },
    {
        "label": "MODEL_FORMAT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "peekOfCode": "MODEL_FORMAT = \".onnx\"\nclass PyTorchModelPreparer(AbstractModel):\n    def __init__(\n            self,\n            height,\n            width,\n            model_name=\"default\",\n            original_model=object,\n            batch_size=1,\n            default_input_name=\"input\",",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.pytorch.pytorch_model",
        "documentation": {}
    },
    {
        "label": "TFClsModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "class TFClsModel(TFModelPreparer):\n    def __init__(self, model_name, original_model):\n        super(TFClsModel, self).__init__(model_name, original_model)\ndef main():\n    set_tf_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name\n    model_name_val = model_dict[model_name]\n    cls_model = TFClsModel(",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "def main():\n    set_tf_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name\n    model_name_val = model_dict[model_name]\n    cls_model = TFClsModel(\n        model_name=model_name,\n        original_model=model_name_val[CNN_CLASS_ID](\n            include_top=True,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "model_dict",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "model_dict = {\n    \"vgg16\": [VGG16, vgg16, tf_model_blob_caffe_mode],\n    \"vgg19\": [VGG19, vgg19, tf_model_blob_caffe_mode],\n    \"resnet50\": [ResNet50, resnet, tf_model_blob_caffe_mode],\n    \"resnet101\": [ResNet101, resnet, tf_model_blob_caffe_mode],\n    \"resnet152\": [ResNet152, resnet, tf_model_blob_caffe_mode],\n    \"densenet121\": [DenseNet121, densenet, pytorch_input_blob],\n    \"densenet169\": [DenseNet169, densenet, pytorch_input_blob],\n    \"densenet201\": [DenseNet201, densenet, pytorch_input_blob],\n    \"inceptionresnetv2\": [InceptionResNetV2, inception_resnet_v2, tf_input_blob],",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "CNN_CLASS_ID",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "CNN_CLASS_ID = 0\nCNN_UTILS_ID = 1\nDEFAULT_BLOB_PARAMS_ID = 2\nclass TFClsModel(TFModelPreparer):\n    def __init__(self, model_name, original_model):\n        super(TFClsModel, self).__init__(model_name, original_model)\ndef main():\n    set_tf_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "CNN_UTILS_ID",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "CNN_UTILS_ID = 1\nDEFAULT_BLOB_PARAMS_ID = 2\nclass TFClsModel(TFModelPreparer):\n    def __init__(self, model_name, original_model):\n        super(TFClsModel, self).__init__(model_name, original_model)\ndef main():\n    set_tf_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BLOB_PARAMS_ID",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "peekOfCode": "DEFAULT_BLOB_PARAMS_ID = 2\nclass TFClsModel(TFModelPreparer):\n    def __init__(self, model_name, original_model):\n        super(TFClsModel, self).__init__(model_name, original_model)\ndef main():\n    set_tf_env()\n    parser = create_extended_parser(list(model_dict.keys()))\n    cmd_args = parser.parse_args()\n    model_name = cmd_args.model_name\n    model_name_val = model_dict[model_name]",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls",
        "documentation": {}
    },
    {
        "label": "get_tf_model_proto",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def get_tf_model_proto(tf_model):\n    # define the directory for .pb model\n    pb_model_path = \"models\"\n    # define the name of .pb model\n    pb_model_name = \"mobilenet.pb\"\n    # create directory for further converted model\n    os.makedirs(pb_model_path, exist_ok=True)\n    # get model TF graph\n    tf_model_graph = tf.function(lambda x: tf_model(x))\n    # get concrete function",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "get_preprocessed_img",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def get_preprocessed_img(img_path):\n    # read the image\n    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    input_img = input_img.astype(np.float32)\n    # define preprocess parameters\n    mean = np.array([1.0, 1.0, 1.0]) * 127.5\n    scale = 1 / 127.5\n    # prepare input blob to fit the model input:\n    # 1. subtract mean\n    # 2. scale to set pixel values from 0 to 1",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "get_imagenet_labels",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def get_imagenet_labels(labels_path):\n    with open(labels_path) as f:\n        imagenet_labels = [line.strip() for line in f.readlines()]\n    return imagenet_labels\ndef get_opencv_dnn_prediction(opencv_net, preproc_img, imagenet_labels):\n    # set OpenCV DNN input\n    opencv_net.setInput(preproc_img)\n    # OpenCV DNN inference\n    out = opencv_net.forward()\n    print(\"OpenCV DNN prediction: \\n\")",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "get_opencv_dnn_prediction",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def get_opencv_dnn_prediction(opencv_net, preproc_img, imagenet_labels):\n    # set OpenCV DNN input\n    opencv_net.setInput(preproc_img)\n    # OpenCV DNN inference\n    out = opencv_net.forward()\n    print(\"OpenCV DNN prediction: \\n\")\n    print(\"* shape: \", out.shape)\n    # get the predicted class ID\n    imagenet_class_id = np.argmax(out)\n    # get confidence",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "get_tf_dnn_prediction",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def get_tf_dnn_prediction(original_net, preproc_img, imagenet_labels):\n    # inference\n    preproc_img = preproc_img.transpose(0, 2, 3, 1)\n    print(\"TF input blob shape: {}\\n\".format(preproc_img.shape))\n    out = original_net(preproc_img)\n    print(\"\\nTensorFlow model prediction: \\n\")\n    print(\"* shape: \", out.shape)\n    # get the predicted class ID\n    imagenet_class_id = np.argmax(out)\n    print(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "peekOfCode": "def main():\n    # configure TF launching\n    set_tf_env()\n    # initialize TF MobileNet model\n    original_tf_model = MobileNet(\n        include_top=True,\n        weights=\"imagenet\"\n    )\n    # get TF frozen graph path\n    full_pb_path = get_tf_model_proto(original_tf_model)",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet",
        "documentation": {}
    },
    {
        "label": "extract_tf_frozen_graph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "peekOfCode": "def extract_tf_frozen_graph(model_name, extracted_model_path):\n    # define model archive name\n    tf_model_tar = model_name + '.tar.gz'\n    # define link to retrieve model archive\n    model_link = DETECTION_MODELS_URL + tf_model_tar\n    tf_frozen_graph_name = 'frozen_inference_graph'\n    try:\n        urllib.request.urlretrieve(model_link, tf_model_tar)\n    except Exception:\n        print(\"TF {} was not retrieved: {}\".format(model_name, model_link))",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "peekOfCode": "def main():\n    tf_model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n    graph_extraction_dir = \"./\"\n    frozen_graph_path = extract_tf_frozen_graph(tf_model_name, graph_extraction_dir)\n    print(\"Frozen graph path for {}: {}\".format(tf_model_name, frozen_graph_path))\nif __name__ == \"__main__\":\n    main()",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "documentation": {}
    },
    {
        "label": "DETECTION_MODELS_URL",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "peekOfCode": "DETECTION_MODELS_URL = 'http://download.tensorflow.org/models/object_detection/'\ndef extract_tf_frozen_graph(model_name, extracted_model_path):\n    # define model archive name\n    tf_model_tar = model_name + '.tar.gz'\n    # define link to retrieve model archive\n    model_link = DETECTION_MODELS_URL + tf_model_tar\n    tf_frozen_graph_name = 'frozen_inference_graph'\n    try:\n        urllib.request.urlretrieve(model_link, tf_model_tar)\n    except Exception:",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet",
        "documentation": {}
    },
    {
        "label": "TFModelPreparer",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "peekOfCode": "class TFModelPreparer(AbstractModel):\n    \"\"\" Class for the preparation of the TF models: original and converted OpenCV Net.\n    Args:\n        model_name: TF model name\n        original_model: TF configured model object or session\n        is_ready_graph: indicates whether ready .pb file already exists\n        tf_model_graph_path: path to the existing frozen TF graph\n    \"\"\"\n    def __init__(\n            self,",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "documentation": {}
    },
    {
        "label": "TFModelProcessor",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "peekOfCode": "class TFModelProcessor(Framework):\n    def __init__(self, prepared_model, model_name):\n        self._prepared_model = prepared_model\n        self._name = model_name\n    def get_output(self, input_blob):\n        assert len(input_blob.shape) == 4\n        batch_tf = input_blob.transpose(0, 2, 3, 1)\n        out = self._prepared_model(batch_tf)\n        return out\n    def get_name(self):",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "documentation": {}
    },
    {
        "label": "TFDnnModelProcessor",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "peekOfCode": "class TFDnnModelProcessor(Framework):\n    def __init__(self, prepared_dnn_model, model_name):\n        self._prepared_dnn_model = prepared_dnn_model\n        self._name = model_name\n    def get_output(self, input_blob):\n        self._prepared_dnn_model.setInput(input_blob)\n        ret_val = self._prepared_dnn_model.forward()\n        return ret_val\n    def get_name(self):\n        return DNN_LIB",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "documentation": {}
    },
    {
        "label": "CURRENT_LIB",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "peekOfCode": "CURRENT_LIB = \"TF\"\nMODEL_FORMAT = \".pb\"\nclass TFModelPreparer(AbstractModel):\n    \"\"\" Class for the preparation of the TF models: original and converted OpenCV Net.\n    Args:\n        model_name: TF model name\n        original_model: TF configured model object or session\n        is_ready_graph: indicates whether ready .pb file already exists\n        tf_model_graph_path: path to the existing frozen TF graph\n    \"\"\"",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "documentation": {}
    },
    {
        "label": "MODEL_FORMAT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "description": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "peekOfCode": "MODEL_FORMAT = \".pb\"\nclass TFModelPreparer(AbstractModel):\n    \"\"\" Class for the preparation of the TF models: original and converted OpenCV Net.\n    Args:\n        model_name: TF model name\n        original_model: TF configured model object or session\n        is_ready_graph: indicates whether ready .pb file already exists\n        tf_model_graph_path: path to the existing frozen TF graph\n    \"\"\"\n    def __init__(",
        "detail": "Hw_2.opencv.samples.dnn.dnn_model_runner.dnn_conversion.tf.tf_model",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.action_recognition",
        "description": "Hw_2.opencv.samples.dnn.action_recognition",
        "peekOfCode": "def get_class_names(path):\n    class_names = []\n    with open(path) as f:\n        for row in f:\n            class_names.append(row[:-1])\n    return class_names\ndef classify_video(video_path, net_path):\n    SAMPLE_DURATION = 16\n    SAMPLE_SIZE = 112\n    mean = (114.7748, 107.7354, 99.4750)",
        "detail": "Hw_2.opencv.samples.dnn.action_recognition",
        "documentation": {}
    },
    {
        "label": "classify_video",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.action_recognition",
        "description": "Hw_2.opencv.samples.dnn.action_recognition",
        "peekOfCode": "def classify_video(video_path, net_path):\n    SAMPLE_DURATION = 16\n    SAMPLE_SIZE = 112\n    mean = (114.7748, 107.7354, 99.4750)\n    class_names = get_class_names(args.classes)\n    net = cv.dnn.readNet(net_path)\n    net.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)\n    net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n    winName = 'Deep learning image classification in OpenCV'\n    cv.namedWindow(winName, cv.WINDOW_AUTOSIZE)",
        "detail": "Hw_2.opencv.samples.dnn.action_recognition",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.action_recognition",
        "description": "Hw_2.opencv.samples.dnn.action_recognition",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Use this script to run action recognition using 3D ResNet34',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--input', '-i', help='Path to input video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--model', required=True, help='Path to model.')\nparser.add_argument('--classes', default=findFile('action_recongnition_kinetics.txt'), help='Path to classes list.')\n# To get net download original repository https://github.com/kenshohara/video-classification-3d-cnn-pytorch\n# For correct ONNX export modify file: video-classification-3d-cnn-pytorch/models/resnet.py\n# change\n# - def downsample_basic_block(x, planes, stride):\n# -     out = F.avg_pool3d(x, kernel_size=1, stride=stride)",
        "detail": "Hw_2.opencv.samples.dnn.action_recognition",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.classification",
        "description": "Hw_2.opencv.samples.dnn.classification",
        "peekOfCode": "def get_args_parser(func_args):\n    backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_HALIDE, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE,\n                cv.dnn.DNN_BACKEND_OPENCV, cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\n    targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD,\n               cv.dnn.DNN_TARGET_HDDL, cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                        help='An optional path to file with preprocessing parameters.')\n    parser.add_argument('--input',\n                        help='Path to input image or video file. Skip this argument to capture frames from a camera.')",
        "detail": "Hw_2.opencv.samples.dnn.classification",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.classification",
        "description": "Hw_2.opencv.samples.dnn.classification",
        "peekOfCode": "def main(func_args=None):\n    args = get_args_parser(func_args)\n    args.model = findFile(args.model)\n    args.config = findFile(args.config)\n    args.classes = findFile(args.classes)\n    # Load names of classes\n    classes = None\n    if args.classes:\n        with open(args.classes, 'rt') as f:\n            classes = f.read().rstrip('\\n').split('\\n')",
        "detail": "Hw_2.opencv.samples.dnn.classification",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.colorization",
        "description": "Hw_2.opencv.samples.dnn.colorization",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description='iColor: deep interactive colorization')\n    parser.add_argument('--input', help='Path to image or video. Skip to capture frames from camera')\n    parser.add_argument('--prototxt', help='Path to colorization_deploy_v2.prototxt', required=True)\n    parser.add_argument('--caffemodel', help='Path to colorization_release_v2.caffemodel', required=True)\n    parser.add_argument('--kernel', help='Path to pts_in_hull.npy', required=True)\n    args = parser.parse_args()\n    return args\nif __name__ == '__main__':\n    W_in = 224",
        "detail": "Hw_2.opencv.samples.dnn.colorization",
        "documentation": {}
    },
    {
        "label": "add_argument",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.common",
        "description": "Hw_2.opencv.samples.dnn.common",
        "peekOfCode": "def add_argument(zoo, parser, name, help, required=False, default=None, type=None, action=None, nargs=None):\n    if len(sys.argv) <= 1:\n        return\n    modelName = sys.argv[1]\n    if os.path.isfile(zoo):\n        fs = cv.FileStorage(zoo, cv.FILE_STORAGE_READ)\n        node = fs.getNode(modelName)\n        if not node.empty():\n            value = node.getNode(name)\n            if not value.empty():",
        "detail": "Hw_2.opencv.samples.dnn.common",
        "documentation": {}
    },
    {
        "label": "add_preproc_args",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.common",
        "description": "Hw_2.opencv.samples.dnn.common",
        "peekOfCode": "def add_preproc_args(zoo, parser, sample):\n    aliases = []\n    if os.path.isfile(zoo):\n        fs = cv.FileStorage(zoo, cv.FILE_STORAGE_READ)\n        root = fs.root()\n        for name in root.keys():\n            model = root.getNode(name)\n            if model.getNode('sample').string() == sample:\n                aliases.append(name)\n    parser.add_argument('alias', nargs='?', choices=aliases,",
        "detail": "Hw_2.opencv.samples.dnn.common",
        "documentation": {}
    },
    {
        "label": "findFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.common",
        "description": "Hw_2.opencv.samples.dnn.common",
        "peekOfCode": "def findFile(filename):\n    if filename:\n        if os.path.exists(filename):\n            return filename\n        fpath = cv.samples.findFile(filename, False)\n        if fpath:\n            return fpath\n        samplesDataDir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                      '..',\n                                      'data',",
        "detail": "Hw_2.opencv.samples.dnn.common",
        "documentation": {}
    },
    {
        "label": "HashMismatchException",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "class HashMismatchException(Exception):\n    def __init__(self, expected, actual):\n        Exception.__init__(self)\n        self.expected = expected\n        self.actual = actual\n    def __str__(self):\n        return 'Hash mismatch: expected {} vs actual of {}'.format(self.expected, self.actual)\ndef getHashsumFromFile(filepath):\n    sha = hashlib.sha1()\n    if os.path.exists(filepath):",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "DownloadInstance",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "class DownloadInstance:\n    def __init__(self, **kwargs):\n        self.name = kwargs.pop('name')\n        self.filename = kwargs.pop('filename')\n        self.loader = kwargs.pop('loader', None)\n        self.save_dir = kwargs.pop('save_dir')\n        self.sha = kwargs.pop('sha', None)\n    def __str__(self):\n        return 'DownloadInstance <{}>'.format(self.name)\n    def get(self):",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "Loader",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "class Loader(object):\n    MB = 1024*1024\n    BUFSIZE = 10*MB\n    def __init__(self, download_name, download_sha, archive_member = None):\n        self.download_name = download_name\n        self.download_sha = download_sha\n        self.archive_member = archive_member\n    def load(self, requested_file, sha, save_dir):\n        if self.download_sha is None:\n            download_dir = save_dir",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "URLLoader",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "class URLLoader(Loader):\n    def __init__(self, download_name, download_sha, url, archive_member = None):\n        super(URLLoader, self).__init__(download_name, download_sha, archive_member)\n        self.download_name = download_name\n        self.download_sha = download_sha\n        self.url = url\n    def download(self, filepath):\n        r = urlopen(self.url, timeout=60)\n        self.printRequest(r)\n        self.save(filepath, r)",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "GDriveLoader",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "class GDriveLoader(Loader):\n    BUFSIZE = 1024 * 1024\n    PROGRESS_SIZE = 10 * 1024 * 1024\n    def __init__(self, download_name, download_sha, gid, archive_member = None):\n        super(GDriveLoader, self).__init__(download_name, download_sha, archive_member)\n        self.download_name = download_name\n        self.download_sha = download_sha\n        self.gid = gid\n    def download(self, filepath):\n        session = requests.Session()  # re-use cookies",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "getHashsumFromFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def getHashsumFromFile(filepath):\n    sha = hashlib.sha1()\n    if os.path.exists(filepath):\n        print('  there is already a file with the same name')\n        with open(filepath, 'rb') as f:\n            while True:\n                buf = f.read(10*1024*1024)\n                if not buf:\n                    break\n                sha.update(buf)",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "checkHashsum",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def checkHashsum(expected_sha, filepath, silent=True):\n    print('  expected SHA1: {}'.format(expected_sha))\n    actual_sha = getHashsumFromFile(filepath)\n    print('  actual SHA1:{}'.format(actual_sha))\n    hashes_matched = expected_sha == actual_sha\n    if not hashes_matched and not silent:\n        raise HashMismatchException(expected_sha, actual_sha)\n    return hashes_matched\ndef isArchive(filepath):\n    return tarfile.is_tarfile(filepath)",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "isArchive",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def isArchive(filepath):\n    return tarfile.is_tarfile(filepath)\nclass DownloadInstance:\n    def __init__(self, **kwargs):\n        self.name = kwargs.pop('name')\n        self.filename = kwargs.pop('filename')\n        self.loader = kwargs.pop('loader', None)\n        self.save_dir = kwargs.pop('save_dir')\n        self.sha = kwargs.pop('sha', None)\n    def __str__(self):",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "produceDownloadInstance",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def produceDownloadInstance(instance_name, filename, sha, url, save_dir, download_name=None, download_sha=None, archive_member=None):\n    spec_param = url\n    loader = URLLoader\n    if download_name is None:\n        download_name = filename\n    if download_sha is None:\n        download_sha = sha\n    if \"drive.google.com\" in url:\n        token = \"\"\n        token_part = url.rsplit('/', 1)[-1]",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "getSaveDir",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def getSaveDir():\n    env_path = os.environ.get(\"OPENCV_DOWNLOAD_DATA_PATH\", None)\n    if env_path:\n        save_dir = env_path\n    else:\n        # TODO reuse binding function cv2.utils.fs.getCacheDirectory when issue #19011 is fixed\n        if platform.system() == \"Darwin\":\n            #On Apple devices\n            temp_env = os.environ.get(\"TMPDIR\", None)\n            if temp_env is None or not os.path.isdir(temp_env):",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "downloadFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def downloadFile(url, sha=None, save_dir=None, filename=None):\n    if save_dir is None:\n        save_dir = getSaveDir()\n    if filename is None:\n        filename = \"download_\" + datetime.now().__str__()\n    name = filename\n    return produceDownloadInstance(name, filename, sha, url, save_dir).get()\ndef parseMetalinkFile(metalink_filepath, save_dir):\n    NS = {'ml': 'urn:ietf:params:xml:ns:metalink'}\n    models = []",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "parseMetalinkFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def parseMetalinkFile(metalink_filepath, save_dir):\n    NS = {'ml': 'urn:ietf:params:xml:ns:metalink'}\n    models = []\n    for file_elem in ET.parse(metalink_filepath).getroot().findall('ml:file', NS):\n        url = file_elem.find('ml:url', NS).text\n        fname = file_elem.attrib['name']\n        name = file_elem.find('ml:identity', NS).text\n        hash_sum = file_elem.find('ml:hash', NS).text\n        models.append(produceDownloadInstance(name, fname, hash_sum, url, save_dir))\n    return models",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "parseYAMLFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "def parseYAMLFile(yaml_filepath, save_dir):\n    models = []\n    with open(yaml_filepath, 'r') as stream:\n        data_loaded = yaml.safe_load(stream)\n        for name, params in data_loaded.items():\n            load_info = params.get(\"load_info\", None)\n            if load_info:\n                fname = os.path.basename(params.get(\"model\"))\n                hash_sum = load_info.get(\"sha1\")\n                url = load_info.get(\"url\")",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.download_models",
        "description": "Hw_2.opencv.samples.dnn.download_models",
        "peekOfCode": "__all__ = [\"downloadFile\"]\nclass HashMismatchException(Exception):\n    def __init__(self, expected, actual):\n        Exception.__init__(self)\n        self.expected = expected\n        self.actual = actual\n    def __str__(self):\n        return 'Hash mismatch: expected {} vs actual of {}'.format(self.expected, self.actual)\ndef getHashsumFromFile(filepath):\n    sha = hashlib.sha1()",
        "detail": "Hw_2.opencv.samples.dnn.download_models",
        "documentation": {}
    },
    {
        "label": "CropLayer",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "class CropLayer(object):\n    def __init__(self, params, blobs):\n        self.xstart = 0\n        self.xend = 0\n        self.ystart = 0\n        self.yend = 0\n    # Our layer receives two inputs. We need to crop the first input blob\n    # to match a shape of the second one (keeping batch size and number of channels)\n    def getMemoryShapes(self, inputs):\n        inputShape, targetShape = inputs[0], inputs[1]",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "parser = argparse.ArgumentParser(\n        description='This sample shows how to define custom OpenCV deep learning layers in Python. '\n                    'Holistically-Nested Edge Detection (https://arxiv.org/abs/1504.06375) neural network '\n                    'is used as an example model. Find a pre-trained model at https://github.com/s9xie/hed.')\nparser.add_argument('--input', help='Path to image or video. Skip to capture frames from camera')\nparser.add_argument('--prototxt', help='Path to deploy.prototxt', required=True)\nparser.add_argument('--caffemodel', help='Path to hed_pretrained_bsds.caffemodel', required=True)\nparser.add_argument('--width', help='Resize input image to a specific width', default=500, type=int)\nparser.add_argument('--height', help='Resize input image to a specific height', default=500, type=int)\nargs = parser.parse_args()",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "args = parser.parse_args()\n#! [CropLayer]\nclass CropLayer(object):\n    def __init__(self, params, blobs):\n        self.xstart = 0\n        self.xend = 0\n        self.ystart = 0\n        self.yend = 0\n    # Our layer receives two inputs. We need to crop the first input blob\n    # to match a shape of the second one (keeping batch size and number of channels)",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "net = cv.dnn.readNet(cv.samples.findFile(args.prototxt), cv.samples.findFile(args.caffemodel))\nkWinName = 'Holistically-Nested Edge Detection'\ncv.namedWindow('Input', cv.WINDOW_NORMAL)\ncv.namedWindow(kWinName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "kWinName",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "kWinName = 'Holistically-Nested Edge Detection'\ncv.namedWindow('Input', cv.WINDOW_NORMAL)\ncv.namedWindow(kWinName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    cv.imshow('Input', frame)",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.edge_detection",
        "description": "Hw_2.opencv.samples.dnn.edge_detection",
        "peekOfCode": "cap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    cv.imshow('Input', frame)\n    inp = cv.dnn.blobFromImage(frame, scalefactor=1.0, size=(args.width, args.height),\n                               mean=(104.00698793, 116.66876762, 122.67891434),\n                               swapRB=False, crop=False)",
        "detail": "Hw_2.opencv.samples.dnn.edge_detection",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.face_detect",
        "description": "Hw_2.opencv.samples.dnn.face_detect",
        "peekOfCode": "def str2bool(v):\n    if v.lower() in ['on', 'yes', 'true', 'y', 't']:\n        return True\n    elif v.lower() in ['off', 'no', 'false', 'n', 'f']:\n        return False\n    else:\n        raise NotImplementedError\nparser = argparse.ArgumentParser()\nparser.add_argument('--image1', '-i1', type=str, help='Path to the input image1. Omit for detecting on default camera.')\nparser.add_argument('--image2', '-i2', type=str, help='Path to the input image2. When image1 and image2 parameters given then the program try to find a face on both images and runs face recognition algorithm.')",
        "detail": "Hw_2.opencv.samples.dnn.face_detect",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.face_detect",
        "description": "Hw_2.opencv.samples.dnn.face_detect",
        "peekOfCode": "def visualize(input, faces, fps, thickness=2):\n    if faces[1] is not None:\n        for idx, face in enumerate(faces[1]):\n            print('Face {}, top-left coordinates: ({:.0f}, {:.0f}), box width: {:.0f}, box height {:.0f}, score: {:.2f}'.format(idx, face[0], face[1], face[2], face[3], face[-1]))\n            coords = face[:-1].astype(np.int32)\n            cv.rectangle(input, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 255, 0), thickness)\n            cv.circle(input, (coords[4], coords[5]), 2, (255, 0, 0), thickness)\n            cv.circle(input, (coords[6], coords[7]), 2, (0, 0, 255), thickness)\n            cv.circle(input, (coords[8], coords[9]), 2, (0, 255, 0), thickness)\n            cv.circle(input, (coords[10], coords[11]), 2, (255, 0, 255), thickness)",
        "detail": "Hw_2.opencv.samples.dnn.face_detect",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.face_detect",
        "description": "Hw_2.opencv.samples.dnn.face_detect",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--image1', '-i1', type=str, help='Path to the input image1. Omit for detecting on default camera.')\nparser.add_argument('--image2', '-i2', type=str, help='Path to the input image2. When image1 and image2 parameters given then the program try to find a face on both images and runs face recognition algorithm.')\nparser.add_argument('--video', '-v', type=str, help='Path to the input video.')\nparser.add_argument('--scale', '-sc', type=float, default=1.0, help='Scale factor used to resize input video frames.')\nparser.add_argument('--face_detection_model', '-fd', type=str, default='face_detection_yunet_2021dec.onnx', help='Path to the face detection model. Download the model at https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet')\nparser.add_argument('--face_recognition_model', '-fr', type=str, default='face_recognition_sface_2021dec.onnx', help='Path to the face recognition model. Download the model at https://github.com/opencv/opencv_zoo/tree/master/models/face_recognition_sface')\nparser.add_argument('--score_threshold', type=float, default=0.9, help='Filtering out faces of score < score_threshold.')\nparser.add_argument('--nms_threshold', type=float, default=0.3, help='Suppress bounding boxes of iou >= nms_threshold.')\nparser.add_argument('--top_k', type=int, default=5000, help='Keep top_k bounding boxes before NMS.')",
        "detail": "Hw_2.opencv.samples.dnn.face_detect",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.face_detect",
        "description": "Hw_2.opencv.samples.dnn.face_detect",
        "peekOfCode": "args = parser.parse_args()\ndef visualize(input, faces, fps, thickness=2):\n    if faces[1] is not None:\n        for idx, face in enumerate(faces[1]):\n            print('Face {}, top-left coordinates: ({:.0f}, {:.0f}), box width: {:.0f}, box height {:.0f}, score: {:.2f}'.format(idx, face[0], face[1], face[2], face[3], face[-1]))\n            coords = face[:-1].astype(np.int32)\n            cv.rectangle(input, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 255, 0), thickness)\n            cv.circle(input, (coords[4], coords[5]), 2, (255, 0, 0), thickness)\n            cv.circle(input, (coords[6], coords[7]), 2, (0, 0, 255), thickness)\n            cv.circle(input, (coords[8], coords[9]), 2, (0, 255, 0), thickness)",
        "detail": "Hw_2.opencv.samples.dnn.face_detect",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "description": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "peekOfCode": "parser = argparse.ArgumentParser(\n        description='This script is used to run style transfer models from '\n                    'https://github.com/jcjohnson/fast-neural-style using OpenCV')\nparser.add_argument('--input', help='Path to image or video. Skip to capture frames from camera')\nparser.add_argument('--model', help='Path to .t7 model')\nparser.add_argument('--width', default=-1, type=int, help='Resize input to specific width.')\nparser.add_argument('--height', default=-1, type=int, help='Resize input to specific height.')\nparser.add_argument('--median_filter', default=0, type=int, help='Kernel size of postprocessing blurring.')\nargs = parser.parse_args()\nnet = cv.dnn.readNetFromTorch(cv.samples.findFile(args.model))",
        "detail": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "description": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "peekOfCode": "args = parser.parse_args()\nnet = cv.dnn.readNetFromTorch(cv.samples.findFile(args.model))\nnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\nif args.input:\n    cap = cv.VideoCapture(args.input)\nelse:\n    cap = cv.VideoCapture(0)\ncv.namedWindow('Styled image', cv.WINDOW_NORMAL)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()",
        "detail": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "description": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "peekOfCode": "net = cv.dnn.readNetFromTorch(cv.samples.findFile(args.model))\nnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\nif args.input:\n    cap = cv.VideoCapture(args.input)\nelse:\n    cap = cv.VideoCapture(0)\ncv.namedWindow('Styled image', cv.WINDOW_NORMAL)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:",
        "detail": "Hw_2.opencv.samples.dnn.fast_neural_style",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "def preprocess(image):\n    \"\"\"\n    Create 4-dimensional blob from image and flip image\n    :param image: input image\n    \"\"\"\n    image_rev = np.flip(image, axis=1)\n    input = cv.dnn.blobFromImages([image, image_rev], mean=(104.00698793, 116.66876762, 122.67891434))\n    return input\ndef run_net(input, model_path, backend, target):\n    \"\"\"",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "run_net",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "def run_net(input, model_path, backend, target):\n    \"\"\"\n    Read network and infer model\n    :param model_path: path to JPPNet model\n    :param backend: computation backend\n    :param target: computation device\n    \"\"\"\n    net = cv.dnn.readNet(model_path)\n    net.setPreferableBackend(backend)\n    net.setPreferableTarget(target)",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "def postprocess(out, input_shape):\n    \"\"\"\n    Create a grayscale human segmentation\n    :param out: network output\n    :param input_shape: input image width and height\n    \"\"\"\n    # LIP classes\n    # 0 Background\n    # 1 Hat\n    # 2 Hair",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "decode_labels",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "def decode_labels(gray_image):\n    \"\"\"\n    Colorize image according to labels\n    :param gray_image: grayscale human segmentation result\n    \"\"\"\n    height, width, _ = gray_image.shape\n    colors = [(0, 0, 0), (128, 0, 0), (255, 0, 0), (0, 85, 0), (170, 0, 51), (255, 85, 0),\n              (0, 0, 85), (0, 119, 221), (85, 85, 0), (0, 85, 85), (85, 51, 0), (52, 86, 128),\n              (0, 128, 0), (0, 0, 255), (51, 170, 221), (0, 255, 255),(85, 255, 170),\n              (170, 255, 85), (255, 255, 0), (255, 170, 0)]",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "parse_human",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "def parse_human(image, model_path, backend=cv.dnn.DNN_BACKEND_OPENCV, target=cv.dnn.DNN_TARGET_CPU):\n    \"\"\"\n    Prepare input for execution, run net and postprocess output to parse human.\n    :param image: input image\n    :param model_path: path to JPPNet model\n    :param backend: name of computation backend\n    :param target: name of computation target\n    \"\"\"\n    input = preprocess(image)\n    input_h, input_w = input.shape[2:]",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "backends",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE, cv.dnn.DNN_BACKEND_OPENCV,\n            cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\ntargets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD,\n           cv.dnn.DNN_TARGET_HDDL, cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\ndef preprocess(image):\n    \"\"\"\n    Create 4-dimensional blob from image and flip image\n    :param image: input image\n    \"\"\"\n    image_rev = np.flip(image, axis=1)",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "targets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.human_parsing",
        "description": "Hw_2.opencv.samples.dnn.human_parsing",
        "peekOfCode": "targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD,\n           cv.dnn.DNN_TARGET_HDDL, cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\ndef preprocess(image):\n    \"\"\"\n    Create 4-dimensional blob from image and flip image\n    :param image: input image\n    \"\"\"\n    image_rev = np.flip(image, axis=1)\n    input = cv.dnn.blobFromImages([image, image_rev], mean=(104.00698793, 116.66876762, 122.67891434))\n    return input",
        "detail": "Hw_2.opencv.samples.dnn.human_parsing",
        "documentation": {}
    },
    {
        "label": "showLegend",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "def showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))\n        legend = np.zeros((blockHeight * len(colors), 200, 3), np.uint8)\n        for i in range(len(classes)):\n            block = legend[i * blockHeight:(i + 1) * blockHeight]\n            block[:,:] = colors[i]\n            cv.putText(block, classes[i], (0, blockHeight//2), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "drawBox",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "def drawBox(frame, classId, conf, left, top, right, bottom):\n    # Draw a bounding box.\n    cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\n    label = '%.2f' % conf\n    # Print a label of class.\n    if classes:\n        assert(classId < len(classes))\n        label = '%s: %s' % (classes[classId], label)\n    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    top = max(top, labelSize[1])",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "parser = argparse.ArgumentParser(description=\n        'Use this script to run Mask-RCNN object detection and semantic '\n        'segmentation network from TensorFlow Object Detection API.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--model', required=True, help='Path to a .pb file with weights.')\nparser.add_argument('--config', required=True, help='Path to a .pxtxt file contains network configuration.')\nparser.add_argument('--classes', help='Optional path to a text file with names of classes.')\nparser.add_argument('--colors', help='Optional path to a text file with colors for an every class. '\n                                     'An every color is represented with three values from 0 to 255 in BGR channels order.')\nparser.add_argument('--width', type=int, default=800,",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "args = parser.parse_args()\nnp.random.seed(324)\n# Load names of classes\nclasses = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors\ncolors = None\nif args.colors:",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "classes = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors\ncolors = None\nif args.colors:\n    with open(args.colors, 'rt') as f:\n        colors = [np.array(color.split(' '), np.uint8) for color in f.read().rstrip('\\n').split('\\n')]\nlegend = None",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "colors = None\nif args.colors:\n    with open(args.colors, 'rt') as f:\n        colors = [np.array(color.split(' '), np.uint8) for color in f.read().rstrip('\\n').split('\\n')]\nlegend = None\ndef showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "legend",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "legend = None\ndef showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))\n        legend = np.zeros((blockHeight * len(colors), 200, 3), np.uint8)\n        for i in range(len(classes)):\n            block = legend[i * blockHeight:(i + 1) * blockHeight]\n            block[:,:] = colors[i]",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "net = cv.dnn.readNet(cv.samples.findFile(args.model), cv.samples.findFile(args.config))\nnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\nwinName = 'Mask-RCNN in OpenCV'\ncv.namedWindow(winName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "winName",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "winName = 'Mask-RCNN in OpenCV'\ncv.namedWindow(winName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameH = frame.shape[0]",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "cap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameH = frame.shape[0]\n    frameW = frame.shape[1]\n    # Create a 4D blob from a frame.",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "legend",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "peekOfCode": "legend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameH = frame.shape[0]\n    frameW = frame.shape[1]\n    # Create a 4D blob from a frame.\n    blob = cv.dnn.blobFromImage(frame, size=(args.width, args.height), swapRB=True, crop=False)",
        "detail": "Hw_2.opencv.samples.dnn.mask_rcnn",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description='Evaluate MobileNet-SSD model using both TensorFlow and OpenCV. '\n                'COCO evaluation framework is required: http://cocodataset.org')\nparser.add_argument('--weights', required=True,\n                    help='Path to frozen_inference_graph.pb of MobileNet-SSD model. '\n                         'Download it from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz')\nparser.add_argument('--prototxt', help='Path to ssd_mobilenet_v1_coco.pbtxt from opencv_extra.', required=True)\nparser.add_argument('--images', help='Path to COCO validation images directory.', required=True)\nparser.add_argument('--annotations', help='Path to COCO annotations file.', required=True)\nargs = parser.parse_args()",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "args = parser.parse_args()\n### Get OpenCV predictions #####################################################\nnet = cv.dnn.readNetFromTensorflow(cv.samples.findFile(args.weights), cv.samples.findFile(args.prototxt))\nnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\ndetections = []\nfor imgName in os.listdir(args.images):\n    inp = cv.imread(cv.samples.findFile(os.path.join(args.images, imgName)))\n    rows = inp.shape[0]\n    cols = inp.shape[1]\n    inp = cv.resize(inp, (300, 300))",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "net = cv.dnn.readNetFromTensorflow(cv.samples.findFile(args.weights), cv.samples.findFile(args.prototxt))\nnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\ndetections = []\nfor imgName in os.listdir(args.images):\n    inp = cv.imread(cv.samples.findFile(os.path.join(args.images, imgName)))\n    rows = inp.shape[0]\n    cols = inp.shape[1]\n    inp = cv.resize(inp, (300, 300))\n    net.setInput(cv.dnn.blobFromImage(inp, 1.0/127.5, (300, 300), (127.5, 127.5, 127.5), True))\n    out = net.forward()",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "detections",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "detections = []\nfor imgName in os.listdir(args.images):\n    inp = cv.imread(cv.samples.findFile(os.path.join(args.images, imgName)))\n    rows = inp.shape[0]\n    cols = inp.shape[1]\n    inp = cv.resize(inp, (300, 300))\n    net.setInput(cv.dnn.blobFromImage(inp, 1.0/127.5, (300, 300), (127.5, 127.5, 127.5), True))\n    out = net.forward()\n    for i in range(out.shape[2]):\n        score = float(out[0, 0, i, 2])",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "pylab.rcParams['figure.figsize']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\nannType = ['segm','bbox','keypoints']\nannType = annType[1]      #specify type here\nprefix = 'person_keypoints' if annType=='keypoints' else 'instances'\nprint('Running demo for *%s* results.'%(annType))\n#initialize COCO ground truth api\ncocoGt=COCO(args.annotations)\n#initialize COCO detections api\nfor resFile in ['tf_result.json', 'cv_result.json']:\n    print(resFile)",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "annType",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "annType = ['segm','bbox','keypoints']\nannType = annType[1]      #specify type here\nprefix = 'person_keypoints' if annType=='keypoints' else 'instances'\nprint('Running demo for *%s* results.'%(annType))\n#initialize COCO ground truth api\ncocoGt=COCO(args.annotations)\n#initialize COCO detections api\nfor resFile in ['tf_result.json', 'cv_result.json']:\n    print(resFile)\n    cocoDt=cocoGt.loadRes(resFile)",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "annType",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "annType = annType[1]      #specify type here\nprefix = 'person_keypoints' if annType=='keypoints' else 'instances'\nprint('Running demo for *%s* results.'%(annType))\n#initialize COCO ground truth api\ncocoGt=COCO(args.annotations)\n#initialize COCO detections api\nfor resFile in ['tf_result.json', 'cv_result.json']:\n    print(resFile)\n    cocoDt=cocoGt.loadRes(resFile)\n    cocoEval = COCOeval(cocoGt,cocoDt,annType)",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "description": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "peekOfCode": "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\nprint('Running demo for *%s* results.'%(annType))\n#initialize COCO ground truth api\ncocoGt=COCO(args.annotations)\n#initialize COCO detections api\nfor resFile in ['tf_result.json', 'cv_result.json']:\n    print(resFile)\n    cocoDt=cocoGt.loadRes(resFile)\n    cocoEval = COCOeval(cocoGt,cocoDt,annType)\n    cocoEval.evaluate()",
        "detail": "Hw_2.opencv.samples.dnn.mobilenet_ssd_accuracy",
        "documentation": {}
    },
    {
        "label": "QueueFPS",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "class QueueFPS(queue.Queue):\n    def __init__(self):\n        queue.Queue.__init__(self)\n        self.startTime = 0\n        self.counter = 0\n    def put(self, v):\n        queue.Queue.put(self, v)\n        self.counter += 1\n        if self.counter == 1:\n            self.startTime = time.time()",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "def postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    def drawPred(classId, conf, left, top, right, bottom):\n        # Draw a bounding box.\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\n        label = '%.2f' % conf\n        # Print a label of class.\n        if classes:\n            assert(classId < len(classes))",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "def callback(pos):\n    global confThreshold\n    confThreshold = pos / 100.0\ncv.createTrackbar('Confidence threshold, %', winName, int(confThreshold * 100), 99, callback)\ncap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nclass QueueFPS(queue.Queue):\n    def __init__(self):\n        queue.Queue.__init__(self)\n        self.startTime = 0\n        self.counter = 0",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "framesThreadBody",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "def framesThreadBody():\n    global framesQueue, process\n    while process:\n        hasFrame, frame = cap.read()\n        if not hasFrame:\n            break\n        framesQueue.put(frame)\n#\n# Frames processing thread\n#",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "processingThreadBody",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "def processingThreadBody():\n    global processedFramesQueue, predictionsQueue, args, process\n    futureOutputs = []\n    while process:\n        # Get a next frame\n        frame = None\n        try:\n            frame = framesQueue.get_nowait()\n            if args.asyncN:\n                if len(futureOutputs) == args.asyncN:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "backends",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_HALIDE, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE, cv.dnn.DNN_BACKEND_OPENCV,\n            cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\ntargets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--out_tf_graph', default='graph.pbtxt',\n                    help='For models from TensorFlow Object Detection API, you may '",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "targets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--out_tf_graph', default='graph.pbtxt',\n                    help='For models from TensorFlow Object Detection API, you may '\n                         'pass a .config file which was used for training through --config '\n                         'argument. This way an additional .pbtxt file with TensorFlow graph will be created.')",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "parser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--out_tf_graph', default='graph.pbtxt',\n                    help='For models from TensorFlow Object Detection API, you may '\n                         'pass a .config file which was used for training through --config '\n                         'argument. This way an additional .pbtxt file with TensorFlow graph will be created.')\nparser.add_argument('--framework', choices=['caffe', 'tensorflow', 'torch', 'darknet', 'dldt'],\n                    help='Optional name of an origin framework of the model. '",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "parser = argparse.ArgumentParser(parents=[parser],\n                                 description='Use this script to run object detection deep learning networks using OpenCV.',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nargs = parser.parse_args()\nargs.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\n# If config specified, try to load it as TensorFlow Object Detection API's pipeline.\nconfig = readTextMessage(args.config)\nif 'model' in config:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "args = parser.parse_args()\nargs.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\n# If config specified, try to load it as TensorFlow Object Detection API's pipeline.\nconfig = readTextMessage(args.config)\nif 'model' in config:\n    print('TensorFlow Object Detection API config detected')\n    if 'ssd' in config['model'][0]:\n        print('Preparing text graph representation for SSD model: ' + args.out_tf_graph)",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "args.model",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "args.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\n# If config specified, try to load it as TensorFlow Object Detection API's pipeline.\nconfig = readTextMessage(args.config)\nif 'model' in config:\n    print('TensorFlow Object Detection API config detected')\n    if 'ssd' in config['model'][0]:\n        print('Preparing text graph representation for SSD model: ' + args.out_tf_graph)\n        createSSDGraph(args.model, args.config, args.out_tf_graph)",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "args.config",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "args.config = findFile(args.config)\nargs.classes = findFile(args.classes)\n# If config specified, try to load it as TensorFlow Object Detection API's pipeline.\nconfig = readTextMessage(args.config)\nif 'model' in config:\n    print('TensorFlow Object Detection API config detected')\n    if 'ssd' in config['model'][0]:\n        print('Preparing text graph representation for SSD model: ' + args.out_tf_graph)\n        createSSDGraph(args.model, args.config, args.out_tf_graph)\n        args.config = args.out_tf_graph",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "args.classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "args.classes = findFile(args.classes)\n# If config specified, try to load it as TensorFlow Object Detection API's pipeline.\nconfig = readTextMessage(args.config)\nif 'model' in config:\n    print('TensorFlow Object Detection API config detected')\n    if 'ssd' in config['model'][0]:\n        print('Preparing text graph representation for SSD model: ' + args.out_tf_graph)\n        createSSDGraph(args.model, args.config, args.out_tf_graph)\n        args.config = args.out_tf_graph\n    elif 'faster_rcnn' in config['model'][0]:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "config = readTextMessage(args.config)\nif 'model' in config:\n    print('TensorFlow Object Detection API config detected')\n    if 'ssd' in config['model'][0]:\n        print('Preparing text graph representation for SSD model: ' + args.out_tf_graph)\n        createSSDGraph(args.model, args.config, args.out_tf_graph)\n        args.config = args.out_tf_graph\n    elif 'faster_rcnn' in config['model'][0]:\n        print('Preparing text graph representation for Faster-RCNN model: ' + args.out_tf_graph)\n        createFasterRCNNGraph(args.model, args.config, args.out_tf_graph)",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "classes = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load a network\nnet = cv.dnn.readNet(cv.samples.findFile(args.model), cv.samples.findFile(args.config), args.framework)\nnet.setPreferableBackend(args.backend)\nnet.setPreferableTarget(args.target)\noutNames = net.getUnconnectedOutLayersNames()\nconfThreshold = args.thr",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "net = cv.dnn.readNet(cv.samples.findFile(args.model), cv.samples.findFile(args.config), args.framework)\nnet.setPreferableBackend(args.backend)\nnet.setPreferableTarget(args.target)\noutNames = net.getUnconnectedOutLayersNames()\nconfThreshold = args.thr\nnmsThreshold = args.nms\ndef postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    def drawPred(classId, conf, left, top, right, bottom):",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "outNames",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "outNames = net.getUnconnectedOutLayersNames()\nconfThreshold = args.thr\nnmsThreshold = args.nms\ndef postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    def drawPred(classId, conf, left, top, right, bottom):\n        # Draw a bounding box.\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\n        label = '%.2f' % conf",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "confThreshold",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "confThreshold = args.thr\nnmsThreshold = args.nms\ndef postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    def drawPred(classId, conf, left, top, right, bottom):\n        # Draw a bounding box.\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\n        label = '%.2f' % conf\n        # Print a label of class.",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "nmsThreshold",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "nmsThreshold = args.nms\ndef postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    def drawPred(classId, conf, left, top, right, bottom):\n        # Draw a bounding box.\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\n        label = '%.2f' % conf\n        # Print a label of class.\n        if classes:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "winName",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "winName = 'Deep learning object detection in OpenCV'\ncv.namedWindow(winName, cv.WINDOW_NORMAL)\ndef callback(pos):\n    global confThreshold\n    confThreshold = pos / 100.0\ncv.createTrackbar('Confidence threshold, %', winName, int(confThreshold * 100), 99, callback)\ncap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nclass QueueFPS(queue.Queue):\n    def __init__(self):\n        queue.Queue.__init__(self)",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "cap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else 0)\nclass QueueFPS(queue.Queue):\n    def __init__(self):\n        queue.Queue.__init__(self)\n        self.startTime = 0\n        self.counter = 0\n    def put(self, v):\n        queue.Queue.put(self, v)\n        self.counter += 1\n        if self.counter == 1:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "process",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "process = True\n#\n# Frames capturing thread\n#\nframesQueue = QueueFPS()\ndef framesThreadBody():\n    global framesQueue, process\n    while process:\n        hasFrame, frame = cap.read()\n        if not hasFrame:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "framesQueue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "framesQueue = QueueFPS()\ndef framesThreadBody():\n    global framesQueue, process\n    while process:\n        hasFrame, frame = cap.read()\n        if not hasFrame:\n            break\n        framesQueue.put(frame)\n#\n# Frames processing thread",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "processedFramesQueue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "processedFramesQueue = queue.Queue()\npredictionsQueue = QueueFPS()\ndef processingThreadBody():\n    global processedFramesQueue, predictionsQueue, args, process\n    futureOutputs = []\n    while process:\n        # Get a next frame\n        frame = None\n        try:\n            frame = framesQueue.get_nowait()",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "predictionsQueue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "predictionsQueue = QueueFPS()\ndef processingThreadBody():\n    global processedFramesQueue, predictionsQueue, args, process\n    futureOutputs = []\n    while process:\n        # Get a next frame\n        frame = None\n        try:\n            frame = framesQueue.get_nowait()\n            if args.asyncN:",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "framesThread",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "framesThread = Thread(target=framesThreadBody)\nframesThread.start()\nprocessingThread = Thread(target=processingThreadBody)\nprocessingThread.start()\n#\n# Postprocessing and rendering loop\n#\nwhile cv.waitKey(1) < 0:\n    try:\n        # Request prediction first because they put after frames",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "processingThread",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "processingThread = Thread(target=processingThreadBody)\nprocessingThread.start()\n#\n# Postprocessing and rendering loop\n#\nwhile cv.waitKey(1) < 0:\n    try:\n        # Request prediction first because they put after frames\n        outs = predictionsQueue.get_nowait()\n        frame = processedFramesQueue.get_nowait()",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "process",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.object_detection",
        "description": "Hw_2.opencv.samples.dnn.object_detection",
        "peekOfCode": "process = False\nframesThread.join()\nprocessingThread.join()",
        "detail": "Hw_2.opencv.samples.dnn.object_detection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "parser = argparse.ArgumentParser(\n        description='This script is used to demonstrate OpenPose human pose estimation network '\n                    'from https://github.com/CMU-Perceptual-Computing-Lab/openpose project using OpenCV. '\n                    'The sample and model are simplified and could be used for a single person on the frame.')\nparser.add_argument('--input', help='Path to image or video. Skip to capture frames from camera')\nparser.add_argument('--proto', help='Path to .prototxt')\nparser.add_argument('--model', help='Path to .caffemodel')\nparser.add_argument('--dataset', help='Specify what kind of model was trained. '\n                                      'It could be (COCO, MPI, HAND) depends on dataset.')\nparser.add_argument('--thr', default=0.1, type=float, help='Threshold value for pose parts heat map')",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "args = parser.parse_args()\nif args.dataset == 'COCO':\n    BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n                   \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n                   \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n                   \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n    POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n                   [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n                   [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n                   [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "inWidth",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "inWidth = args.width\ninHeight = args.height\ninScale = args.scale\nnet = cv.dnn.readNet(cv.samples.findFile(args.proto), cv.samples.findFile(args.model))\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "inHeight",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "inHeight = args.height\ninScale = args.scale\nnet = cv.dnn.readNet(cv.samples.findFile(args.proto), cv.samples.findFile(args.model))\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameWidth = frame.shape[1]",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "inScale",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "inScale = args.scale\nnet = cv.dnn.readNet(cv.samples.findFile(args.proto), cv.samples.findFile(args.model))\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameWidth = frame.shape[1]\n    frameHeight = frame.shape[0]",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "net = cv.dnn.readNet(cv.samples.findFile(args.proto), cv.samples.findFile(args.model))\ncap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameWidth = frame.shape[1]\n    frameHeight = frame.shape[0]\n    inp = cv.dnn.blobFromImage(frame, inScale, (inWidth, inHeight),",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.openpose",
        "description": "Hw_2.opencv.samples.dnn.openpose",
        "peekOfCode": "cap = cv.VideoCapture(args.input if args.input else 0)\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameWidth = frame.shape[1]\n    frameHeight = frame.shape[0]\n    inp = cv.dnn.blobFromImage(frame, inScale, (inWidth, inHeight),\n                              (0, 0, 0), swapRB=False, crop=False)",
        "detail": "Hw_2.opencv.samples.dnn.openpose",
        "documentation": {}
    },
    {
        "label": "OpticalFlow",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.optical_flow",
        "description": "Hw_2.opencv.samples.dnn.optical_flow",
        "peekOfCode": "class OpticalFlow(object):\n    def __init__(self, proto, model, height, width):\n        self.net = cv.dnn.readNetFromCaffe(proto, model)\n        self.net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n        self.height = height\n        self.width = width\n    def compute_flow(self, first_img, second_img):\n        inp0 = cv.dnn.blobFromImage(first_img, size=(self.width, self.height))\n        inp1 = cv.dnn.blobFromImage(second_img, size=(self.width, self.height))\n        self.net.setInput(inp0, \"img0\")",
        "detail": "Hw_2.opencv.samples.dnn.optical_flow",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def preprocess(images, height, width):\n    \"\"\"\n    Create 4-dimensional blob from image\n    :param image: input image\n    :param height: the height of the resized input image\n    :param width: the width of the resized input image\n    \"\"\"\n    img_list = []\n    for image in images:\n        image = cv.resize(image, (width, height))",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "extract_feature",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def extract_feature(img_dir, model_path, batch_size = 32, resize_h = 384, resize_w = 128, backend=cv.dnn.DNN_BACKEND_OPENCV, target=cv.dnn.DNN_TARGET_CPU):\n    \"\"\"\n    Extract features from images in a target directory\n    :param img_dir: the input image directory\n    :param model_path: path to ReID model\n    :param batch_size: the batch size for each network inference iteration\n    :param resize_h: the height of the input image\n    :param resize_w: the width of the input image\n    :param backend: name of computation backend\n    :param target: name of computation target",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "run_net",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def run_net(inputs, model_path, backend=cv.dnn.DNN_BACKEND_OPENCV, target=cv.dnn.DNN_TARGET_CPU):\n    \"\"\"\n    Forword propagation for a batch of images.\n    :param inputs: input batch of images\n    :param model_path: path to ReID model\n    :param backend: name of computation backend\n    :param target: name of computation target\n    \"\"\"\n    net = cv.dnn.readNet(model_path)\n    net.setPreferableBackend(backend)",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def read_data(path_list):\n    \"\"\"\n    Read all images from a directory into a list\n    :param path_list: the list of image path\n    \"\"\"\n    img_list = []\n    for img_path in path_list:\n        img = cv.imread(img_path)\n        if img is None:\n            continue",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def normalize(nparray, order=2, axis=0):\n    \"\"\"\n    Normalize a N-D numpy array along the specified axis.\n    :param nparry: the array of vectors to be normalized\n    :param order: order of the norm\n    :param axis: the axis of x along which to compute the vector norms\n    \"\"\"\n    norm = np.linalg.norm(nparray, ord=order, axis=axis, keepdims=True)\n    return nparray / (norm + np.finfo(np.float32).eps)\ndef similarity(array1, array2):",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "similarity",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def similarity(array1, array2):\n    \"\"\"\n    Compute the euclidean or cosine distance of all pairs.\n    :param  array1: numpy array with shape [m1, n]\n    :param  array2: numpy array with shape [m2, n]\n    Returns:\n      numpy array with shape [m1, m2]\n    \"\"\"\n    array1 = normalize(array1, axis=1)\n    array2 = normalize(array2, axis=1)",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "topk",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def topk(query_feat, gallery_feat, topk = 5):\n    \"\"\"\n    Return the index of top K gallery images most similar to the query images\n    :param query_feat: array of feature vectors of query images\n    :param gallery_feat: array of feature vectors of gallery images\n    :param topk: number of gallery images to return\n    \"\"\"\n    sim = similarity(query_feat, gallery_feat)\n    index = np.argsort(-sim, axis = 1)\n    return [i[0:int(topk)] for i in index]",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "drawRankList",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def drawRankList(query_name, gallery_list, output_size = (128, 384)):\n    \"\"\"\n    Draw the rank list\n    :param query_name: path of the query image\n    :param gallery_name: path of the gallery image\n    \"param output_size: the output size of each image in the rank list\n    \"\"\"\n    def addBorder(im, color):\n        bordersize = 5\n        border = cv.copyMakeBorder(",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "visualization",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "def visualization(topk_idx, query_names, gallery_names, output_dir = 'vis'):\n    \"\"\"\n    Visualize the retrieval results with the person ReID model\n    :param topk_idx: the index of ranked gallery images for each query image\n    :param query_names: the list of paths of query images\n    :param gallery_names: the list of paths of gallery images\n    :param output_dir: the path to save the visualize results\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "backends",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "backends = (cv.dnn.DNN_BACKEND_DEFAULT,\n    cv.dnn.DNN_BACKEND_INFERENCE_ENGINE,\n    cv.dnn.DNN_BACKEND_OPENCV,\n    cv.dnn.DNN_BACKEND_VKCOM,\n    cv.dnn.DNN_BACKEND_CUDA)\ntargets = (cv.dnn.DNN_TARGET_CPU,\n    cv.dnn.DNN_TARGET_OPENCL,\n    cv.dnn.DNN_TARGET_OPENCL_FP16,\n    cv.dnn.DNN_TARGET_MYRIAD,\n    cv.dnn.DNN_TARGET_HDDL,",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "targets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "targets = (cv.dnn.DNN_TARGET_CPU,\n    cv.dnn.DNN_TARGET_OPENCL,\n    cv.dnn.DNN_TARGET_OPENCL_FP16,\n    cv.dnn.DNN_TARGET_MYRIAD,\n    cv.dnn.DNN_TARGET_HDDL,\n    cv.dnn.DNN_TARGET_VULKAN,\n    cv.dnn.DNN_TARGET_CUDA,\n    cv.dnn.DNN_TARGET_CUDA_FP16)\nMEAN = (0.485, 0.456, 0.406)\nSTD = (0.229, 0.224, 0.225)",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "MEAN",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "MEAN = (0.485, 0.456, 0.406)\nSTD = (0.229, 0.224, 0.225)\ndef preprocess(images, height, width):\n    \"\"\"\n    Create 4-dimensional blob from image\n    :param image: input image\n    :param height: the height of the resized input image\n    :param width: the width of the resized input image\n    \"\"\"\n    img_list = []",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "STD",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.person_reid",
        "description": "Hw_2.opencv.samples.dnn.person_reid",
        "peekOfCode": "STD = (0.229, 0.224, 0.225)\ndef preprocess(images, height, width):\n    \"\"\"\n    Create 4-dimensional blob from image\n    :param image: input image\n    :param height: the height of the resized input image\n    :param width: the width of the resized input image\n    \"\"\"\n    img_list = []\n    for image in images:",
        "detail": "Hw_2.opencv.samples.dnn.person_reid",
        "documentation": {}
    },
    {
        "label": "showLegend",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "def showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))\n        legend = np.zeros((blockHeight * len(colors), 200, 3), np.uint8)\n        for i in range(len(classes)):\n            block = legend[i * blockHeight:(i + 1) * blockHeight]\n            block[:,:] = colors[i]\n            cv.putText(block, classes[i], (0, blockHeight//2), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "backends",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_HALIDE, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE, cv.dnn.DNN_BACKEND_OPENCV,\n            cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\ntargets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--framework', choices=['caffe', 'tensorflow', 'torch', 'darknet'],\n                    help='Optional name of an origin framework of the model. '",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "targets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--framework', choices=['caffe', 'tensorflow', 'torch', 'darknet'],\n                    help='Optional name of an origin framework of the model. '\n                         'Detect it automatically if it does not set.')\nparser.add_argument('--colors', help='Optional path to a text file with colors for an every class. '",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "parser = argparse.ArgumentParser(add_help=False)\nparser.add_argument('--zoo', default=os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models.yml'),\n                    help='An optional path to file with preprocessing parameters.')\nparser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--framework', choices=['caffe', 'tensorflow', 'torch', 'darknet'],\n                    help='Optional name of an origin framework of the model. '\n                         'Detect it automatically if it does not set.')\nparser.add_argument('--colors', help='Optional path to a text file with colors for an every class. '\n                                     'An every color is represented with three values from 0 to 255 in BGR channels order.')\nparser.add_argument('--backend', choices=backends, default=cv.dnn.DNN_BACKEND_DEFAULT, type=int,",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "parser = argparse.ArgumentParser(parents=[parser],\n                                 description='Use this script to run semantic segmentation deep learning networks using OpenCV.',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nargs = parser.parse_args()\nargs.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\nnp.random.seed(324)\n# Load names of classes\nclasses = None",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "args = parser.parse_args()\nargs.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\nnp.random.seed(324)\n# Load names of classes\nclasses = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "args.model",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "args.model = findFile(args.model)\nargs.config = findFile(args.config)\nargs.classes = findFile(args.classes)\nnp.random.seed(324)\n# Load names of classes\nclasses = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "args.config",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "args.config = findFile(args.config)\nargs.classes = findFile(args.classes)\nnp.random.seed(324)\n# Load names of classes\nclasses = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors\ncolors = None",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "args.classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "args.classes = findFile(args.classes)\nnp.random.seed(324)\n# Load names of classes\nclasses = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors\ncolors = None\nif args.colors:",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "classes = None\nif args.classes:\n    with open(args.classes, 'rt') as f:\n        classes = f.read().rstrip('\\n').split('\\n')\n# Load colors\ncolors = None\nif args.colors:\n    with open(args.colors, 'rt') as f:\n        colors = [np.array(color.split(' '), np.uint8) for color in f.read().rstrip('\\n').split('\\n')]\nlegend = None",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "colors = None\nif args.colors:\n    with open(args.colors, 'rt') as f:\n        colors = [np.array(color.split(' '), np.uint8) for color in f.read().rstrip('\\n').split('\\n')]\nlegend = None\ndef showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "legend",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "legend = None\ndef showLegend(classes):\n    global legend\n    if not classes is None and legend is None:\n        blockHeight = 30\n        assert(len(classes) == len(colors))\n        legend = np.zeros((blockHeight * len(colors), 200, 3), np.uint8)\n        for i in range(len(classes)):\n            block = legend[i * blockHeight:(i + 1) * blockHeight]\n            block[:,:] = colors[i]",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "net = cv.dnn.readNet(args.model, args.config, args.framework)\nnet.setPreferableBackend(args.backend)\nnet.setPreferableTarget(args.target)\nwinName = 'Deep learning semantic segmentation in OpenCV'\ncv.namedWindow(winName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(args.input if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "winName",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "winName = 'Deep learning semantic segmentation in OpenCV'\ncv.namedWindow(winName, cv.WINDOW_NORMAL)\ncap = cv.VideoCapture(args.input if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameHeight = frame.shape[0]",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "cap = cv.VideoCapture(args.input if args.input else 0)\nlegend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    # Create a 4D blob from a frame.",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "legend",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.segmentation",
        "description": "Hw_2.opencv.samples.dnn.segmentation",
        "peekOfCode": "legend = None\nwhile cv.waitKey(1) < 0:\n    hasFrame, frame = cap.read()\n    if not hasFrame:\n        cv.waitKey()\n        break\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n    # Create a 4D blob from a frame.\n    inpWidth = args.width if args.width else frameWidth",
        "detail": "Hw_2.opencv.samples.dnn.segmentation",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Convert weights of a frozen TensorFlow graph to fp16.')\nparser.add_argument('--input', required=True, help='Path to frozen graph.')\nparser.add_argument('--output', required=True, help='Path to output graph.')\nparser.add_argument('--ops', default=['Conv2D', 'MatMul'], nargs='+',\n                    help='List of ops which weights are converted.')\nargs = parser.parse_args()\nDT_FLOAT = 1\nDT_HALF = 19\n# For the frozen graphs, an every node that uses weights connected to Const nodes\n# through an Identity node. Usually they're called in the same way with '/read' suffix.",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "args = parser.parse_args()\nDT_FLOAT = 1\nDT_HALF = 19\n# For the frozen graphs, an every node that uses weights connected to Const nodes\n# through an Identity node. Usually they're called in the same way with '/read' suffix.\n# We'll replace all of them to Cast nodes.\n# Load the model\nwith tf.gfile.FastGFile(args.input) as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "DT_FLOAT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "DT_FLOAT = 1\nDT_HALF = 19\n# For the frozen graphs, an every node that uses weights connected to Const nodes\n# through an Identity node. Usually they're called in the same way with '/read' suffix.\n# We'll replace all of them to Cast nodes.\n# Load the model\nwith tf.gfile.FastGFile(args.input) as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n# Set of all inputs from desired nodes.",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "DT_HALF",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "DT_HALF = 19\n# For the frozen graphs, an every node that uses weights connected to Const nodes\n# through an Identity node. Usually they're called in the same way with '/read' suffix.\n# We'll replace all of them to Cast nodes.\n# Load the model\nwith tf.gfile.FastGFile(args.input) as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n# Set of all inputs from desired nodes.\ninputs = []",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "inputs = []\nfor node in graph_def.node:\n    if node.op in args.ops:\n        inputs += node.input\nweightsNodes = []\nfor node in graph_def.node:\n    # From the whole inputs we need to keep only an Identity nodes.\n    if node.name in inputs and node.op == 'Identity' and node.attr['T'].type == DT_FLOAT:\n        weightsNodes.append(node.input[0])\n        # Replace Identity to Cast.",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "weightsNodes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "description": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "peekOfCode": "weightsNodes = []\nfor node in graph_def.node:\n    # From the whole inputs we need to keep only an Identity nodes.\n    if node.name in inputs and node.op == 'Identity' and node.attr['T'].type == DT_FLOAT:\n        weightsNodes.append(node.input[0])\n        # Replace Identity to Cast.\n        node.op = 'Cast'\n        node.attr['DstT'].type = DT_FLOAT\n        node.attr['SrcT'].type = DT_HALF\n        del node.attr['T']",
        "detail": "Hw_2.opencv.samples.dnn.shrink_tf_graph_weights",
        "documentation": {}
    },
    {
        "label": "ModelBuilder",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "description": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "peekOfCode": "class ModelBuilder():\n    \"\"\" This class generates the SiamRPN++ Tracker Model by using Imported ONNX Nets\n    \"\"\"\n    def __init__(self, target_net, search_net, rpn_head):\n        super(ModelBuilder, self).__init__()\n        # Build the target branch\n        self.target_net = target_net\n        # Build the search branch\n        self.search_net = search_net\n        # Build RPN_Head",
        "detail": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "documentation": {}
    },
    {
        "label": "Anchors",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "description": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "peekOfCode": "class Anchors:\n    \"\"\" This class generate anchors.\n    \"\"\"\n    def __init__(self, stride, ratios, scales, image_center=0, size=0):\n        self.stride = stride\n        self.ratios = ratios\n        self.scales = scales\n        self.image_center = image_center\n        self.size = size\n        self.anchor_num = len(self.scales) * len(self.ratios)",
        "detail": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "documentation": {}
    },
    {
        "label": "SiamRPNTracker",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "description": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "peekOfCode": "class SiamRPNTracker:\n    def __init__(self, model):\n        super(SiamRPNTracker, self).__init__()\n        self.anchor_stride = 8\n        self.anchor_ratios = [0.33, 0.5, 1, 2, 3]\n        self.anchor_scales = [8]\n        self.track_base_size = 8\n        self.track_context_amount = 0.5\n        self.track_exemplar_size = 127\n        self.track_instance_size = 255",
        "detail": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "documentation": {}
    },
    {
        "label": "get_frames",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "description": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "peekOfCode": "def get_frames(video_name):\n    \"\"\"\n    Args:\n        Path to input video frame\n    Return:\n        Frame\n    \"\"\"\n    cap = cv.VideoCapture(video_name if video_name else 0)\n    while True:\n        ret, frame = cap.read()",
        "detail": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "description": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "peekOfCode": "def main():\n    \"\"\" Sample SiamRPN Tracker\n    \"\"\"\n    # Computation backends supported by layers\n    backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_HALIDE, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE, cv.dnn.DNN_BACKEND_OPENCV,\n                cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\n    # Target Devices for computation\n    targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD,\n               cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\n    parser = argparse.ArgumentParser(description='Use this script to run SiamRPN++ Visual Tracker',",
        "detail": "Hw_2.opencv.samples.dnn.siamrpnpp",
        "documentation": {}
    },
    {
        "label": "FilterbankFeatures",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.speech_recognition",
        "description": "Hw_2.opencv.samples.dnn.speech_recognition",
        "peekOfCode": "class FilterbankFeatures:\n    def __init__(self,\n                 sample_rate=16000, window_size=0.02, window_stride=0.01,\n                 n_fft=512, preemph=0.97, n_filt=64, lowfreq=0,\n                 highfreq=None, log=True, dither=1e-5):\n        '''\n            Initializes pre-processing class. Default values are the values used by the Jasper\n            architecture for pre-processing. For more details, refer to the paper here:\n            https://arxiv.org/abs/1904.03288\n        '''",
        "detail": "Hw_2.opencv.samples.dnn.speech_recognition",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.speech_recognition",
        "description": "Hw_2.opencv.samples.dnn.speech_recognition",
        "peekOfCode": "class Decoder:\n    '''\n        Used for decoding the output of jasper model.\n    '''\n    def __init__(self):\n        labels=[' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\"'\"]\n        self.labels_map = {i: label for i,label in enumerate(labels)}\n        self.blank_id = 28\n    def decode(self,x):\n        \"\"\"",
        "detail": "Hw_2.opencv.samples.dnn.speech_recognition",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.speech_recognition",
        "description": "Hw_2.opencv.samples.dnn.speech_recognition",
        "peekOfCode": "def predict(features, net, decoder):\n    '''\n        Passes the features through the Jasper model and decodes the output to english transcripts.\n        args:\n            features : input features, calculated using FilterbankFeatures class\n            net : Jasper model dnn.net object\n            decoder : Decoder object\n        return : Predicted text\n    '''\n    # make prediction",
        "detail": "Hw_2.opencv.samples.dnn.speech_recognition",
        "documentation": {}
    },
    {
        "label": "readAudioFile",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.speech_recognition",
        "description": "Hw_2.opencv.samples.dnn.speech_recognition",
        "peekOfCode": "def readAudioFile(file, audioStream):\n    cap = cv.VideoCapture(file)\n    samplingRate = 16000\n    params = np.asarray([cv.CAP_PROP_AUDIO_STREAM, audioStream,\n              cv.CAP_PROP_VIDEO_STREAM, -1,\n              cv.CAP_PROP_AUDIO_DATA_DEPTH, cv.CV_32F,\n              cv.CAP_PROP_AUDIO_SAMPLES_PER_SECOND, samplingRate\n              ])\n    cap.open(file, cv.CAP_ANY, params)\n    if cap.isOpened() is False:",
        "detail": "Hw_2.opencv.samples.dnn.speech_recognition",
        "documentation": {}
    },
    {
        "label": "readAudioMicrophone",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.speech_recognition",
        "description": "Hw_2.opencv.samples.dnn.speech_recognition",
        "peekOfCode": "def readAudioMicrophone(microTime):\n    cap = cv.VideoCapture()\n    samplingRate = 16000\n    params = np.asarray([cv.CAP_PROP_AUDIO_STREAM, 0,\n              cv.CAP_PROP_VIDEO_STREAM, -1,\n              cv.CAP_PROP_AUDIO_DATA_DEPTH, cv.CV_32F,\n              cv.CAP_PROP_AUDIO_SAMPLES_PER_SECOND, samplingRate\n              ])\n    cap.open(0, cv.CAP_ANY, params)\n    if cap.isOpened() is False:",
        "detail": "Hw_2.opencv.samples.dnn.speech_recognition",
        "documentation": {}
    },
    {
        "label": "fourPointsTransform",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "def fourPointsTransform(frame, vertices):\n    vertices = np.asarray(vertices)\n    outputSize = (100, 32)\n    targetVertices = np.array([\n        [0, outputSize[1] - 1],\n        [0, 0],\n        [outputSize[0] - 1, 0],\n        [outputSize[0] - 1, outputSize[1] - 1]], dtype=\"float32\")\n    rotationMatrix = cv.getPerspectiveTransform(vertices, targetVertices)\n    result = cv.warpPerspective(frame, rotationMatrix, outputSize)",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "decodeText",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "def decodeText(scores):\n    text = \"\"\n    alphabet = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n    for i in range(scores.shape[0]):\n        c = np.argmax(scores[i][0])\n        if c != 0:\n            text += alphabet[c - 1]\n        else:\n            text += '-'\n    # adjacent same letters as well as background text must be removed to get the final output",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "decodeBoundingBoxes",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "def decodeBoundingBoxes(scores, geometry, scoreThresh):\n    detections = []\n    confidences = []\n    ############ CHECK DIMENSIONS AND SHAPES OF geometry AND scores ############\n    assert len(scores.shape) == 4, \"Incorrect dimensions of scores\"\n    assert len(geometry.shape) == 4, \"Incorrect dimensions of geometry\"\n    assert scores.shape[0] == 1, \"Invalid dimensions of scores\"\n    assert geometry.shape[0] == 1, \"Invalid dimensions of geometry\"\n    assert scores.shape[1] == 1, \"Invalid dimensions of scores\"\n    assert geometry.shape[1] == 5, \"Invalid dimensions of geometry\"",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "def main():\n    # Read and store arguments\n    confThreshold = args.thr\n    nmsThreshold = args.nms\n    inpWidth = args.width\n    inpHeight = args.height\n    modelDetector = args.model\n    modelRecognition = args.ocr\n    # Load network\n    detector = cv.dnn.readNet(modelDetector)",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Use this script to run TensorFlow implementation (https://github.com/argman/EAST) of \"\n                \"EAST: An Efficient and Accurate Scene Text Detector (https://arxiv.org/abs/1704.03155v2)\"\n                \"The OCR model can be obtained from converting the pretrained CRNN model to .onnx format from the github repository https://github.com/meijieru/crnn.pytorch\"\n                \"Or you can download trained OCR model directly from https://drive.google.com/drive/folders/1cTbQ3nuZG-EKWak6emD_s8_hHXWz7lAr?usp=sharing\")\nparser.add_argument('--input',\n                    help='Path to input image or video file. Skip this argument to capture frames from a camera.')\nparser.add_argument('--model', '-m', required=True,\n                    help='Path to a binary .pb file contains trained detector network.')\nparser.add_argument('--ocr', default=\"crnn.onnx\",",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.text_detection",
        "description": "Hw_2.opencv.samples.dnn.text_detection",
        "peekOfCode": "args = parser.parse_args()\n############ Utility functions ############\ndef fourPointsTransform(frame, vertices):\n    vertices = np.asarray(vertices)\n    outputSize = (100, 32)\n    targetVertices = np.array([\n        [0, outputSize[1] - 1],\n        [0, 0],\n        [outputSize[0] - 1, 0],\n        [outputSize[0] - 1, outputSize[1] - 1]], dtype=\"float32\")",
        "detail": "Hw_2.opencv.samples.dnn.text_detection",
        "documentation": {}
    },
    {
        "label": "NodeDef",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "class NodeDef:\n    def __init__(self):\n        self.input = []\n        self.name = \"\"\n        self.op = \"\"\n        self.attr = {}\n    def addAttr(self, key, value):\n        assert(not key in self.attr)\n        if isinstance(value, bool):\n            self.attr[key] = {'b': value}",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "GraphDef",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "class GraphDef:\n    def __init__(self):\n        self.node = []\n    def save(self, filePath):\n        with open(filePath, 'wt') as f:\n            def printAttr(d, indent):\n                indent = ' ' * indent\n                for key, value in sorted(d.items(), key=lambda x:x[0].lower()):\n                    value = value if isinstance(value, list) else [value]\n                    for v in value:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def tokenize(s):\n    tokens = []\n    token = \"\"\n    isString = False\n    isComment = False\n    for symbol in s:\n        isComment = (isComment and symbol != '\\n') or (not isString and symbol == '#')\n        if isComment:\n            continue\n        if symbol == ' ' or symbol == '\\t' or symbol == '\\r' or symbol == '\\'' or \\",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "parseMessage",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def parseMessage(tokens, idx):\n    msg = {}\n    assert(tokens[idx] == '{')\n    isArray = False\n    while True:\n        if not isArray:\n            idx += 1\n            if idx < len(tokens):\n                fieldName = tokens[idx]\n            else:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "readTextMessage",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def readTextMessage(filePath):\n    if not filePath:\n        return {}\n    with open(filePath, 'rt') as f:\n        content = f.read()\n    tokens = tokenize('{' + content + '}')\n    msg = parseMessage(tokens, 0)\n    return msg[0] if msg else {}\ndef listToTensor(values):\n    if all([isinstance(v, float) for v in values]):",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "listToTensor",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def listToTensor(values):\n    if all([isinstance(v, float) for v in values]):\n        dtype = 'DT_FLOAT'\n        field = 'float_val'\n    elif all([isinstance(v, int) for v in values]):\n        dtype = 'DT_INT32'\n        field = 'int_val'\n    else:\n        raise Exception('Wrong values types')\n    msg = {",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "addConstNode",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def addConstNode(name, values, graph_def):\n    node = NodeDef()\n    node.name = name\n    node.op = 'Const'\n    node.addAttr('value', values)\n    graph_def.node.extend([node])\ndef addSlice(inp, out, begins, sizes, graph_def):\n    beginsNode = NodeDef()\n    beginsNode.name = out + '/begins'\n    beginsNode.op = 'Const'",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "addSlice",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def addSlice(inp, out, begins, sizes, graph_def):\n    beginsNode = NodeDef()\n    beginsNode.name = out + '/begins'\n    beginsNode.op = 'Const'\n    beginsNode.addAttr('value', begins)\n    graph_def.node.extend([beginsNode])\n    sizesNode = NodeDef()\n    sizesNode.name = out + '/sizes'\n    sizesNode.op = 'Const'\n    sizesNode.addAttr('value', sizes)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "addReshape",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def addReshape(inp, out, shape, graph_def):\n    shapeNode = NodeDef()\n    shapeNode.name = out + '/shape'\n    shapeNode.op = 'Const'\n    shapeNode.addAttr('value', shape)\n    graph_def.node.extend([shapeNode])\n    reshape = NodeDef()\n    reshape.name = out\n    reshape.op = 'Reshape'\n    reshape.input.append(inp)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "addSoftMax",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def addSoftMax(inp, out, graph_def):\n    softmax = NodeDef()\n    softmax.name = out\n    softmax.op = 'Softmax'\n    softmax.addAttr('axis', -1)\n    softmax.input.append(inp)\n    graph_def.node.extend([softmax])\ndef addFlatten(inp, out, graph_def):\n    flatten = NodeDef()\n    flatten.name = out",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "addFlatten",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def addFlatten(inp, out, graph_def):\n    flatten = NodeDef()\n    flatten.name = out\n    flatten.op = 'Flatten'\n    flatten.input.append(inp)\n    graph_def.node.extend([flatten])\nclass NodeDef:\n    def __init__(self):\n        self.input = []\n        self.name = \"\"",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "parseTextGraph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def parseTextGraph(filePath):\n    msg = readTextMessage(filePath)\n    graph = GraphDef()\n    for node in msg['node']:\n        graphNode = NodeDef()\n        graphNode.name = node['name'][0]\n        graphNode.op = node['op'][0]\n        graphNode.input = node['input'] if 'input' in node else []\n        if 'attr' in node:\n            for attr in node['attr']:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "removeIdentity",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def removeIdentity(graph_def):\n    identities = {}\n    for node in graph_def.node:\n        if node.op == 'Identity' or node.op == 'IdentityN':\n            inp = node.input[0]\n            if inp in identities:\n                identities[node.name] = identities[inp]\n            else:\n                identities[node.name] = inp\n            graph_def.node.remove(node)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "removeUnusedNodesAndAttrs",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def removeUnusedNodesAndAttrs(to_remove, graph_def):\n    unusedAttrs = ['T', 'Tshape', 'N', 'Tidx', 'Tdim', 'use_cudnn_on_gpu',\n                   'Index', 'Tperm', 'is_training', 'Tpaddings']\n    removedNodes = []\n    for i in reversed(range(len(graph_def.node))):\n        op = graph_def.node[i].op\n        name = graph_def.node[i].name\n        if to_remove(name, op):\n            if op != 'Const':\n                removedNodes.append(name)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "writeTextGraph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "peekOfCode": "def writeTextGraph(modelPath, outputPath, outNodes):\n    try:\n        import cv2 as cv\n        cv.dnn.writeTextGraph(modelPath, outputPath)\n    except:\n        import tensorflow as tf\n        from tensorflow.tools.graph_transforms import TransformGraph\n        with tf.gfile.FastGFile(modelPath, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_common",
        "documentation": {}
    },
    {
        "label": "AnchorGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "peekOfCode": "class AnchorGenerator:\n    def __init__(self, min_level, aspect_ratios, num_scales, anchor_scale):\n        self.min_level = min_level\n        self.aspect_ratios = aspect_ratios\n        self.anchor_scale = anchor_scale\n        self.scales = [2**(float(s) / num_scales) for s in range(num_scales)]\n    def get(self, layer_id):\n        widths = []\n        heights = []\n        for s in self.scales:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "documentation": {}
    },
    {
        "label": "createGraph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "peekOfCode": "def createGraph(modelPath, outputPath, min_level, aspect_ratios, num_scales,\n                anchor_scale, num_classes, image_width, image_height):\n    print('Min level: %d' % min_level)\n    print('Anchor scale: %f' % anchor_scale)\n    print('Num scales: %d' % num_scales)\n    print('Aspect ratios: %s' % str(aspect_ratios))\n    print('Number of classes: %d' % num_classes)\n    print('Input image size: %dx%d' % (image_width, image_height))\n    # Read the graph.\n    _inpNames = ['image_arrays']",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_efficientdet",
        "documentation": {}
    },
    {
        "label": "createFasterRCNNGraph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_faster_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_faster_rcnn",
        "peekOfCode": "def createFasterRCNNGraph(modelPath, configPath, outputPath):\n    scopesToKeep = ('FirstStageFeatureExtractor', 'Conv',\n                    'FirstStageBoxPredictor/BoxEncodingPredictor',\n                    'FirstStageBoxPredictor/ClassPredictor',\n                    'CropAndResize',\n                    'MaxPool2D',\n                    'SecondStageFeatureExtractor',\n                    'SecondStageBoxPredictor',\n                    'Preprocessor/sub',\n                    'Preprocessor/mul',",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_faster_rcnn",
        "documentation": {}
    },
    {
        "label": "to_remove",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "def to_remove(name, op):\n    if name in nodesToKeep:\n        return False\n    return op == 'Const' or name.startswith(scopesToIgnore) or not name.startswith(scopesToKeep) or \\\n           (name.startswith('CropAndResize') and op != 'CropAndResize')\n# Fuse atrous convolutions (with dilations).\nnodesMap = {node.name: node for node in graph_def.node}\nfor node in reversed(graph_def.node):\n    if node.op == 'BatchToSpaceND':\n        del node.input[2]",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "getUnconnectedNodes",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "def getUnconnectedNodes():\n    unconnected = [node.name for node in graph_def.node]\n    for node in graph_def.node:\n        for inp in node.input:\n            if inp in unconnected:\n                unconnected.remove(inp)\n    return unconnected\nwhile True:\n    unconnectedNodes = getUnconnectedNodes()\n    unconnectedNodes.remove(graph_def.node[-1].name)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Run this script to get a text graph of '\n                                             'Mask-RCNN model from TensorFlow Object Detection API. '\n                                             'Then pass it with .pb file to cv::dnn::readNetFromTensorflow function.')\nparser.add_argument('--input', required=True, help='Path to frozen TensorFlow graph.')\nparser.add_argument('--output', required=True, help='Path to output text graph.')\nparser.add_argument('--config', required=True, help='Path to a *.config file is used for training.')\nargs = parser.parse_args()\nscopesToKeep = ('FirstStageFeatureExtractor', 'Conv',\n                'FirstStageBoxPredictor/BoxEncodingPredictor',\n                'FirstStageBoxPredictor/ClassPredictor',",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "args = parser.parse_args()\nscopesToKeep = ('FirstStageFeatureExtractor', 'Conv',\n                'FirstStageBoxPredictor/BoxEncodingPredictor',\n                'FirstStageBoxPredictor/ClassPredictor',\n                'CropAndResize',\n                'MaxPool2D',\n                'SecondStageFeatureExtractor',\n                'SecondStageBoxPredictor',\n                'Preprocessor/sub',\n                'Preprocessor/mul',",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "scopesToKeep",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "scopesToKeep = ('FirstStageFeatureExtractor', 'Conv',\n                'FirstStageBoxPredictor/BoxEncodingPredictor',\n                'FirstStageBoxPredictor/ClassPredictor',\n                'CropAndResize',\n                'MaxPool2D',\n                'SecondStageFeatureExtractor',\n                'SecondStageBoxPredictor',\n                'Preprocessor/sub',\n                'Preprocessor/mul',\n                'image_tensor')",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "scopesToIgnore",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "scopesToIgnore = ('FirstStageFeatureExtractor/Assert',\n                  'FirstStageFeatureExtractor/Shape',\n                  'FirstStageFeatureExtractor/strided_slice',\n                  'FirstStageFeatureExtractor/GreaterEqual',\n                  'FirstStageFeatureExtractor/LogicalAnd',\n                  'Conv/required_space_to_batch_paddings')\n# Load a config file.\nconfig = readTextMessage(args.config)\nconfig = config['model'][0]['faster_rcnn'][0]\nnum_classes = int(config['num_classes'][0])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "config = readTextMessage(args.config)\nconfig = config['model'][0]['faster_rcnn'][0]\nnum_classes = int(config['num_classes'][0])\ngrid_anchor_generator = config['first_stage_anchor_generator'][0]['grid_anchor_generator'][0]\nscales = [float(s) for s in grid_anchor_generator['scales']]\naspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "config = config['model'][0]['faster_rcnn'][0]\nnum_classes = int(config['num_classes'][0])\ngrid_anchor_generator = config['first_stage_anchor_generator'][0]['grid_anchor_generator'][0]\nscales = [float(s) for s in grid_anchor_generator['scales']]\naspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "num_classes = int(config['num_classes'][0])\ngrid_anchor_generator = config['first_stage_anchor_generator'][0]['grid_anchor_generator'][0]\nscales = [float(s) for s in grid_anchor_generator['scales']]\naspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "grid_anchor_generator",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "grid_anchor_generator = config['first_stage_anchor_generator'][0]['grid_anchor_generator'][0]\nscales = [float(s) for s in grid_anchor_generator['scales']]\naspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "scales",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "scales = [float(s) for s in grid_anchor_generator['scales']]\naspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "aspect_ratios",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "aspect_ratios = [float(ar) for ar in grid_anchor_generator['aspect_ratios']]\nwidth_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "width_stride",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "width_stride = float(grid_anchor_generator['width_stride'][0])\nheight_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)\nprint('Height stride:     %f' % height_stride)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "height_stride",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "height_stride = float(grid_anchor_generator['height_stride'][0])\nfeatures_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)\nprint('Height stride:     %f' % height_stride)\nprint('Features stride:   %f' % features_stride)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "features_stride",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "features_stride = float(config['feature_extractor'][0]['first_stage_features_stride'][0])\nfirst_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)\nprint('Height stride:     %f' % height_stride)\nprint('Features stride:   %f' % features_stride)\n# Read the graph.",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "first_stage_nms_iou_threshold",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "first_stage_nms_iou_threshold = float(config['first_stage_nms_iou_threshold'][0])\nfirst_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)\nprint('Height stride:     %f' % height_stride)\nprint('Features stride:   %f' % features_stride)\n# Read the graph.\nwriteTextGraph(args.input, args.output, ['num_detections', 'detection_scores', 'detection_boxes', 'detection_classes', 'detection_masks'])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "first_stage_max_proposals",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "first_stage_max_proposals = int(config['first_stage_max_proposals'][0])\nprint('Number of classes: %d' % num_classes)\nprint('Scales:            %s' % str(scales))\nprint('Aspect ratios:     %s' % str(aspect_ratios))\nprint('Width stride:      %f' % width_stride)\nprint('Height stride:     %f' % height_stride)\nprint('Features stride:   %f' % features_stride)\n# Read the graph.\nwriteTextGraph(args.input, args.output, ['num_detections', 'detection_scores', 'detection_boxes', 'detection_classes', 'detection_masks'])\ngraph_def = parseTextGraph(args.output)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "graph_def",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "graph_def = parseTextGraph(args.output)\nremoveIdentity(graph_def)\nnodesToKeep = []\ndef to_remove(name, op):\n    if name in nodesToKeep:\n        return False\n    return op == 'Const' or name.startswith(scopesToIgnore) or not name.startswith(scopesToKeep) or \\\n           (name.startswith('CropAndResize') and op != 'CropAndResize')\n# Fuse atrous convolutions (with dilations).\nnodesMap = {node.name: node for node in graph_def.node}",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "nodesToKeep",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "nodesToKeep = []\ndef to_remove(name, op):\n    if name in nodesToKeep:\n        return False\n    return op == 'Const' or name.startswith(scopesToIgnore) or not name.startswith(scopesToKeep) or \\\n           (name.startswith('CropAndResize') and op != 'CropAndResize')\n# Fuse atrous convolutions (with dilations).\nnodesMap = {node.name: node for node in graph_def.node}\nfor node in reversed(graph_def.node):\n    if node.op == 'BatchToSpaceND':",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "nodesMap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "nodesMap = {node.name: node for node in graph_def.node}\nfor node in reversed(graph_def.node):\n    if node.op == 'BatchToSpaceND':\n        del node.input[2]\n        conv = nodesMap[node.input[0]]\n        spaceToBatchND = nodesMap[conv.input[0]]\n        paddingsNode = NodeDef()\n        paddingsNode.name = conv.name + '/paddings'\n        paddingsNode.op = 'Const'\n        paddingsNode.addAttr('value', [2, 2, 2, 2])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "topNodes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "topNodes = []\nnumCropAndResize = 0\nwhile True:\n    node = graph_def.node.pop()\n    topNodes.append(node)\n    if node.op == 'CropAndResize':\n        numCropAndResize += 1\n        if numCropAndResize == 2:\n            break\naddReshape('FirstStageBoxPredictor/ClassPredictor/BiasAdd',",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "numCropAndResize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "numCropAndResize = 0\nwhile True:\n    node = graph_def.node.pop()\n    topNodes.append(node)\n    if node.op == 'CropAndResize':\n        numCropAndResize += 1\n        if numCropAndResize == 2:\n            break\naddReshape('FirstStageBoxPredictor/ClassPredictor/BiasAdd',\n           'FirstStageBoxPredictor/ClassPredictor/reshape_1', [0, -1, 2], graph_def)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "proposals",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "proposals = NodeDef()\nproposals.name = 'proposals'  # Compare with ClipToWindow/Gather/Gather (NOTE: normalized)\nproposals.op = 'PriorBox'\nproposals.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/BiasAdd')\nproposals.input.append(graph_def.node[0].name)  # image_tensor\nproposals.addAttr('flip', False)\nproposals.addAttr('clip', True)\nproposals.addAttr('step', features_stride)\nproposals.addAttr('offset', 0.0)\nproposals.addAttr('variance', [0.1, 0.1, 0.2, 0.2])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "proposals.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "proposals.name = 'proposals'  # Compare with ClipToWindow/Gather/Gather (NOTE: normalized)\nproposals.op = 'PriorBox'\nproposals.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/BiasAdd')\nproposals.input.append(graph_def.node[0].name)  # image_tensor\nproposals.addAttr('flip', False)\nproposals.addAttr('clip', True)\nproposals.addAttr('step', features_stride)\nproposals.addAttr('offset', 0.0)\nproposals.addAttr('variance', [0.1, 0.1, 0.2, 0.2])\nwidths = []",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "proposals.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "proposals.op = 'PriorBox'\nproposals.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/BiasAdd')\nproposals.input.append(graph_def.node[0].name)  # image_tensor\nproposals.addAttr('flip', False)\nproposals.addAttr('clip', True)\nproposals.addAttr('step', features_stride)\nproposals.addAttr('offset', 0.0)\nproposals.addAttr('variance', [0.1, 0.1, 0.2, 0.2])\nwidths = []\nheights = []",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "widths",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "widths = []\nheights = []\nfor a in aspect_ratios:\n    for s in scales:\n        ar = np.sqrt(a)\n        heights.append((height_stride**2) * s / ar)\n        widths.append((width_stride**2) * s * ar)\nproposals.addAttr('width', widths)\nproposals.addAttr('height', heights)\ngraph_def.node.extend([proposals])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "heights",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "heights = []\nfor a in aspect_ratios:\n    for s in scales:\n        ar = np.sqrt(a)\n        heights.append((height_stride**2) * s / ar)\n        widths.append((width_stride**2) * s * ar)\nproposals.addAttr('width', widths)\nproposals.addAttr('height', heights)\ngraph_def.node.extend([proposals])\n# Compare with Reshape_5",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut = NodeDef()\ndetectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/flatten')\ndetectionOut.input.append('FirstStageBoxPredictor/ClassPredictor/softmax/flatten')\ndetectionOut.input.append('proposals')\ndetectionOut.addAttr('num_classes', 2)\ndetectionOut.addAttr('share_location', True)\ndetectionOut.addAttr('background_label_id', 0)\ndetectionOut.addAttr('nms_threshold', first_stage_nms_iou_threshold)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut.name = 'detection_out'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/flatten')\ndetectionOut.input.append('FirstStageBoxPredictor/ClassPredictor/softmax/flatten')\ndetectionOut.input.append('proposals')\ndetectionOut.addAttr('num_classes', 2)\ndetectionOut.addAttr('share_location', True)\ndetectionOut.addAttr('background_label_id', 0)\ndetectionOut.addAttr('nms_threshold', first_stage_nms_iou_threshold)\ndetectionOut.addAttr('top_k', 6000)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('FirstStageBoxPredictor/BoxEncodingPredictor/flatten')\ndetectionOut.input.append('FirstStageBoxPredictor/ClassPredictor/softmax/flatten')\ndetectionOut.input.append('proposals')\ndetectionOut.addAttr('num_classes', 2)\ndetectionOut.addAttr('share_location', True)\ndetectionOut.addAttr('background_label_id', 0)\ndetectionOut.addAttr('nms_threshold', first_stage_nms_iou_threshold)\ndetectionOut.addAttr('top_k', 6000)\ndetectionOut.addAttr('code_type', \"CENTER_SIZE\")",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "cropAndResizeNodesNames",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "cropAndResizeNodesNames = []\nfor node in reversed(topNodes):\n    if node.op != 'CropAndResize':\n        graph_def.node.extend([node])\n        topNodes.pop()\n    else:\n        cropAndResizeNodesNames.append(node.name)\n        if numCropAndResize == 1:\n            break\n        else:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "variance",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "variance = NodeDef()\nvariance.name = 'proposals/variance'\nvariance.op = 'Const'\nvariance.addAttr('value', [0.1, 0.1, 0.2, 0.2])\ngraph_def.node.extend([variance])\nvarianceEncoder = NodeDef()\nvarianceEncoder.name = 'variance_encoded'\nvarianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "variance.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "variance.name = 'proposals/variance'\nvariance.op = 'Const'\nvariance.addAttr('value', [0.1, 0.1, 0.2, 0.2])\ngraph_def.node.extend([variance])\nvarianceEncoder = NodeDef()\nvarianceEncoder.name = 'variance_encoded'\nvarianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)\nvarianceEncoder.addAttr('axis', 2)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "variance.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "variance.op = 'Const'\nvariance.addAttr('value', [0.1, 0.1, 0.2, 0.2])\ngraph_def.node.extend([variance])\nvarianceEncoder = NodeDef()\nvarianceEncoder.name = 'variance_encoded'\nvarianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)\nvarianceEncoder.addAttr('axis', 2)\ngraph_def.node.extend([varianceEncoder])",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "varianceEncoder",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "varianceEncoder = NodeDef()\nvarianceEncoder.name = 'variance_encoded'\nvarianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)\nvarianceEncoder.addAttr('axis', 2)\ngraph_def.node.extend([varianceEncoder])\naddReshape('detection_out/slice', 'detection_out/slice/reshape', [1, 1, -1], graph_def)\naddFlatten('variance_encoded', 'variance_encoded/flatten', graph_def)\ndetectionOut = NodeDef()",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "varianceEncoder.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "varianceEncoder.name = 'variance_encoded'\nvarianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)\nvarianceEncoder.addAttr('axis', 2)\ngraph_def.node.extend([varianceEncoder])\naddReshape('detection_out/slice', 'detection_out/slice/reshape', [1, 1, -1], graph_def)\naddFlatten('variance_encoded', 'variance_encoded/flatten', graph_def)\ndetectionOut = NodeDef()\ndetectionOut.name = 'detection_out_final'",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "varianceEncoder.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "varianceEncoder.op = 'Mul'\nvarianceEncoder.input.append('SecondStageBoxPredictor/Reshape')\nvarianceEncoder.input.append(variance.name)\nvarianceEncoder.addAttr('axis', 2)\ngraph_def.node.extend([varianceEncoder])\naddReshape('detection_out/slice', 'detection_out/slice/reshape', [1, 1, -1], graph_def)\naddFlatten('variance_encoded', 'variance_encoded/flatten', graph_def)\ndetectionOut = NodeDef()\ndetectionOut.name = 'detection_out_final'\ndetectionOut.op = 'DetectionOutput'",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut = NodeDef()\ndetectionOut.name = 'detection_out_final'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('variance_encoded/flatten')\ndetectionOut.input.append('SecondStageBoxPredictor/Reshape_1/Reshape')\ndetectionOut.input.append('detection_out/slice/reshape')\ndetectionOut.addAttr('num_classes', num_classes)\ndetectionOut.addAttr('share_location', False)\ndetectionOut.addAttr('background_label_id', num_classes + 1)\ndetectionOut.addAttr('nms_threshold', 0.6)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut.name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut.name = 'detection_out_final'\ndetectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('variance_encoded/flatten')\ndetectionOut.input.append('SecondStageBoxPredictor/Reshape_1/Reshape')\ndetectionOut.input.append('detection_out/slice/reshape')\ndetectionOut.addAttr('num_classes', num_classes)\ndetectionOut.addAttr('share_location', False)\ndetectionOut.addAttr('background_label_id', num_classes + 1)\ndetectionOut.addAttr('nms_threshold', 0.6)\ndetectionOut.addAttr('code_type', \"CENTER_SIZE\")",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "detectionOut.op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "detectionOut.op = 'DetectionOutput'\ndetectionOut.input.append('variance_encoded/flatten')\ndetectionOut.input.append('SecondStageBoxPredictor/Reshape_1/Reshape')\ndetectionOut.input.append('detection_out/slice/reshape')\ndetectionOut.addAttr('num_classes', num_classes)\ndetectionOut.addAttr('share_location', False)\ndetectionOut.addAttr('background_label_id', num_classes + 1)\ndetectionOut.addAttr('nms_threshold', 0.6)\ndetectionOut.addAttr('code_type', \"CENTER_SIZE\")\ndetectionOut.addAttr('keep_top_k',100)",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "graph_def.node[-1].name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "graph_def.node[-1].name = 'detection_masks'\ngraph_def.node[-1].op = 'Sigmoid'\ngraph_def.node[-1].input.pop()\ndef getUnconnectedNodes():\n    unconnected = [node.name for node in graph_def.node]\n    for node in graph_def.node:\n        for inp in node.input:\n            if inp in unconnected:\n                unconnected.remove(inp)\n    return unconnected",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "graph_def.node[-1].op",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "peekOfCode": "graph_def.node[-1].op = 'Sigmoid'\ngraph_def.node[-1].input.pop()\ndef getUnconnectedNodes():\n    unconnected = [node.name for node in graph_def.node]\n    for node in graph_def.node:\n        for inp in node.input:\n            if inp in unconnected:\n                unconnected.remove(inp)\n    return unconnected\nwhile True:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_mask_rcnn",
        "documentation": {}
    },
    {
        "label": "SSDAnchorGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "peekOfCode": "class SSDAnchorGenerator:\n    def __init__(self, min_scale, max_scale, num_layers, aspect_ratios,\n                 reduce_boxes_in_lowest_layer, image_width, image_height):\n        self.min_scale = min_scale\n        self.aspect_ratios = aspect_ratios\n        self.reduce_boxes_in_lowest_layer = reduce_boxes_in_lowest_layer\n        self.image_width = image_width\n        self.image_height = image_height\n        self.scales =  [min_scale + (max_scale - min_scale) * i / (num_layers - 1)\n                            for i in range(num_layers)] + [1.0]",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "documentation": {}
    },
    {
        "label": "MultiscaleAnchorGenerator",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "peekOfCode": "class MultiscaleAnchorGenerator:\n    def __init__(self, min_level, aspect_ratios, scales_per_octave, anchor_scale):\n        self.min_level = min_level\n        self.aspect_ratios = aspect_ratios\n        self.anchor_scale = anchor_scale\n        self.scales = [2**(float(s) / scales_per_octave) for s in range(scales_per_octave)]\n    def get(self, layer_id):\n        widths = []\n        heights = []\n        for a in self.aspect_ratios:",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "documentation": {}
    },
    {
        "label": "createSSDGraph",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "description": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "peekOfCode": "def createSSDGraph(modelPath, configPath, outputPath):\n    # Nodes that should be kept.\n    keepOps = ['Conv2D', 'BiasAdd', 'Add', 'AddV2', 'Relu', 'Relu6', 'Placeholder', 'FusedBatchNorm',\n               'DepthwiseConv2dNative', 'ConcatV2', 'Mul', 'MaxPool', 'AvgPool', 'Identity',\n               'Sub', 'ResizeNearestNeighbor', 'Pad', 'FusedBatchNormV3', 'Mean']\n    # Node with which prefixes should be removed\n    prefixesToRemove = ('MultipleGridAnchorGenerator/', 'Concatenate/', 'Postprocessor/', 'Preprocessor/map')\n    # Load a config file.\n    config = readTextMessage(configPath)\n    config = config['model'][0]['ssd'][0]",
        "detail": "Hw_2.opencv.samples.dnn.tf_text_graph_ssd",
        "documentation": {}
    },
    {
        "label": "BilinearFilter",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "class BilinearFilter(object):\n    \"\"\"\n    PIL bilinear resize implementation\n    image = image.resize((image_width // 16, image_height // 16), Image.BILINEAR)\n    \"\"\"\n    def _precompute_coeffs(self, inSize, outSize):\n        filterscale = max(1.0, inSize / outSize)\n        ksize = int(np.ceil(filterscale)) * 2 + 1\n        kk = np.zeros(shape=(outSize * ksize, ), dtype=np.float32)\n        bounds = np.empty(shape=(outSize * 2, ), dtype=np.int32)",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "CpVton",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "class CpVton(object):\n    def __init__(self, gmm_model, tom_model, backend, target):\n        super(CpVton, self).__init__()\n        self.gmm_net = cv.dnn.readNet(gmm_model)\n        self.tom_net = cv.dnn.readNet(tom_model)\n        self.gmm_net.setPreferableBackend(backend)\n        self.gmm_net.setPreferableTarget(target)\n        self.tom_net.setPreferableBackend(backend)\n        self.tom_net.setPreferableTarget(target)\n    def prepare_agnostic(self, segm_image, input_image, pose_map, height=256, width=192):",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "CorrelationLayer",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "class CorrelationLayer(object):\n    def __init__(self, params, blobs):\n        super(CorrelationLayer, self).__init__()\n    def getMemoryShapes(self, inputs):\n        fetureAShape = inputs[0]\n        b, _, h, w = fetureAShape\n        return [[b, h * w, h, w]]\n    def forward(self, inputs):\n        feature_A, feature_B = inputs\n        b, c, h, w = feature_A.shape",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "get_pose_map",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "def get_pose_map(image, proto_path, model_path, backend, target, height=256, width=192):\n    radius = 5\n    inp = cv.dnn.blobFromImage(image, 1.0 / 255, (width, height))\n    net = cv.dnn.readNet(proto_path, model_path)\n    net.setPreferableBackend(backend)\n    net.setPreferableTarget(target)\n    net.setInput(inp)\n    out = net.forward()\n    threshold = 0.1\n    _, out_c, out_h, out_w = out.shape",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "backends",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "backends = (cv.dnn.DNN_BACKEND_DEFAULT, cv.dnn.DNN_BACKEND_HALIDE, cv.dnn.DNN_BACKEND_INFERENCE_ENGINE, cv.dnn.DNN_BACKEND_OPENCV,\n            cv.dnn.DNN_BACKEND_VKCOM, cv.dnn.DNN_BACKEND_CUDA)\ntargets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(description='Use this script to run virtial try-on using CP-VTON',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--input_image', '-i', required=True, help='Path to image with person.')\nparser.add_argument('--input_cloth', '-c', required=True, help='Path to target cloth image')\nparser.add_argument('--gmm_model', '-gmm', default='cp_vton_gmm.onnx', help='Path to Geometric Matching Module .onnx model.')\nparser.add_argument('--tom_model', '-tom', default='cp_vton_tom.onnx', help='Path to Try-On Module .onnx model.')",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "targets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "targets = (cv.dnn.DNN_TARGET_CPU, cv.dnn.DNN_TARGET_OPENCL, cv.dnn.DNN_TARGET_OPENCL_FP16, cv.dnn.DNN_TARGET_MYRIAD, cv.dnn.DNN_TARGET_HDDL,\n           cv.dnn.DNN_TARGET_VULKAN, cv.dnn.DNN_TARGET_CUDA, cv.dnn.DNN_TARGET_CUDA_FP16)\nparser = argparse.ArgumentParser(description='Use this script to run virtial try-on using CP-VTON',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--input_image', '-i', required=True, help='Path to image with person.')\nparser.add_argument('--input_cloth', '-c', required=True, help='Path to target cloth image')\nparser.add_argument('--gmm_model', '-gmm', default='cp_vton_gmm.onnx', help='Path to Geometric Matching Module .onnx model.')\nparser.add_argument('--tom_model', '-tom', default='cp_vton_tom.onnx', help='Path to Try-On Module .onnx model.')\nparser.add_argument('--segmentation_model', default='lip_jppnet_384.pb', help='Path to cloth segmentation .pb model.')\nparser.add_argument('--openpose_proto', default='openpose_pose_coco.prototxt', help='Path to OpenPose .prototxt model was trained on COCO dataset.')",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "description": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Use this script to run virtial try-on using CP-VTON',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--input_image', '-i', required=True, help='Path to image with person.')\nparser.add_argument('--input_cloth', '-c', required=True, help='Path to target cloth image')\nparser.add_argument('--gmm_model', '-gmm', default='cp_vton_gmm.onnx', help='Path to Geometric Matching Module .onnx model.')\nparser.add_argument('--tom_model', '-tom', default='cp_vton_tom.onnx', help='Path to Try-On Module .onnx model.')\nparser.add_argument('--segmentation_model', default='lip_jppnet_384.pb', help='Path to cloth segmentation .pb model.')\nparser.add_argument('--openpose_proto', default='openpose_pose_coco.prototxt', help='Path to OpenPose .prototxt model was trained on COCO dataset.')\nparser.add_argument('--openpose_model', default='openpose_pose_coco.caffemodel', help='Path to OpenPose .caffemodel model was trained on COCO dataset.')\nparser.add_argument('--backend', choices=backends, default=cv.dnn.DNN_BACKEND_DEFAULT, type=int,",
        "detail": "Hw_2.opencv.samples.dnn.virtual_try_on",
        "documentation": {}
    },
    {
        "label": "MagicValues",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class MagicValues(Enum):\n    MAGIC_VAL = 0x42FF0000\n    AUTO_STEP = 0\n    CONTINUOUS_FLAG = 1 << 14\n    SUBMATRIX_FLAG = 1 << 15\nclass MagicMasks(Enum):\n    MAGIC_MASK = 0xFFFF0000\n    TYPE_MASK = 0x00000FFF\n    DEPTH_MASK = 7\nclass Depth(Enum):",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "MagicMasks",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class MagicMasks(Enum):\n    MAGIC_MASK = 0xFFFF0000\n    TYPE_MASK = 0x00000FFF\n    DEPTH_MASK = 7\nclass Depth(Enum):\n    CV_8U = 0\n    CV_8S = 1\n    CV_16U = 2\n    CV_16S = 3\n    CV_32S = 4",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "Depth",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class Depth(Enum):\n    CV_8U = 0\n    CV_8S = 1\n    CV_16U = 2\n    CV_16S = 3\n    CV_32S = 4\n    CV_32F = 5\n    CV_64F = 6\n    CV_16F = 7\ndef create_enum(n):",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "Flags",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class Flags:\n    def depth(self):\n        return Depth(self.flags & MagicMasks.DEPTH_MASK.value)\n    def dtype(self):\n        depth = self.depth()\n        ret = None\n        if depth == Depth.CV_8U:\n            ret = (np.uint8, 'uint8_t')\n        elif depth == Depth.CV_8S:\n            ret = (np.int8, 'int8_t')",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "Size",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class Size:\n    def __init__(self, ptr):\n        self.ptr = ptr\n    def dims(self):\n        return int((self.ptr - 1).dereference())\n    def to_numpy(self):\n        return np.array([int(self.ptr[i]) for i in range(self.dims())], dtype=np.int64)\n    def __iter__(self):\n        return iter({'size': stri(self.to_numpy())}.items())\nclass Mat:",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "Mat",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class Mat:\n    def __init__(self, m, size, flags):\n        (dtype, ctype) = flags.dtype()\n        elsize = np.dtype(dtype).itemsize\n        shape = size.to_numpy()\n        steps = np.asarray([int(m['step']['p'][i]) for i in range(len(shape))], dtype=np.int64)\n        ptr = m['data']\n        # either we are default-constructed or sizes are zero\n        if int(ptr) == 0 or np.prod(shape * steps) == 0:\n            self.mat = np.array([])",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "MatPrinter",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "class MatPrinter:\n    \"\"\"Print a cv::Mat\"\"\"\n    def __init__(self, mat):\n        self.mat = mat\n    def views(self):\n        m = self.mat\n        flags = Flags(int(m['flags']))\n        size = Size(m['size']['p'])\n        data = Mat(m, size, flags)\n        for x in [flags, size, data]:",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "conv",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def conv(obj, t):\n    return gdb.parse_and_eval(f'({t})({obj})')\ndef booli(obj):\n    return conv(str(obj).lower(), 'bool')\ndef stri(obj):\n    s = f'\"{obj}\"'\n    return conv(s.translate(s.maketrans('\\n', ' ')), 'char*')\nclass MagicValues(Enum):\n    MAGIC_VAL = 0x42FF0000\n    AUTO_STEP = 0",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "booli",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def booli(obj):\n    return conv(str(obj).lower(), 'bool')\ndef stri(obj):\n    s = f'\"{obj}\"'\n    return conv(s.translate(s.maketrans('\\n', ' ')), 'char*')\nclass MagicValues(Enum):\n    MAGIC_VAL = 0x42FF0000\n    AUTO_STEP = 0\n    CONTINUOUS_FLAG = 1 << 14\n    SUBMATRIX_FLAG = 1 << 15",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "stri",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def stri(obj):\n    s = f'\"{obj}\"'\n    return conv(s.translate(s.maketrans('\\n', ' ')), 'char*')\nclass MagicValues(Enum):\n    MAGIC_VAL = 0x42FF0000\n    AUTO_STEP = 0\n    CONTINUOUS_FLAG = 1 << 14\n    SUBMATRIX_FLAG = 1 << 15\nclass MagicMasks(Enum):\n    MAGIC_MASK = 0xFFFF0000",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "create_enum",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def create_enum(n):\n    def make_type(depth, cn):\n        return depth.value + ((cn - 1) << 3)\n    defs = [(f'{depth.name}C{i}', make_type(depth, i)) for depth in Depth for i in range(1, n + 1)]\n    return Enum('Type', defs)\nType = create_enum(512)\nclass Flags:\n    def depth(self):\n        return Depth(self.flags & MagicMasks.DEPTH_MASK.value)\n    def dtype(self):",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "get_type",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def get_type(val):\n    # Get the type.\n    vtype = val.type\n    # If it points to a reference, get the reference.\n    if vtype.code == gdb.TYPE_CODE_REF:\n        vtype = vtype.target()\n    # Get the unqualified type, stripped of typedefs.\n    vtype = vtype.unqualified().strip_typedefs()\n    # Get the type name.\n    typename = vtype.tag",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "mat_printer",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "def mat_printer(val):\n    typename = get_type(val)\n    if typename is None:\n        return None\n    if str(typename) == 'cv::Mat':\n        return MatPrinter(val)\ngdb.pretty_printers.append(mat_printer)",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "Type",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "description": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "peekOfCode": "Type = create_enum(512)\nclass Flags:\n    def depth(self):\n        return Depth(self.flags & MagicMasks.DEPTH_MASK.value)\n    def dtype(self):\n        depth = self.depth()\n        ret = None\n        if depth == Depth.CV_8U:\n            ret = (np.uint8, 'uint8_t')\n        elif depth == Depth.CV_8S:",
        "detail": "Hw_2.opencv.samples.gdb.mat_pretty_printer",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "alpha = 0.5\ntry:\n    raw_input          # Python 2\nexcept NameError:\n    raw_input = input  # Python 3\nprint(''' Simple Linear Blender\n-----------------------\n* Enter alpha [0.0-1.0]: ''')\ninput_alpha = float(raw_input().strip())\nif 0 <= alpha <= 1:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "input_alpha",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "input_alpha = float(raw_input().strip())\nif 0 <= alpha <= 1:\n    alpha = input_alpha\n# [load]\nsrc1 = cv.imread(cv.samples.findFile('LinuxLogo.jpg'))\nsrc2 = cv.imread(cv.samples.findFile('WindowsLogo.jpg'))\n# [load]\nif src1 is None:\n    print(\"Error loading src1\")\n    exit(-1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "src1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "src1 = cv.imread(cv.samples.findFile('LinuxLogo.jpg'))\nsrc2 = cv.imread(cv.samples.findFile('WindowsLogo.jpg'))\n# [load]\nif src1 is None:\n    print(\"Error loading src1\")\n    exit(-1)\nelif src2 is None:\n    print(\"Error loading src2\")\n    exit(-1)\n# [blend_images]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "src2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "src2 = cv.imread(cv.samples.findFile('WindowsLogo.jpg'))\n# [load]\nif src1 is None:\n    print(\"Error loading src1\")\n    exit(-1)\nelif src2 is None:\n    print(\"Error loading src2\")\n    exit(-1)\n# [blend_images]\nbeta = (1.0 - alpha)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "beta",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "beta = (1.0 - alpha)\ndst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n# [blend_images]\n# [display]\ncv.imshow('dst', dst)\ncv.waitKey(0)\n# [display]\ncv.destroyAllWindows()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "peekOfCode": "dst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n# [blend_images]\n# [display]\ncv.imshow('dst', dst)\ncv.waitKey(0)\n# [display]\ncv.destroyAllWindows()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.AddingImages.adding_images",
        "documentation": {}
    },
    {
        "label": "print_help",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "peekOfCode": "def print_help():\n    print('''\n    This program demonstrated the use of the discrete Fourier transform (DFT).\n    The dft of an image is taken and it's power spectrum is displayed.\n    Usage:\n    discrete_fourier_transform.py [image_name -- default lena.jpg]''')\ndef main(argv):\n    print_help()\n    filename = argv[0] if len(argv) > 0 else 'lena.jpg'\n    I = cv.imread(cv.samples.findFile(filename), cv.IMREAD_GRAYSCALE)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "peekOfCode": "def main(argv):\n    print_help()\n    filename = argv[0] if len(argv) > 0 else 'lena.jpg'\n    I = cv.imread(cv.samples.findFile(filename), cv.IMREAD_GRAYSCALE)\n    if I is None:\n        print('Error opening image')\n        return -1\n    ## [expand]\n    rows, cols = I.shape\n    m = cv.getOptimalDFTSize( rows )",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.discrete_fourier_transform.discrete_fourier_transform",
        "documentation": {}
    },
    {
        "label": "MyData",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "peekOfCode": "class MyData:\n    A = 97\n    X = np.pi\n    name = 'mydata1234'\n    def __repr__(self):\n        s = '{ name = ' + self.name + ', X = ' + str(self.X)\n        s = s + ', A = ' +  str(self.A) + '}'\n        return s\n    ## [inside]\n    def write(self, fs, name):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "documentation": {}
    },
    {
        "label": "help",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "peekOfCode": "def help(filename):\n    print (\n        '''\n        {0} shows the usage of the OpenCV serialization functionality. \\n\\n\n        usage:\\n\n            python3 {0} outputfile.yml.gz\\n\\n\n        The output file may be either in XML, YAML or JSON. You can even compress it\\n\n        by specifying this in its extension like xml.gz yaml.gz etc... With\\n\n        FileStorage you can serialize objects in OpenCV.\\n\\n\n        For example: - create a class and have it serialized\\n",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "peekOfCode": "def main(argv):\n    if len(argv) != 2:\n        help(argv[0])\n        exit(1)\n    # write\n    ## [iomati]\n    R = np.eye(3,3)\n    T = np.zeros((3,1))\n    ## [iomati]\n    ## [customIOi]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.file_input_output.file_input_output",
        "documentation": {}
    },
    {
        "label": "is_grayscale",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "peekOfCode": "def is_grayscale(my_image):\n    return len(my_image.shape) < 3\ndef saturated(sum_value):\n    if sum_value > 255:\n        sum_value = 255\n    if sum_value < 0:\n        sum_value = 0\n    return sum_value\ndef sharpen(my_image):\n    if is_grayscale(my_image):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "documentation": {}
    },
    {
        "label": "saturated",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "peekOfCode": "def saturated(sum_value):\n    if sum_value > 255:\n        sum_value = 255\n    if sum_value < 0:\n        sum_value = 0\n    return sum_value\ndef sharpen(my_image):\n    if is_grayscale(my_image):\n        height, width = my_image.shape\n    else:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "documentation": {}
    },
    {
        "label": "sharpen",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "peekOfCode": "def sharpen(my_image):\n    if is_grayscale(my_image):\n        height, width = my_image.shape\n    else:\n        my_image = cv.cvtColor(my_image, cv.CV_8U)\n        height, width, n_channels = my_image.shape\n    result = np.zeros(my_image.shape, my_image.dtype)\n    ## [basic_method_loop]\n    for j in range(1, height - 1):\n        for i in range(1, width - 1):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "peekOfCode": "def main(argv):\n    filename = 'lena.jpg'\n    img_codec = cv.IMREAD_COLOR\n    if argv:\n        filename = sys.argv[1]\n        if len(argv) >= 2 and sys.argv[2] == \"G\":\n            img_codec = cv.IMREAD_GRAYSCALE\n    src = cv.imread(cv.samples.findFile(filename), img_codec)\n    if src is None:\n        print(\"Can't open image [\" + filename + \"]\")",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_mask_operations.mat_mask_operations",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "peekOfCode": "def load():\n    # Input/Output\n    filename = 'img.jpg'\n    ## [Load an image from a file]\n    img = cv.imread(filename)\n    ## [Load an image from a file]\n    ## [Load an image from a file in grayscale]\n    img = cv.imread(filename, cv.IMREAD_GRAYSCALE)\n    ## [Load an image from a file in grayscale]\n    ## [Save image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "documentation": {}
    },
    {
        "label": "access_pixel",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "peekOfCode": "def access_pixel():\n    # Accessing pixel intensity values\n    img = np.empty((4,4,3), np.uint8)\n    y = 0\n    x = 0\n    ## [Pixel access 1]\n    _intensity = img[y,x]\n    ## [Pixel access 1]\n    ## [Pixel access 3]\n    _blue = img[y,x,0]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "documentation": {}
    },
    {
        "label": "reference_counting",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "peekOfCode": "def reference_counting():\n    # Memory management and reference counting\n    ## [Reference counting 2]\n    img = cv.imread('image.jpg')\n    _img1 = np.copy(img)\n    ## [Reference counting 2]\n    ## [Reference counting 3]\n    img = cv.imread('image.jpg')\n    _sobelx = cv.Sobel(img, cv.CV_32F, 1, 0)\n    ## [Reference counting 3]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "documentation": {}
    },
    {
        "label": "primitive_operations",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "peekOfCode": "def primitive_operations():\n    img = np.empty((4,4,3), np.uint8)\n    ## [Set image to black]\n    img[:] = 0\n    ## [Set image to black]\n    ## [Select ROI]\n    _smallImg = img[10:110,10:110]\n    ## [Select ROI]\n    ## [BGR to Gray]\n    img = cv.imread('image.jpg')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "documentation": {}
    },
    {
        "label": "visualize_images",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "description": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "peekOfCode": "def visualize_images():\n    ## [imshow 1]\n    img = cv.imread('image.jpg')\n    cv.namedWindow('image', cv.WINDOW_AUTOSIZE)\n    cv.imshow('image', img)\n    cv.waitKey()\n    ## [imshow 1]\n    ## [imshow 2]\n    img = cv.imread('image.jpg')\n    grey = cv.cvtColor(img, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.core.mat_operations.mat_operations",
        "documentation": {}
    },
    {
        "label": "test_segm_models",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.common.test.voc_segm_test",
        "description": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.common.test.voc_segm_test",
        "peekOfCode": "def test_segm_models(models_list, data_fetcher, eval_params, experiment_name, is_print_eval_params=True,\n                     is_plot_acc=True):\n    if is_print_eval_params:\n        print(\n            \"===== Running evaluation of the classification models with the following params:\\n\"\n            \"\\t0. val data location: {}\\n\"\n            \"\\t1. val data labels: {}\\n\"\n            \"\\t2. frame size: {}\\n\"\n            \"\\t3. batch size: {}\\n\"\n            \"\\t4. transform to RGB: {}\\n\"",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.common.test.voc_segm_test",
        "documentation": {}
    },
    {
        "label": "PyTorchFcnResNet50",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "description": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "peekOfCode": "class PyTorchFcnResNet50(PyTorchModelPreparer):\n    def __init__(self, model_name, original_model):\n        super(PyTorchFcnResNet50, self).__init__(model_name, original_model)\ndef main():\n    parser = create_parser()\n    cmd_args = parser.parse_args()\n    set_pytorch_env()\n    # Test the base process of model retrieval\n    resnets = PyTorchFcnResNet50(\n        model_name=\"resnet50\",",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "description": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "peekOfCode": "def main():\n    parser = create_parser()\n    cmd_args = parser.parse_args()\n    set_pytorch_env()\n    # Test the base process of model retrieval\n    resnets = PyTorchFcnResNet50(\n        model_name=\"resnet50\",\n        original_model=models.segmentation.fcn_resnet50(pretrained=True)\n    )\n    model_dict = resnets.get_prepared_models()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.dnn.dnn_conversion.pytorch.segmentation.py_to_py_fcn_resnet50",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for AKAZE local features matching tutorial.')\nparser.add_argument('--input1', help='Path to input image 1.', default='graf1.png')\nparser.add_argument('--input2', help='Path to input image 2.', default='graf3.png')\nparser.add_argument('--homography', help='Path to the homography matrix.', default='H1to3p.xml')\nargs = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "args = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\nfs = cv.FileStorage(cv.samples.findFile(args.homography), cv.FILE_STORAGE_READ)\nhomography = fs.getFirstTopLevelNode().mat()\n## [load]\n## [AKAZE]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "img1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "img1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\nfs = cv.FileStorage(cv.samples.findFile(args.homography), cv.FILE_STORAGE_READ)\nhomography = fs.getFirstTopLevelNode().mat()\n## [load]\n## [AKAZE]\nakaze = cv.AKAZE_create()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "img2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "img2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\nfs = cv.FileStorage(cv.samples.findFile(args.homography), cv.FILE_STORAGE_READ)\nhomography = fs.getFirstTopLevelNode().mat()\n## [load]\n## [AKAZE]\nakaze = cv.AKAZE_create()\nkpts1, desc1 = akaze.detectAndCompute(img1, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "fs",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "fs = cv.FileStorage(cv.samples.findFile(args.homography), cv.FILE_STORAGE_READ)\nhomography = fs.getFirstTopLevelNode().mat()\n## [load]\n## [AKAZE]\nakaze = cv.AKAZE_create()\nkpts1, desc1 = akaze.detectAndCompute(img1, None)\nkpts2, desc2 = akaze.detectAndCompute(img2, None)\n## [AKAZE]\n## [2-nn matching]\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "homography",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "homography = fs.getFirstTopLevelNode().mat()\n## [load]\n## [AKAZE]\nakaze = cv.AKAZE_create()\nkpts1, desc1 = akaze.detectAndCompute(img1, None)\nkpts2, desc2 = akaze.detectAndCompute(img2, None)\n## [AKAZE]\n## [2-nn matching]\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)\nnn_matches = matcher.knnMatch(desc1, desc2, 2)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "akaze",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "akaze = cv.AKAZE_create()\nkpts1, desc1 = akaze.detectAndCompute(img1, None)\nkpts2, desc2 = akaze.detectAndCompute(img2, None)\n## [AKAZE]\n## [2-nn matching]\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)\nnn_matches = matcher.knnMatch(desc1, desc2, 2)\n## [2-nn matching]\n## [ratio test filtering]\nmatched1 = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "matcher",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)\nnn_matches = matcher.knnMatch(desc1, desc2, 2)\n## [2-nn matching]\n## [ratio test filtering]\nmatched1 = []\nmatched2 = []\nnn_match_ratio = 0.8 # Nearest neighbor matching ratio\nfor m, n in nn_matches:\n    if m.distance < nn_match_ratio * n.distance:\n        matched1.append(kpts1[m.queryIdx])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "nn_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "nn_matches = matcher.knnMatch(desc1, desc2, 2)\n## [2-nn matching]\n## [ratio test filtering]\nmatched1 = []\nmatched2 = []\nnn_match_ratio = 0.8 # Nearest neighbor matching ratio\nfor m, n in nn_matches:\n    if m.distance < nn_match_ratio * n.distance:\n        matched1.append(kpts1[m.queryIdx])\n        matched2.append(kpts2[m.trainIdx])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "matched1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "matched1 = []\nmatched2 = []\nnn_match_ratio = 0.8 # Nearest neighbor matching ratio\nfor m, n in nn_matches:\n    if m.distance < nn_match_ratio * n.distance:\n        matched1.append(kpts1[m.queryIdx])\n        matched2.append(kpts2[m.trainIdx])\n## [ratio test filtering]\n## [homography check]\ninliers1 = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "matched2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "matched2 = []\nnn_match_ratio = 0.8 # Nearest neighbor matching ratio\nfor m, n in nn_matches:\n    if m.distance < nn_match_ratio * n.distance:\n        matched1.append(kpts1[m.queryIdx])\n        matched2.append(kpts2[m.trainIdx])\n## [ratio test filtering]\n## [homography check]\ninliers1 = []\ninliers2 = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "nn_match_ratio",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "nn_match_ratio = 0.8 # Nearest neighbor matching ratio\nfor m, n in nn_matches:\n    if m.distance < nn_match_ratio * n.distance:\n        matched1.append(kpts1[m.queryIdx])\n        matched2.append(kpts2[m.trainIdx])\n## [ratio test filtering]\n## [homography check]\ninliers1 = []\ninliers2 = []\ngood_matches = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "inliers1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "inliers1 = []\ninliers2 = []\ngood_matches = []\ninlier_threshold = 2.5 # Distance threshold to identify inliers with homography check\nfor i, m in enumerate(matched1):\n    col = np.ones((3,1), dtype=np.float64)\n    col[0:2,0] = m.pt\n    col = np.dot(homography, col)\n    col /= col[2,0]\n    dist = sqrt(pow(col[0,0] - matched2[i].pt[0], 2) +\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "inliers2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "inliers2 = []\ngood_matches = []\ninlier_threshold = 2.5 # Distance threshold to identify inliers with homography check\nfor i, m in enumerate(matched1):\n    col = np.ones((3,1), dtype=np.float64)\n    col[0:2,0] = m.pt\n    col = np.dot(homography, col)\n    col /= col[2,0]\n    dist = sqrt(pow(col[0,0] - matched2[i].pt[0], 2) +\\\n                pow(col[1,0] - matched2[i].pt[1], 2))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "good_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "good_matches = []\ninlier_threshold = 2.5 # Distance threshold to identify inliers with homography check\nfor i, m in enumerate(matched1):\n    col = np.ones((3,1), dtype=np.float64)\n    col[0:2,0] = m.pt\n    col = np.dot(homography, col)\n    col /= col[2,0]\n    dist = sqrt(pow(col[0,0] - matched2[i].pt[0], 2) +\\\n                pow(col[1,0] - matched2[i].pt[1], 2))\n    if dist < inlier_threshold:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "inlier_threshold",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "inlier_threshold = 2.5 # Distance threshold to identify inliers with homography check\nfor i, m in enumerate(matched1):\n    col = np.ones((3,1), dtype=np.float64)\n    col[0:2,0] = m.pt\n    col = np.dot(homography, col)\n    col /= col[2,0]\n    dist = sqrt(pow(col[0,0] - matched2[i].pt[0], 2) +\\\n                pow(col[1,0] - matched2[i].pt[1], 2))\n    if dist < inlier_threshold:\n        good_matches.append(cv.DMatch(len(inliers1), len(inliers2), 0))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "res",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "res = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, inliers1, img2, inliers2, good_matches, res)\ncv.imwrite(\"akaze_result.png\", res)\ninlier_ratio = len(inliers1) / float(len(matched1))\nprint('A-KAZE Matching Results')\nprint('*******************************')\nprint('# Keypoints 1:                        \\t', len(kpts1))\nprint('# Keypoints 2:                        \\t', len(kpts2))\nprint('# Matches:                            \\t', len(matched1))\nprint('# Inliers:                            \\t', len(inliers1))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "inlier_ratio",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "peekOfCode": "inlier_ratio = len(inliers1) / float(len(matched1))\nprint('A-KAZE Matching Results')\nprint('*******************************')\nprint('# Keypoints 1:                        \\t', len(kpts1))\nprint('# Keypoints 2:                        \\t', len(kpts2))\nprint('# Matches:                            \\t', len(matched1))\nprint('# Inliers:                            \\t', len(inliers1))\nprint('# Inliers Ratio:                      \\t', inlier_ratio)\ncv.imshow('result', res)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.akaze_matching.AKAZE_match",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Feature Detection tutorial.')\nparser.add_argument('--input1', help='Path to input image 1.', default='box.png')\nparser.add_argument('--input2', help='Path to input image 2.', default='box_in_scene.png')\nargs = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "args = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "img1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "img2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a brute force matcher",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "minHessian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "minHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a brute force matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE)\nmatches = matcher.match(descriptors1, descriptors2)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a brute force matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE)\nmatches = matcher.match(descriptors1, descriptors2)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "matcher",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE)\nmatches = matcher.match(descriptors1, descriptors2)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)\n#-- Show detected matches\ncv.imshow('Matches', img_matches)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "matches = matcher.match(descriptors1, descriptors2)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)\n#-- Show detected matches\ncv.imshow('Matches', img_matches)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "peekOfCode": "img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)\n#-- Show detected matches\ncv.imshow('Matches', img_matches)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_description.SURF_matching_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Feature Detection tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='box.png')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_GRAYSCALE)\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_GRAYSCALE)\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints = detector.detect(src)\n#-- Draw keypoints",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_GRAYSCALE)\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints = detector.detect(src)\n#-- Draw keypoints\nimg_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "minHessian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "minHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints = detector.detect(src)\n#-- Draw keypoints\nimg_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\ncv.drawKeypoints(src, keypoints, img_keypoints)\n#-- Show detected (drawn) keypoints\ncv.imshow('SURF Keypoints', img_keypoints)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints = detector.detect(src)\n#-- Draw keypoints\nimg_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\ncv.drawKeypoints(src, keypoints, img_keypoints)\n#-- Show detected (drawn) keypoints\ncv.imshow('SURF Keypoints', img_keypoints)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "keypoints",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "keypoints = detector.detect(src)\n#-- Draw keypoints\nimg_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\ncv.drawKeypoints(src, keypoints, img_keypoints)\n#-- Show detected (drawn) keypoints\ncv.imshow('SURF Keypoints', img_keypoints)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "img_keypoints",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "peekOfCode": "img_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\ncv.drawKeypoints(src, keypoints, img_keypoints)\n#-- Show detected (drawn) keypoints\ncv.imshow('SURF Keypoints', img_keypoints)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_detection.SURF_detection_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Feature Matching with FLANN tutorial.')\nparser.add_argument('--input1', help='Path to input image 1.', default='box.png')\nparser.add_argument('--input2', help='Path to input image 2.', default='box_in_scene.png')\nargs = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "args = parser.parse_args()\nimg1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "img1 = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "img2 = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "minHessian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "minHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.7",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.7\ngood_matches = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "matcher",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.7\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "knn_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "knn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.7\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "ratio_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "ratio_thresh = 0.7\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Show detected matches\ncv.imshow('Good Matches', img_matches)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "good_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "good_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Show detected matches\ncv.imshow('Good Matches', img_matches)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "img_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "peekOfCode": "img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Show detected matches\ncv.imshow('Good Matches', img_matches)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_flann_matcher.SURF_FLANN_matching_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Feature Matching with FLANN tutorial.')\nparser.add_argument('--input1', help='Path to input image 1.', default='box.png')\nparser.add_argument('--input2', help='Path to input image 2.', default='box_in_scene.png')\nargs = parser.parse_args()\nimg_object = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg_scene = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img_object is None or img_scene is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "args = parser.parse_args()\nimg_object = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg_scene = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img_object is None or img_scene is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints_obj, descriptors_obj = detector.detectAndCompute(img_object, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "img_object",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "img_object = cv.imread(cv.samples.findFile(args.input1), cv.IMREAD_GRAYSCALE)\nimg_scene = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img_object is None or img_scene is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints_obj, descriptors_obj = detector.detectAndCompute(img_object, None)\nkeypoints_scene, descriptors_scene = detector.detectAndCompute(img_scene, None)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "img_scene",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "img_scene = cv.imread(cv.samples.findFile(args.input2), cv.IMREAD_GRAYSCALE)\nif img_object is None or img_scene is None:\n    print('Could not open or find the images!')\n    exit(0)\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints_obj, descriptors_obj = detector.detectAndCompute(img_object, None)\nkeypoints_scene, descriptors_scene = detector.detectAndCompute(img_scene, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "minHessian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "minHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints_obj, descriptors_obj = detector.detectAndCompute(img_object, None)\nkeypoints_scene, descriptors_scene = detector.detectAndCompute(img_scene, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors_obj, descriptors_scene, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.75",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints_obj, descriptors_obj = detector.detectAndCompute(img_object, None)\nkeypoints_scene, descriptors_scene = detector.detectAndCompute(img_scene, None)\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors_obj, descriptors_scene, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.75\ngood_matches = []",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "matcher",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors_obj, descriptors_scene, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.75\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img_object.shape[0], img_scene.shape[0]), img_object.shape[1]+img_scene.shape[1], 3), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "knn_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "knn_matches = matcher.knnMatch(descriptors_obj, descriptors_scene, 2)\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.75\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img_object.shape[0], img_scene.shape[0]), img_object.shape[1]+img_scene.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img_object, keypoints_obj, img_scene, keypoints_scene, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "ratio_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "ratio_thresh = 0.75\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img_object.shape[0], img_scene.shape[0]), img_object.shape[1]+img_scene.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img_object, keypoints_obj, img_scene, keypoints_scene, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Localize the object\nobj = np.empty((len(good_matches),2), dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "good_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "good_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n#-- Draw matches\nimg_matches = np.empty((max(img_object.shape[0], img_scene.shape[0]), img_object.shape[1]+img_scene.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img_object, keypoints_obj, img_scene, keypoints_scene, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Localize the object\nobj = np.empty((len(good_matches),2), dtype=np.float32)\nscene = np.empty((len(good_matches),2), dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "img_matches",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "img_matches = np.empty((max(img_object.shape[0], img_scene.shape[0]), img_object.shape[1]+img_scene.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img_object, keypoints_obj, img_scene, keypoints_scene, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n#-- Localize the object\nobj = np.empty((len(good_matches),2), dtype=np.float32)\nscene = np.empty((len(good_matches),2), dtype=np.float32)\nfor i in range(len(good_matches)):\n    #-- Get the keypoints from the good matches\n    obj[i,0] = keypoints_obj[good_matches[i].queryIdx].pt[0]\n    obj[i,1] = keypoints_obj[good_matches[i].queryIdx].pt[1]\n    scene[i,0] = keypoints_scene[good_matches[i].trainIdx].pt[0]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj = np.empty((len(good_matches),2), dtype=np.float32)\nscene = np.empty((len(good_matches),2), dtype=np.float32)\nfor i in range(len(good_matches)):\n    #-- Get the keypoints from the good matches\n    obj[i,0] = keypoints_obj[good_matches[i].queryIdx].pt[0]\n    obj[i,1] = keypoints_obj[good_matches[i].queryIdx].pt[1]\n    scene[i,0] = keypoints_scene[good_matches[i].trainIdx].pt[0]\n    scene[i,1] = keypoints_scene[good_matches[i].trainIdx].pt[1]\nH, _ =  cv.findHomography(obj, scene, cv.RANSAC)\n#-- Get the corners from the image_1 ( the object to be \"detected\" )",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "scene",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "scene = np.empty((len(good_matches),2), dtype=np.float32)\nfor i in range(len(good_matches)):\n    #-- Get the keypoints from the good matches\n    obj[i,0] = keypoints_obj[good_matches[i].queryIdx].pt[0]\n    obj[i,1] = keypoints_obj[good_matches[i].queryIdx].pt[1]\n    scene[i,0] = keypoints_scene[good_matches[i].trainIdx].pt[0]\n    scene[i,1] = keypoints_scene[good_matches[i].trainIdx].pt[1]\nH, _ =  cv.findHomography(obj, scene, cv.RANSAC)\n#-- Get the corners from the image_1 ( the object to be \"detected\" )\nobj_corners = np.empty((4,1,2), dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners = np.empty((4,1,2), dtype=np.float32)\nobj_corners[0,0,0] = 0\nobj_corners[0,0,1] = 0\nobj_corners[1,0,0] = img_object.shape[1]\nobj_corners[1,0,1] = 0\nobj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[0,0,0]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[0,0,0] = 0\nobj_corners[0,0,1] = 0\nobj_corners[1,0,0] = img_object.shape[1]\nobj_corners[1,0,1] = 0\nobj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[0,0,1]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[0,0,1] = 0\nobj_corners[1,0,0] = img_object.shape[1]\nobj_corners[1,0,1] = 0\nobj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[1,0,0]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[1,0,0] = img_object.shape[1]\nobj_corners[1,0,1] = 0\nobj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[1,0,1]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[1,0,1] = 0\nobj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[2,0,0]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[2,0,0] = img_object.shape[1]\nobj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\\n    (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])), (0,255,0), 4)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[2,0,1]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[2,0,1] = img_object.shape[0]\nobj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\\n    (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])),\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[3,0,0]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[3,0,0] = 0\nobj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\\n    (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])),\\\n    (int(scene_corners[3,0,0] + img_object.shape[1]), int(scene_corners[3,0,1])), (0,255,0), 4)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "obj_corners[3,0,1]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "obj_corners[3,0,1] = img_object.shape[0]\nscene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\\n    (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])),\\\n    (int(scene_corners[3,0,0] + img_object.shape[1]), int(scene_corners[3,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[3,0,0] + img_object.shape[1]), int(scene_corners[3,0,1])),\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "scene_corners",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "peekOfCode": "scene_corners = cv.perspectiveTransform(obj_corners, H)\n#-- Draw lines between the corners (the mapped object in the scene - image_2 )\ncv.line(img_matches, (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])),\\\n    (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[1,0,0] + img_object.shape[1]), int(scene_corners[1,0,1])),\\\n    (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[2,0,0] + img_object.shape[1]), int(scene_corners[2,0,1])),\\\n    (int(scene_corners[3,0,0] + img_object.shape[1]), int(scene_corners[3,0,1])), (0,255,0), 4)\ncv.line(img_matches, (int(scene_corners[3,0,0] + img_object.shape[1]), int(scene_corners[3,0,1])),\\\n    (int(scene_corners[0,0,0] + img_object.shape[1]), int(scene_corners[0,0,1])), (0,255,0), 4)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.feature_homography.SURF_FLANN_matching_homography_Demo",
        "documentation": {}
    },
    {
        "label": "basicPanoramaStitching",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "peekOfCode": "def basicPanoramaStitching(img1Path, img2Path):\n    img1 = cv.imread(cv.samples.findFile(img1Path))\n    img2 = cv.imread(cv.samples.findFile(img2Path))\n    # [camera-pose-from-Blender-at-location-1]\n    c1Mo = np.array([[0.9659258723258972, 0.2588190734386444, 0.0, 1.5529145002365112],\n                     [ 0.08852133899927139, -0.3303661346435547, -0.9396926164627075, -0.10281121730804443],\n                     [-0.24321036040782928, 0.9076734185218811, -0.342020183801651, 6.130080699920654],\n                     [0, 0, 0, 1]],dtype=np.float64)\n    # [camera-pose-from-Blender-at-location-1]\n    # [camera-pose-from-Blender-at-location-2]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "peekOfCode": "def main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Code for homography tutorial. Example 5: basic panorama stitching from a rotating camera.\")\n    parser.add_argument(\"-I1\",\"--image1\", help = \"path to first image\", default=\"Blender_Suzanne1.jpg\")\n    parser.add_argument(\"-I2\",\"--image2\", help = \"path to second image\", default=\"Blender_Suzanne2.jpg\")\n    args = parser.parse_args()\n    print(\"Panorama Stitching Started\")\n    basicPanoramaStitching(args.image1, args.image2)\n    print(\"Panorama Stitching Completed Successfully\")\nif __name__ == '__main__':",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.panorama_stitching_rotating_camera",
        "documentation": {}
    },
    {
        "label": "randomColor",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "peekOfCode": "def randomColor():\n    color = np.random.randint(0, 255,(1, 3))\n    return color[0].tolist()\ndef  perspectiveCorrection(img1Path, img2Path ,patternSize ):\n    img1 = cv.imread(cv.samples.findFile(img1Path))\n    img2 = cv.imread(cv.samples.findFile(img2Path))\n    # [find-corners]\n    ret1, corners1 = cv.findChessboardCorners(img1, patternSize)\n    ret2, corners2 = cv.findChessboardCorners(img2, patternSize)\n    # [find-corners]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "documentation": {}
    },
    {
        "label": {
            "name": "",
            "kind": 2,
            "peekOfCode": "def  perspectiveCorrection(img1Path, img2Path ,patternSize ):\n    img1 = cv.imread(cv.samples.findFile(img1Path))\n    img2 = cv.imread(cv.samples.findFile(img2Path))\n    # [find-corners]\n    ret1, corners1 = cv.findChessboardCorners(img1, patternSize)\n    ret2, corners2 = cv.findChessboardCorners(img2, patternSize)\n    # [find-corners]\n    if not ret1 or not ret2:\n        print(\"Error, cannot find the chessboard corners in both images.\")\n        sys.exit(-1)"
        },
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "peekOfCode": "def  perspectiveCorrection(img1Path, img2Path ,patternSize ):\n    img1 = cv.imread(cv.samples.findFile(img1Path))\n    img2 = cv.imread(cv.samples.findFile(img2Path))\n    # [find-corners]\n    ret1, corners1 = cv.findChessboardCorners(img1, patternSize)\n    ret2, corners2 = cv.findChessboardCorners(img2, patternSize)\n    # [find-corners]\n    if not ret1 or not ret2:\n        print(\"Error, cannot find the chessboard corners in both images.\")\n        sys.exit(-1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "description": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "peekOfCode": "def main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-I1', \"--image1\", help=\"Path to the first image\", default=\"left02.jpg\")\n    parser.add_argument('-I2', \"--image2\", help=\"Path to the second image\", default=\"left01.jpg\")\n    parser.add_argument('-H', \"--height\", help=\"Height of pattern size\", default=6)\n    parser.add_argument('-W', \"--width\", help=\"Width of pattern size\", default=9)\n    args = parser.parse_args()\n    img1Path = args.image1\n    img2Path = args.image2",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.features2D.Homography.perspective_correction",
        "documentation": {}
    },
    {
        "label": "on_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "def on_trackbar(val):\n    alpha = val / alpha_slider_max\n    beta = ( 1.0 - alpha )\n    dst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n    cv.imshow(title_window, dst)\n## [on_trackbar]\nparser = argparse.ArgumentParser(description='Code for Adding a Trackbar to our applications tutorial.')\nparser.add_argument('--input1', help='Path to the first input image.', default='LinuxLogo.jpg')\nparser.add_argument('--input2', help='Path to the second input image.', default='WindowsLogo.jpg')\nargs = parser.parse_args()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "alpha_slider_max",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "alpha_slider_max = 100\ntitle_window = 'Linear Blend'\n## [on_trackbar]\ndef on_trackbar(val):\n    alpha = val / alpha_slider_max\n    beta = ( 1.0 - alpha )\n    dst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n    cv.imshow(title_window, dst)\n## [on_trackbar]\nparser = argparse.ArgumentParser(description='Code for Adding a Trackbar to our applications tutorial.')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "title_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "title_window = 'Linear Blend'\n## [on_trackbar]\ndef on_trackbar(val):\n    alpha = val / alpha_slider_max\n    beta = ( 1.0 - alpha )\n    dst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n    cv.imshow(title_window, dst)\n## [on_trackbar]\nparser = argparse.ArgumentParser(description='Code for Adding a Trackbar to our applications tutorial.')\nparser.add_argument('--input1', help='Path to the first input image.', default='LinuxLogo.jpg')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Adding a Trackbar to our applications tutorial.')\nparser.add_argument('--input1', help='Path to the first input image.', default='LinuxLogo.jpg')\nparser.add_argument('--input2', help='Path to the second input image.', default='WindowsLogo.jpg')\nargs = parser.parse_args()\n## [load]\n# Read images ( both have to be of the same size and type )\nsrc1 = cv.imread(cv.samples.findFile(args.input1))\nsrc2 = cv.imread(cv.samples.findFile(args.input2))\n## [load]\nif src1 is None:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "args = parser.parse_args()\n## [load]\n# Read images ( both have to be of the same size and type )\nsrc1 = cv.imread(cv.samples.findFile(args.input1))\nsrc2 = cv.imread(cv.samples.findFile(args.input2))\n## [load]\nif src1 is None:\n    print('Could not open or find the image: ', args.input1)\n    exit(0)\nif src2 is None:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "src1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "src1 = cv.imread(cv.samples.findFile(args.input1))\nsrc2 = cv.imread(cv.samples.findFile(args.input2))\n## [load]\nif src1 is None:\n    print('Could not open or find the image: ', args.input1)\n    exit(0)\nif src2 is None:\n    print('Could not open or find the image: ', args.input2)\n    exit(0)\n## [window]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "src2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "src2 = cv.imread(cv.samples.findFile(args.input2))\n## [load]\nif src1 is None:\n    print('Could not open or find the image: ', args.input1)\n    exit(0)\nif src2 is None:\n    print('Could not open or find the image: ', args.input2)\n    exit(0)\n## [window]\ncv.namedWindow(title_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "trackbar_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "description": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "peekOfCode": "trackbar_name = 'Alpha x %d' % alpha_slider_max\ncv.createTrackbar(trackbar_name, title_window , 0, alpha_slider_max, on_trackbar)\n## [create_trackbar]\n# Show some stuff\non_trackbar(0)\n# Wait until user press some key\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.highgui.trackbar.AddingImagesTrackbar",
        "documentation": {}
    },
    {
        "label": "Hist_and_Backproj",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "def Hist_and_Backproj(val):\n    ## [initialize]\n    bins = val\n    histSize = max(bins, 2)\n    ranges = [0, 180] # hue_range\n    ## [initialize]\n    ## [Get the Histogram and normalize it]\n    hist = cv.calcHist([hue], [0], None, [histSize], ranges, accumulate=False)\n    cv.normalize(hist, hist, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\n    ## [Get the Histogram and normalize it]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Back Projection tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='home.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Read the image]\n## [Transform it to HSV]\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Read the image]\n## [Transform it to HSV]\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n## [Transform it to HSV]\n## [Use only the Hue value]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Read the image]\n## [Transform it to HSV]\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n## [Transform it to HSV]\n## [Use only the Hue value]\nch = (0, 0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "hsv",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "hsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n## [Transform it to HSV]\n## [Use only the Hue value]\nch = (0, 0)\nhue = np.empty(hsv.shape, hsv.dtype)\ncv.mixChannels([hsv], [hue], ch)\n## [Use only the Hue value]\n## [Create Trackbar to enter the number of bins]\nwindow_image = 'Source image'\ncv.namedWindow(window_image)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "ch",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "ch = (0, 0)\nhue = np.empty(hsv.shape, hsv.dtype)\ncv.mixChannels([hsv], [hue], ch)\n## [Use only the Hue value]\n## [Create Trackbar to enter the number of bins]\nwindow_image = 'Source image'\ncv.namedWindow(window_image)\nbins = 25\ncv.createTrackbar('* Hue  bins: ', window_image, bins, 180, Hist_and_Backproj )\nHist_and_Backproj(bins)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "hue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "hue = np.empty(hsv.shape, hsv.dtype)\ncv.mixChannels([hsv], [hue], ch)\n## [Use only the Hue value]\n## [Create Trackbar to enter the number of bins]\nwindow_image = 'Source image'\ncv.namedWindow(window_image)\nbins = 25\ncv.createTrackbar('* Hue  bins: ', window_image, bins, 180, Hist_and_Backproj )\nHist_and_Backproj(bins)\n## [Create Trackbar to enter the number of bins]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "window_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "window_image = 'Source image'\ncv.namedWindow(window_image)\nbins = 25\ncv.createTrackbar('* Hue  bins: ', window_image, bins, 180, Hist_and_Backproj )\nHist_and_Backproj(bins)\n## [Create Trackbar to enter the number of bins]\n## [Show the image]\ncv.imshow(window_image, src)\ncv.waitKey()\n## [Show the image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "bins",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "peekOfCode": "bins = 25\ncv.createTrackbar('* Hue  bins: ', window_image, bins, 180, Hist_and_Backproj )\nHist_and_Backproj(bins)\n## [Create Trackbar to enter the number of bins]\n## [Show the image]\ncv.imshow(window_image, src)\ncv.waitKey()\n## [Show the image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo1",
        "documentation": {}
    },
    {
        "label": "callback_low",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "def callback_low(val):\n    global low\n    low = val\ndef callback_up(val):\n    global up\n    up = val\ndef pickPoint(event, x, y, flags, param):\n    if event != cv.EVENT_LBUTTONDOWN:\n        return\n    # Fill and get the mask",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "callback_up",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "def callback_up(val):\n    global up\n    up = val\ndef pickPoint(event, x, y, flags, param):\n    if event != cv.EVENT_LBUTTONDOWN:\n        return\n    # Fill and get the mask\n    seed = (x, y)\n    newMaskVal = 255\n    newVal = (120, 120, 120)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "pickPoint",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "def pickPoint(event, x, y, flags, param):\n    if event != cv.EVENT_LBUTTONDOWN:\n        return\n    # Fill and get the mask\n    seed = (x, y)\n    newMaskVal = 255\n    newVal = (120, 120, 120)\n    connectivity = 8\n    flags = connectivity + (newMaskVal << 8 ) + cv.FLOODFILL_FIXED_RANGE + cv.FLOODFILL_MASK_ONLY\n    mask2 = np.zeros((src.shape[0] + 2, src.shape[1] + 2), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "Hist_and_Backproj",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "def Hist_and_Backproj(mask):\n    h_bins = 30\n    s_bins = 32\n    histSize = [h_bins, s_bins]\n    h_range = [0, 180]\n    s_range = [0, 256]\n    ranges = h_range + s_range # Concat list\n    channels = [0, 1]\n    # Get the Histogram and normalize it\n    hist = cv.calcHist([hsv], channels, mask, histSize, ranges, accumulate=False)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "low",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "low = 20\nup = 20\ndef callback_low(val):\n    global low\n    low = val\ndef callback_up(val):\n    global up\n    up = val\ndef pickPoint(event, x, y, flags, param):\n    if event != cv.EVENT_LBUTTONDOWN:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "up",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "up = 20\ndef callback_low(val):\n    global low\n    low = val\ndef callback_up(val):\n    global up\n    up = val\ndef pickPoint(event, x, y, flags, param):\n    if event != cv.EVENT_LBUTTONDOWN:\n        return",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Back Projection tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='home.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Transform it to HSV\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n# Show the image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Transform it to HSV\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n# Show the image\nwindow_image = 'Source image'\ncv.namedWindow(window_image)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Transform it to HSV\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n# Show the image\nwindow_image = 'Source image'\ncv.namedWindow(window_image)\ncv.imshow(window_image, src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "hsv",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "hsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\n# Show the image\nwindow_image = 'Source image'\ncv.namedWindow(window_image)\ncv.imshow(window_image, src)\n# Set Trackbars for floodfill thresholds\ncv.createTrackbar('Low thresh', window_image, low, 255, callback_low)\ncv.createTrackbar('High thresh', window_image, up, 255, callback_up)\n# Set a Mouse Callback\ncv.setMouseCallback(window_image, pickPoint)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "window_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "peekOfCode": "window_image = 'Source image'\ncv.namedWindow(window_image)\ncv.imshow(window_image, src)\n# Set Trackbars for floodfill thresholds\ncv.createTrackbar('Low thresh', window_image, low, 255, callback_low)\ncv.createTrackbar('High thresh', window_image, up, 255, callback_up)\n# Set a Mouse Callback\ncv.setMouseCallback(window_image, pickPoint)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.back_projection.calcBackProject_Demo2",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Histogram Calculation tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Separate the image in 3 places ( B, G and R )]\nbgr_planes = cv.split(src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Separate the image in 3 places ( B, G and R )]\nbgr_planes = cv.split(src)\n## [Separate the image in 3 places ( B, G and R )]\n## [Establish the number of bins]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Separate the image in 3 places ( B, G and R )]\nbgr_planes = cv.split(src)\n## [Separate the image in 3 places ( B, G and R )]\n## [Establish the number of bins]\nhistSize = 256",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "bgr_planes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "bgr_planes = cv.split(src)\n## [Separate the image in 3 places ( B, G and R )]\n## [Establish the number of bins]\nhistSize = 256\n## [Establish the number of bins]\n## [Set the ranges ( for B,G,R) )]\nhistRange = (0, 256) # the upper boundary is exclusive\n## [Set the ranges ( for B,G,R) )]\n## [Set histogram param]\naccumulate = False",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "histSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "histSize = 256\n## [Establish the number of bins]\n## [Set the ranges ( for B,G,R) )]\nhistRange = (0, 256) # the upper boundary is exclusive\n## [Set the ranges ( for B,G,R) )]\n## [Set histogram param]\naccumulate = False\n## [Set histogram param]\n## [Compute the histograms]\nb_hist = cv.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "histRange",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "histRange = (0, 256) # the upper boundary is exclusive\n## [Set the ranges ( for B,G,R) )]\n## [Set histogram param]\naccumulate = False\n## [Set histogram param]\n## [Compute the histograms]\nb_hist = cv.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)\ng_hist = cv.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\nr_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n## [Compute the histograms]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "accumulate",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "accumulate = False\n## [Set histogram param]\n## [Compute the histograms]\nb_hist = cv.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)\ng_hist = cv.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\nr_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n## [Compute the histograms]\n## [Draw the histograms for B, G and R]\nhist_w = 512\nhist_h = 400",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "b_hist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "b_hist = cv.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)\ng_hist = cv.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\nr_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n## [Compute the histograms]\n## [Draw the histograms for B, G and R]\nhist_w = 512\nhist_h = 400\nbin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "g_hist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "g_hist = cv.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\nr_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n## [Compute the histograms]\n## [Draw the histograms for B, G and R]\nhist_w = 512\nhist_h = 400\nbin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "r_hist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "r_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)\n## [Compute the histograms]\n## [Draw the histograms for B, G and R]\nhist_w = 512\nhist_h = 400\nbin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]\ncv.normalize(b_hist, b_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_w",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "hist_w = 512\nhist_h = 400\nbin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]\ncv.normalize(b_hist, b_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(g_hist, g_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(r_hist, r_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n## [Normalize the result to ( 0, histImage.rows )]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_h",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "hist_h = 400\nbin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]\ncv.normalize(b_hist, b_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(g_hist, g_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(r_hist, r_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n## [Normalize the result to ( 0, histImage.rows )]\n## [Draw for each channel]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "bin_w",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "bin_w = int(round( hist_w/histSize ))\nhistImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]\ncv.normalize(b_hist, b_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(g_hist, g_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(r_hist, r_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n## [Normalize the result to ( 0, histImage.rows )]\n## [Draw for each channel]\nfor i in range(1, histSize):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "histImage",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "peekOfCode": "histImage = np.zeros((hist_h, hist_w, 3), dtype=np.uint8)\n## [Draw the histograms for B, G and R]\n## [Normalize the result to ( 0, histImage.rows )]\ncv.normalize(b_hist, b_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(g_hist, g_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\ncv.normalize(r_hist, r_hist, alpha=0, beta=hist_h, norm_type=cv.NORM_MINMAX)\n## [Normalize the result to ( 0, histImage.rows )]\n## [Draw for each channel]\nfor i in range(1, histSize):\n    cv.line(histImage, ( bin_w*(i-1), hist_h - int(b_hist[i-1]) ),",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_calculation.calcHist_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Histogram Comparison tutorial.')\nparser.add_argument('--input1', help='Path to input image 1.')\nparser.add_argument('--input2', help='Path to input image 2.')\nparser.add_argument('--input3', help='Path to input image 3.')\nargs = parser.parse_args()\nsrc_base = cv.imread(args.input1)\nsrc_test1 = cv.imread(args.input2)\nsrc_test2 = cv.imread(args.input3)\nif src_base is None or src_test1 is None or src_test2 is None:\n    print('Could not open or find the images!')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc_base = cv.imread(args.input1)\nsrc_test1 = cv.imread(args.input2)\nsrc_test2 = cv.imread(args.input3)\nif src_base is None or src_test1 is None or src_test2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n## [Load three images with different environment settings]\n## [Convert to HSV]\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "src_base",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "src_base = cv.imread(args.input1)\nsrc_test1 = cv.imread(args.input2)\nsrc_test2 = cv.imread(args.input3)\nif src_base is None or src_test1 is None or src_test2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n## [Load three images with different environment settings]\n## [Convert to HSV]\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "src_test1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "src_test1 = cv.imread(args.input2)\nsrc_test2 = cv.imread(args.input3)\nif src_base is None or src_test1 is None or src_test2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n## [Load three images with different environment settings]\n## [Convert to HSV]\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\nhsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "src_test2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "src_test2 = cv.imread(args.input3)\nif src_base is None or src_test1 is None or src_test2 is None:\n    print('Could not open or find the images!')\n    exit(0)\n## [Load three images with different environment settings]\n## [Convert to HSV]\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\nhsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n## [Convert to HSV]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hsv_base",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\nhsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n## [Convert to HSV]\n## [Convert to HSV half]\nhsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n## [Convert to HSV half]\n## [Using 50 bins for hue and 60 for saturation]\nh_bins = 50\ns_bins = 60",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hsv_test1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\nhsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n## [Convert to HSV]\n## [Convert to HSV half]\nhsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n## [Convert to HSV half]\n## [Using 50 bins for hue and 60 for saturation]\nh_bins = 50\ns_bins = 60\nhistSize = [h_bins, s_bins]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hsv_test2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n## [Convert to HSV]\n## [Convert to HSV half]\nhsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n## [Convert to HSV half]\n## [Using 50 bins for hue and 60 for saturation]\nh_bins = 50\ns_bins = 60\nhistSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hsv_half_down",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n## [Convert to HSV half]\n## [Using 50 bins for hue and 60 for saturation]\nh_bins = 50\ns_bins = 60\nhistSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "h_bins",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "h_bins = 50\ns_bins = 60\nhistSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "s_bins",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "s_bins = 60\nhistSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "histSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "histSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "h_ranges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "h_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "s_ranges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "s_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "ranges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "ranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "channels",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "channels = [0, 1]\n## [Using 50 bins for hue and 60 for saturation]\n## [Calculate the histograms for the HSV images]\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_base",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n## [Calculate the histograms for the HSV images]\n## [Apply the histogram comparison methods]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_half_down",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n## [Calculate the histograms for the HSV images]\n## [Apply the histogram comparison methods]\nfor compare_method in range(4):\n    base_base = cv.compareHist(hist_base, hist_base, compare_method)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_test1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n## [Calculate the histograms for the HSV images]\n## [Apply the histogram comparison methods]\nfor compare_method in range(4):\n    base_base = cv.compareHist(hist_base, hist_base, compare_method)\n    base_half = cv.compareHist(hist_base, hist_half_down, compare_method)\n    base_test1 = cv.compareHist(hist_base, hist_test1, compare_method)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "hist_test2",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "peekOfCode": "hist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n## [Calculate the histograms for the HSV images]\n## [Apply the histogram comparison methods]\nfor compare_method in range(4):\n    base_base = cv.compareHist(hist_base, hist_base, compare_method)\n    base_half = cv.compareHist(hist_base, hist_half_down, compare_method)\n    base_test1 = cv.compareHist(hist_base, hist_test1, compare_method)\n    base_test2 = cv.compareHist(hist_base, hist_test2, compare_method)\n    print('Method:', compare_method, 'Perfect, Base-Half, Base-Test(1), Base-Test(2) :',\\",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_comparison.compareHist_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Histogram Equalization tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Convert to grayscale]\nsrc = cv.cvtColor(src, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Convert to grayscale]\nsrc = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [Convert to grayscale]\n## [Apply Histogram Equalization]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load image]\n## [Convert to grayscale]\nsrc = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [Convert to grayscale]\n## [Apply Histogram Equalization]\ndst = cv.equalizeHist(src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "peekOfCode": "src = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [Convert to grayscale]\n## [Apply Histogram Equalization]\ndst = cv.equalizeHist(src)\n## [Apply Histogram Equalization]\n## [Display results]\ncv.imshow('Source image', src)\ncv.imshow('Equalized Image', dst)\n## [Display results]\n## [Wait until user exits the program]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "documentation": {}
    },
    {
        "label": "dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "peekOfCode": "dst = cv.equalizeHist(src)\n## [Apply Histogram Equalization]\n## [Display results]\ncv.imshow('Source image', src)\ncv.imshow('Equalized Image', dst)\n## [Display results]\n## [Wait until user exits the program]\ncv.waitKey()\n## [Wait until user exits the program]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.Histograms_Matching.histogram_equalization.EqualizeHist_Demo",
        "documentation": {}
    },
    {
        "label": "calcGST",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "def calcGST(inputIMG, w):\n## [calcGST_proto]\n    img = inputIMG.astype(np.float32)\n    # GST components calculation (start)\n    # J =  (J11 J12; J12 J22) - GST\n    imgDiffX = cv.Sobel(img, cv.CV_32F, 1, 0, 3)\n    imgDiffY = cv.Sobel(img, cv.CV_32F, 0, 1, 3)\n    imgDiffXY = cv.multiply(imgDiffX, imgDiffY)\n    ## [calcJ_header]\n    imgDiffXX = cv.multiply(imgDiffX, imgDiffX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "W = 52          # window size is WxW\nC_Thr = 0.43    # threshold for coherency\nLowThr = 35     # threshold1 for orientation, it ranges from 0 to 180\nHighThr = 57    # threshold2 for orientation, it ranges from 0 to 180\n## [calcGST]\n## [calcJ_header]\n## [calcGST_proto]\ndef calcGST(inputIMG, w):\n## [calcGST_proto]\n    img = inputIMG.astype(np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "C_Thr",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "C_Thr = 0.43    # threshold for coherency\nLowThr = 35     # threshold1 for orientation, it ranges from 0 to 180\nHighThr = 57    # threshold2 for orientation, it ranges from 0 to 180\n## [calcGST]\n## [calcJ_header]\n## [calcGST_proto]\ndef calcGST(inputIMG, w):\n## [calcGST_proto]\n    img = inputIMG.astype(np.float32)\n    # GST components calculation (start)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "LowThr",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "LowThr = 35     # threshold1 for orientation, it ranges from 0 to 180\nHighThr = 57    # threshold2 for orientation, it ranges from 0 to 180\n## [calcGST]\n## [calcJ_header]\n## [calcGST_proto]\ndef calcGST(inputIMG, w):\n## [calcGST_proto]\n    img = inputIMG.astype(np.float32)\n    # GST components calculation (start)\n    # J =  (J11 J12; J12 J22) - GST",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "HighThr",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "HighThr = 57    # threshold2 for orientation, it ranges from 0 to 180\n## [calcGST]\n## [calcJ_header]\n## [calcGST_proto]\ndef calcGST(inputIMG, w):\n## [calcGST_proto]\n    img = inputIMG.astype(np.float32)\n    # GST components calculation (start)\n    # J =  (J11 J12; J12 J22) - GST\n    imgDiffX = cv.Sobel(img, cv.CV_32F, 1, 0, 3)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Anisotropic image segmentation tutorial.')\nparser.add_argument('-i', '--input', help='Path to input image.', required=True)\nargs = parser.parse_args()\nimgIn = cv.imread(args.input, cv.IMREAD_GRAYSCALE)\nif imgIn is None:\n    print('Could not open or find the image: {}'.format(args.input))\n    exit(0)\n## [main_extra]\n## [main]\nimgCoherency, imgOrientation = calcGST(imgIn, W)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "args = parser.parse_args()\nimgIn = cv.imread(args.input, cv.IMREAD_GRAYSCALE)\nif imgIn is None:\n    print('Could not open or find the image: {}'.format(args.input))\n    exit(0)\n## [main_extra]\n## [main]\nimgCoherency, imgOrientation = calcGST(imgIn, W)\n## [thresholding]\n_, imgCoherencyBin = cv.threshold(imgCoherency, C_Thr, 255, cv.THRESH_BINARY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "imgIn",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "imgIn = cv.imread(args.input, cv.IMREAD_GRAYSCALE)\nif imgIn is None:\n    print('Could not open or find the image: {}'.format(args.input))\n    exit(0)\n## [main_extra]\n## [main]\nimgCoherency, imgOrientation = calcGST(imgIn, W)\n## [thresholding]\n_, imgCoherencyBin = cv.threshold(imgCoherency, C_Thr, 255, cv.THRESH_BINARY)\n_, imgOrientationBin = cv.threshold(imgOrientation, LowThr, HighThr, cv.THRESH_BINARY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "imgBin",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "imgBin = cv.bitwise_and(imgCoherencyBin, imgOrientationBin)\n## [combining]\n## [main]\nimgCoherency = cv.normalize(imgCoherency, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\nimgOrientation = cv.normalize(imgOrientation, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\ncv.imshow('result.jpg', np.uint8(0.5*(imgIn + imgBin)))\ncv.imshow('Coherency.jpg', imgCoherency)\ncv.imshow('Orientation.jpg', imgOrientation)\ncv.waitKey(0)\n## [main_extra]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "imgCoherency",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "imgCoherency = cv.normalize(imgCoherency, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\nimgOrientation = cv.normalize(imgOrientation, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\ncv.imshow('result.jpg', np.uint8(0.5*(imgIn + imgBin)))\ncv.imshow('Coherency.jpg', imgCoherency)\ncv.imshow('Orientation.jpg', imgOrientation)\ncv.waitKey(0)\n## [main_extra]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "imgOrientation",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "peekOfCode": "imgOrientation = cv.normalize(imgOrientation, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\ncv.imshow('result.jpg', np.uint8(0.5*(imgIn + imgBin)))\ncv.imshow('Coherency.jpg', imgCoherency)\ncv.imshow('Orientation.jpg', imgOrientation)\ncv.waitKey(0)\n## [main_extra]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.anisotropic_image_segmentation.anisotropic_image_segmentation",
        "documentation": {}
    },
    {
        "label": "my_ellipse",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "def my_ellipse(img, angle):\n    thickness = 2\n    line_type = 8\n    cv.ellipse(img,\n                (W // 2, W // 2),\n                (W // 4, W // 16),\n                angle,\n                0,\n                360,\n                (255, 0, 0),",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "my_filled_circle",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "def my_filled_circle(img, center):\n    thickness = -1\n    line_type = 8\n    cv.circle(img,\n               center,\n               W // 32,\n               (0, 0, 255),\n               thickness,\n               line_type)\n## [my_filled_circle]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "my_polygon",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "def my_polygon(img):\n    line_type = 8\n    # Create some points\n    ppt = np.array([[W / 4, 7 * W / 8], [3 * W / 4, 7 * W / 8],\n                    [3 * W / 4, 13 * W / 16], [11 * W / 16, 13 * W / 16],\n                    [19 * W / 32, 3 * W / 8], [3 * W / 4, 3 * W / 8],\n                    [3 * W / 4, W / 8], [26 * W / 40, W / 8],\n                    [26 * W / 40, W / 4], [22 * W / 40, W / 4],\n                    [22 * W / 40, W / 8], [18 * W / 40, W / 8],\n                    [18 * W / 40, W / 4], [14 * W / 40, W / 4],",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "my_line",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "def my_line(img, start, end):\n    thickness = 2\n    line_type = 8\n    cv.line(img,\n             start,\n             end,\n             (0, 0, 0),\n             thickness,\n             line_type)\n## [my_line]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "W = 400\n## [my_ellipse]\ndef my_ellipse(img, angle):\n    thickness = 2\n    line_type = 8\n    cv.ellipse(img,\n                (W // 2, W // 2),\n                (W // 4, W // 16),\n                angle,\n                0,",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "atom_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "atom_window = \"Drawing 1: Atom\"\nrook_window = \"Drawing 2: Rook\"\n# Create black empty images\nsize = W, W, 3\natom_image = np.zeros(size, dtype=np.uint8)\nrook_image = np.zeros(size, dtype=np.uint8)\n## [create_images]\n## [draw_atom]\n# 1. Draw a simple atom:\n# -----------------------",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "rook_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "rook_window = \"Drawing 2: Rook\"\n# Create black empty images\nsize = W, W, 3\natom_image = np.zeros(size, dtype=np.uint8)\nrook_image = np.zeros(size, dtype=np.uint8)\n## [create_images]\n## [draw_atom]\n# 1. Draw a simple atom:\n# -----------------------\n# 1.a. Creating ellipses",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "size = W, W, 3\natom_image = np.zeros(size, dtype=np.uint8)\nrook_image = np.zeros(size, dtype=np.uint8)\n## [create_images]\n## [draw_atom]\n# 1. Draw a simple atom:\n# -----------------------\n# 1.a. Creating ellipses\nmy_ellipse(atom_image, 90)\nmy_ellipse(atom_image, 0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "atom_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "atom_image = np.zeros(size, dtype=np.uint8)\nrook_image = np.zeros(size, dtype=np.uint8)\n## [create_images]\n## [draw_atom]\n# 1. Draw a simple atom:\n# -----------------------\n# 1.a. Creating ellipses\nmy_ellipse(atom_image, 90)\nmy_ellipse(atom_image, 0)\nmy_ellipse(atom_image, 45)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "rook_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "peekOfCode": "rook_image = np.zeros(size, dtype=np.uint8)\n## [create_images]\n## [draw_atom]\n# 1. Draw a simple atom:\n# -----------------------\n# 1.a. Creating ellipses\nmy_ellipse(atom_image, 90)\nmy_ellipse(atom_image, 0)\nmy_ellipse(atom_image, 45)\nmy_ellipse(atom_image, -45)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.BasicGeometricDrawing.basic_geometric_drawing",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Changing the contrast and brightness of an image! tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nimage = cv.imread(cv.samples.findFile(args.input))\nif image is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [basic-linear-transform-load]\n## [basic-linear-transform-output]\nnew_image = np.zeros(image.shape, image.dtype)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "args = parser.parse_args()\nimage = cv.imread(cv.samples.findFile(args.input))\nif image is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [basic-linear-transform-load]\n## [basic-linear-transform-output]\nnew_image = np.zeros(image.shape, image.dtype)\n## [basic-linear-transform-output]\n## [basic-linear-transform-parameters]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "image = cv.imread(cv.samples.findFile(args.input))\nif image is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [basic-linear-transform-load]\n## [basic-linear-transform-output]\nnew_image = np.zeros(image.shape, image.dtype)\n## [basic-linear-transform-output]\n## [basic-linear-transform-parameters]\nalpha = 1.0 # Simple contrast control",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "new_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "new_image = np.zeros(image.shape, image.dtype)\n## [basic-linear-transform-output]\n## [basic-linear-transform-parameters]\nalpha = 1.0 # Simple contrast control\nbeta = 0    # Simple brightness control\n# Initialize values\nprint(' Basic Linear Transforms ')\nprint('-------------------------')\ntry:\n    alpha = float(input('* Enter the alpha value [1.0-3.0]: '))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "alpha = 1.0 # Simple contrast control\nbeta = 0    # Simple brightness control\n# Initialize values\nprint(' Basic Linear Transforms ')\nprint('-------------------------')\ntry:\n    alpha = float(input('* Enter the alpha value [1.0-3.0]: '))\n    beta = int(input('* Enter the beta value [0-100]: '))\nexcept ValueError:\n    print('Error, not a number')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "beta",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "peekOfCode": "beta = 0    # Simple brightness control\n# Initialize values\nprint(' Basic Linear Transforms ')\nprint('-------------------------')\ntry:\n    alpha = float(input('* Enter the alpha value [1.0-3.0]: '))\n    beta = int(input('* Enter the beta value [0-100]: '))\nexcept ValueError:\n    print('Error, not a number')\n## [basic-linear-transform-parameters]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.BasicLinearTransforms",
        "documentation": {}
    },
    {
        "label": "basicLinearTransform",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "def basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]\n    lookUpTable = np.empty((1,256), np.uint8)\n    for i in range(256):\n        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n    res = cv.LUT(img_original, lookUpTable)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "gammaCorrection",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "def gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]\n    lookUpTable = np.empty((1,256), np.uint8)\n    for i in range(256):\n        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n    res = cv.LUT(img_original, lookUpTable)\n    ## [changing-contrast-brightness-gamma-correction]\n    img_gamma_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Gamma correction\", img_gamma_corrected)\ndef on_linear_transform_alpha_trackbar(val):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "on_linear_transform_alpha_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "def on_linear_transform_alpha_trackbar(val):\n    global alpha\n    alpha = val / 100\n    basicLinearTransform()\ndef on_linear_transform_beta_trackbar(val):\n    global beta\n    beta = val - 100\n    basicLinearTransform()\ndef on_gamma_correction_trackbar(val):\n    global gamma",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "on_linear_transform_beta_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "def on_linear_transform_beta_trackbar(val):\n    global beta\n    beta = val - 100\n    basicLinearTransform()\ndef on_gamma_correction_trackbar(val):\n    global gamma\n    gamma = val / 100\n    gammaCorrection()\nparser = argparse.ArgumentParser(description='Code for Changing the contrast and brightness of an image! tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "on_gamma_correction_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "def on_gamma_correction_trackbar(val):\n    global gamma\n    gamma = val / 100\n    gammaCorrection()\nparser = argparse.ArgumentParser(description='Code for Changing the contrast and brightness of an image! tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nimg_original = cv.imread(cv.samples.findFile(args.input))\nif img_original is None:\n    print('Could not open or find the image: ', args.input)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "alpha",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "alpha = 1.0\nalpha_max = 500\nbeta = 0\nbeta_max = 200\ngamma = 1.0\ngamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "alpha_max",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "alpha_max = 500\nbeta = 0\nbeta_max = 200\ngamma = 1.0\ngamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "beta",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "beta = 0\nbeta_max = 200\ngamma = 1.0\ngamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "beta_max",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "beta_max = 200\ngamma = 1.0\ngamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]\n    lookUpTable = np.empty((1,256), np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "gamma",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "gamma = 1.0\ngamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]\n    lookUpTable = np.empty((1,256), np.uint8)\n    for i in range(256):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "gamma_max",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "gamma_max = 200\ndef basicLinearTransform():\n    res = cv.convertScaleAbs(img_original, alpha=alpha, beta=beta)\n    img_corrected = cv.hconcat([img_original, res])\n    cv.imshow(\"Brightness and contrast adjustments\", img_corrected)\ndef gammaCorrection():\n    ## [changing-contrast-brightness-gamma-correction]\n    lookUpTable = np.empty((1,256), np.uint8)\n    for i in range(256):\n        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Changing the contrast and brightness of an image! tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nimg_original = cv.imread(cv.samples.findFile(args.input))\nif img_original is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nimg_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_gamma_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_corrected = cv.hconcat([img_original, img_original])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "args = parser.parse_args()\nimg_original = cv.imread(cv.samples.findFile(args.input))\nif img_original is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nimg_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_gamma_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_corrected = cv.hconcat([img_original, img_original])\nimg_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "img_original",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "img_original = cv.imread(cv.samples.findFile(args.input))\nif img_original is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nimg_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_gamma_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_corrected = cv.hconcat([img_original, img_original])\nimg_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')\ncv.namedWindow('Gamma correction')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "img_corrected",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "img_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_gamma_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_corrected = cv.hconcat([img_original, img_original])\nimg_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')\ncv.namedWindow('Gamma correction')\nalpha_init = int(alpha *100)\ncv.createTrackbar('Alpha gain (contrast)', 'Brightness and contrast adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\nbeta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "img_gamma_corrected",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "img_gamma_corrected = np.empty((img_original.shape[0], img_original.shape[1]*2, img_original.shape[2]), img_original.dtype)\nimg_corrected = cv.hconcat([img_original, img_original])\nimg_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')\ncv.namedWindow('Gamma correction')\nalpha_init = int(alpha *100)\ncv.createTrackbar('Alpha gain (contrast)', 'Brightness and contrast adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\nbeta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\ngamma_init = int(gamma * 100)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "img_corrected",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "img_corrected = cv.hconcat([img_original, img_original])\nimg_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')\ncv.namedWindow('Gamma correction')\nalpha_init = int(alpha *100)\ncv.createTrackbar('Alpha gain (contrast)', 'Brightness and contrast adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\nbeta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\ngamma_init = int(gamma * 100)\ncv.createTrackbar('Gamma correction', 'Gamma correction', gamma_init, gamma_max, on_gamma_correction_trackbar)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "img_gamma_corrected",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "img_gamma_corrected = cv.hconcat([img_original, img_original])\ncv.namedWindow('Brightness and contrast adjustments')\ncv.namedWindow('Gamma correction')\nalpha_init = int(alpha *100)\ncv.createTrackbar('Alpha gain (contrast)', 'Brightness and contrast adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\nbeta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\ngamma_init = int(gamma * 100)\ncv.createTrackbar('Gamma correction', 'Gamma correction', gamma_init, gamma_max, on_gamma_correction_trackbar)\non_linear_transform_alpha_trackbar(alpha_init)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "alpha_init",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "alpha_init = int(alpha *100)\ncv.createTrackbar('Alpha gain (contrast)', 'Brightness and contrast adjustments', alpha_init, alpha_max, on_linear_transform_alpha_trackbar)\nbeta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\ngamma_init = int(gamma * 100)\ncv.createTrackbar('Gamma correction', 'Gamma correction', gamma_init, gamma_max, on_gamma_correction_trackbar)\non_linear_transform_alpha_trackbar(alpha_init)\non_gamma_correction_trackbar(gamma_init)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "beta_init",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "beta_init = beta + 100\ncv.createTrackbar('Beta bias (brightness)', 'Brightness and contrast adjustments', beta_init, beta_max, on_linear_transform_beta_trackbar)\ngamma_init = int(gamma * 100)\ncv.createTrackbar('Gamma correction', 'Gamma correction', gamma_init, gamma_max, on_gamma_correction_trackbar)\non_linear_transform_alpha_trackbar(alpha_init)\non_gamma_correction_trackbar(gamma_init)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "gamma_init",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "peekOfCode": "gamma_init = int(gamma * 100)\ncv.createTrackbar('Gamma correction', 'Gamma correction', gamma_init, gamma_max, on_gamma_correction_trackbar)\non_linear_transform_alpha_trackbar(alpha_init)\non_gamma_correction_trackbar(gamma_init)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.changing_contrast_brightness_image.changing_contrast_brightness_image",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "def main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:\n        print('Could not open or find the image: ', image)\n        exit(0)\n    cv.namedWindow(title_erosion_window)\n    cv.createTrackbar(title_trackbar_element_shape, title_erosion_window, 0, max_elem, erosion)\n    cv.createTrackbar(title_trackbar_kernel_size, title_erosion_window, 0, max_kernel_size, erosion)\n    cv.namedWindow(title_dilation_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "morph_shape",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "def morph_shape(val):\n    if val == 0:\n        return cv.MORPH_RECT\n    elif val == 1:\n        return cv.MORPH_CROSS\n    elif val == 2:\n        return cv.MORPH_ELLIPSE\n## [erosion]\ndef erosion(val):\n    erosion_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_erosion_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "erosion",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "def erosion(val):\n    erosion_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_erosion_window)\n    erosion_shape = morph_shape(cv.getTrackbarPos(title_trackbar_element_shape, title_erosion_window))\n    ## [kernel]\n    element = cv.getStructuringElement(erosion_shape, (2 * erosion_size + 1, 2 * erosion_size + 1),\n                                       (erosion_size, erosion_size))\n    ## [kernel]\n    erosion_dst = cv.erode(src, element)\n    cv.imshow(title_erosion_window, erosion_dst)\n## [erosion]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "dilatation",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "def dilatation(val):\n    dilatation_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_dilation_window)\n    dilation_shape = morph_shape(cv.getTrackbarPos(title_trackbar_element_shape, title_dilation_window))\n    element = cv.getStructuringElement(dilation_shape, (2 * dilatation_size + 1, 2 * dilatation_size + 1),\n                                       (dilatation_size, dilatation_size))\n    dilatation_dst = cv.dilate(src, element)\n    cv.imshow(title_dilation_window, dilatation_dst)\n## [dilation]\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Code for Eroding and Dilating tutorial.')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "src = None\nerosion_size = 0\nmax_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_element_shape = 'Element:\\n 0: Rect \\n 1: Cross \\n 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "erosion_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "erosion_size = 0\nmax_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_element_shape = 'Element:\\n 0: Rect \\n 1: Cross \\n 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "max_elem",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "max_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_element_shape = 'Element:\\n 0: Rect \\n 1: Cross \\n 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "max_kernel_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "max_kernel_size = 21\ntitle_trackbar_element_shape = 'Element:\\n 0: Rect \\n 1: Cross \\n 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "title_trackbar_element_shape",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "title_trackbar_element_shape = 'Element:\\n 0: Rect \\n 1: Cross \\n 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:\n        print('Could not open or find the image: ', image)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "title_trackbar_kernel_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "title_trackbar_kernel_size = 'Kernel size:\\n 2n +1'\ntitle_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:\n        print('Could not open or find the image: ', image)\n        exit(0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "title_erosion_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "title_erosion_window = 'Erosion Demo'\ntitle_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:\n        print('Could not open or find the image: ', image)\n        exit(0)\n    cv.namedWindow(title_erosion_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "title_dilation_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "peekOfCode": "title_dilation_window = 'Dilation Demo'\n## [main]\ndef main(image):\n    global src\n    src = cv.imread(cv.samples.findFile(image))\n    if src is None:\n        print('Could not open or find the image: ', image)\n        exit(0)\n    cv.namedWindow(title_erosion_window)\n    cv.createTrackbar(title_trackbar_element_shape, title_erosion_window, 0, max_elem, erosion)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.erosion_dilatation.morphology_1",
        "documentation": {}
    },
    {
        "label": "input_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "input_image = np.array((\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 255, 255, 255, 0, 0, 0, 255],\n    [0, 255, 255, 255, 0, 0, 0, 0],\n    [0, 255, 255, 255, 0, 255, 0, 0],\n    [0, 0, 255, 0, 0, 0, 0, 0],\n    [0, 0, 255, 0, 0, 255, 255, 0],\n    [0,255, 0, 255, 0, 0, 255, 0],\n    [0, 255, 255, 255, 0, 0, 0, 0]), dtype=\"uint8\")\nkernel = np.array((",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "kernel = np.array((\n        [0, 1, 0],\n        [1, -1, 1],\n        [0, 1, 0]), dtype=\"int\")\noutput_image = cv.morphologyEx(input_image, cv.MORPH_HITMISS, kernel)\nrate = 50\nkernel = (kernel + 1) * 127\nkernel = np.uint8(kernel)\nkernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "output_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "output_image = cv.morphologyEx(input_image, cv.MORPH_HITMISS, kernel)\nrate = 50\nkernel = (kernel + 1) * 127\nkernel = np.uint8(kernel)\nkernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)\ncv.moveWindow(\"kernel\", 0, 0)\ninput_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "rate",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "rate = 50\nkernel = (kernel + 1) * 127\nkernel = np.uint8(kernel)\nkernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)\ncv.moveWindow(\"kernel\", 0, 0)\ninput_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)\noutput_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "kernel = (kernel + 1) * 127\nkernel = np.uint8(kernel)\nkernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)\ncv.moveWindow(\"kernel\", 0, 0)\ninput_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)\noutput_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Hit or Miss\", output_image)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "kernel = np.uint8(kernel)\nkernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)\ncv.moveWindow(\"kernel\", 0, 0)\ninput_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)\noutput_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Hit or Miss\", output_image)\ncv.moveWindow(\"Hit or Miss\", 500, 200)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "kernel = cv.resize(kernel, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"kernel\", kernel)\ncv.moveWindow(\"kernel\", 0, 0)\ninput_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)\noutput_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Hit or Miss\", output_image)\ncv.moveWindow(\"Hit or Miss\", 500, 200)\ncv.waitKey(0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "input_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "input_image = cv.resize(input_image, None, fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Original\", input_image)\ncv.moveWindow(\"Original\", 0, 200)\noutput_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Hit or Miss\", output_image)\ncv.moveWindow(\"Hit or Miss\", 500, 200)\ncv.waitKey(0)\ncv.destroyAllWindows()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "output_image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "peekOfCode": "output_image = cv.resize(output_image, None , fx = rate, fy = rate, interpolation = cv.INTER_NEAREST)\ncv.imshow(\"Hit or Miss\", output_image)\ncv.moveWindow(\"Hit or Miss\", 500, 200)\ncv.waitKey(0)\ncv.destroyAllWindows()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.HitMiss.hit_miss",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "peekOfCode": "img = cv.imread(cv.samples.findFile('sudoku.png'))\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nedges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLines(edges,1,np.pi/180,200)\nfor line in lines:\n    rho,theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a*rho\n    y0 = b*rho",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "documentation": {}
    },
    {
        "label": "gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "peekOfCode": "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nedges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLines(edges,1,np.pi/180,200)\nfor line in lines:\n    rho,theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a*rho\n    y0 = b*rho\n    x1 = int(x0 + 1000*(-b))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "documentation": {}
    },
    {
        "label": "edges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "peekOfCode": "edges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLines(edges,1,np.pi/180,200)\nfor line in lines:\n    rho,theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a*rho\n    y0 = b*rho\n    x1 = int(x0 + 1000*(-b))\n    y1 = int(y0 + 1000*(a))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "documentation": {}
    },
    {
        "label": "lines",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "peekOfCode": "lines = cv.HoughLines(edges,1,np.pi/180,200)\nfor line in lines:\n    rho,theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a*rho\n    y0 = b*rho\n    x1 = int(x0 + 1000*(-b))\n    y1 = int(y0 + 1000*(a))\n    x2 = int(x0 - 1000*(-b))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.hough_line_transform",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "peekOfCode": "img = cv.imread(cv.samples.findFile('sudoku.png'))\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nedges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\nfor line in lines:\n    x1,y1,x2,y2 = line[0]\n    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\ncv.imwrite('houghlines5.jpg',img)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "documentation": {}
    },
    {
        "label": "gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "peekOfCode": "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nedges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\nfor line in lines:\n    x1,y1,x2,y2 = line[0]\n    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\ncv.imwrite('houghlines5.jpg',img)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "documentation": {}
    },
    {
        "label": "edges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "peekOfCode": "edges = cv.Canny(gray,50,150,apertureSize = 3)\nlines = cv.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\nfor line in lines:\n    x1,y1,x2,y2 = line[0]\n    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\ncv.imwrite('houghlines5.jpg',img)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "documentation": {}
    },
    {
        "label": "lines",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "peekOfCode": "lines = cv.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\nfor line in lines:\n    x1,y1,x2,y2 = line[0]\n    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\ncv.imwrite('houghlines5.jpg',img)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.hough_line_transform.probabilistic_hough_line_transform",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "def main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')\n        return -1\n    ## [load_image]\n    global img\n    global templ\n    img = cv.imread(sys.argv[1], cv.IMREAD_COLOR)\n    templ = cv.imread(sys.argv[2], cv.IMREAD_COLOR)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "MatchingMethod",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "def MatchingMethod(param):\n    global match_method\n    match_method = param\n    ## [copy_source]\n    img_display = img.copy()\n    ## [copy_source]\n    ## [match_template]\n    method_accepts_mask = (cv.TM_SQDIFF == match_method or match_method == cv.TM_CCORR_NORMED)\n    if (use_mask and method_accepts_mask):\n        result = cv.matchTemplate(img, templ, match_method, None, mask)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "use_mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "use_mask = False\nimg = None\ntempl = None\nmask = None\nimage_window = \"Source Image\"\nresult_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "img = None\ntempl = None\nmask = None\nimage_window = \"Source Image\"\nresult_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "templ",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "templ = None\nmask = None\nimage_window = \"Source Image\"\nresult_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "mask = None\nimage_window = \"Source Image\"\nresult_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "image_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "image_window = \"Source Image\"\nresult_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')\n        return -1",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "result_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "result_window = \"Result window\"\nmatch_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')\n        return -1\n    ## [load_image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "match_method",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "match_method = 0\nmax_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')\n        return -1\n    ## [load_image]\n    global img",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "max_Trackbar",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "peekOfCode": "max_Trackbar = 5\n## [global_variables]\ndef main(argv):\n    if (len(sys.argv) < 3):\n        print('Not enough parameters')\n        print('Usage:\\nmatch_template_demo.py <image_name> <template_name> [<mask_name>]')\n        return -1\n    ## [load_image]\n    global img\n    global templ",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.match_template.match_template",
        "documentation": {}
    },
    {
        "label": "show_wait_destroy",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "peekOfCode": "def show_wait_destroy(winname, img):\n    cv.imshow(winname, img)\n    cv.moveWindow(winname, 500, 0)\n    cv.waitKey(0)\n    cv.destroyWindow(winname)\ndef main(argv):\n    # [load_image]\n    # Check number of arguments\n    if len(argv) < 1:\n        print ('Not enough parameters')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "peekOfCode": "def main(argv):\n    # [load_image]\n    # Check number of arguments\n    if len(argv) < 1:\n        print ('Not enough parameters')\n        print ('Usage:\\nmorph_lines_detection.py < path_to_image >')\n        return -1\n    # Load the image\n    src = cv.imread(argv[0], cv.IMREAD_COLOR)\n    # Check if image is loaded fine",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.morph_lines_detection.morph_lines_detection",
        "documentation": {}
    },
    {
        "label": "morphology_operations",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "def morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)\n    if val_type == 0:\n        morph_elem = cv.MORPH_RECT\n    elif val_type == 1:\n        morph_elem = cv.MORPH_CROSS\n    elif val_type == 2:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "morph_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "morph_size = 0\nmax_operator = 4\nmax_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_operator_type = 'Operator:\\n 0: Opening - 1: Closing  \\n 2: Gradient - 3: Top Hat \\n 4: Black Hat'\ntitle_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "max_operator",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "max_operator = 4\nmax_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_operator_type = 'Operator:\\n 0: Opening - 1: Closing  \\n 2: Gradient - 3: Top Hat \\n 4: Black Hat'\ntitle_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "max_elem",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "max_elem = 2\nmax_kernel_size = 21\ntitle_trackbar_operator_type = 'Operator:\\n 0: Opening - 1: Closing  \\n 2: Gradient - 3: Top Hat \\n 4: Black Hat'\ntitle_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "max_kernel_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "max_kernel_size = 21\ntitle_trackbar_operator_type = 'Operator:\\n 0: Opening - 1: Closing  \\n 2: Gradient - 3: Top Hat \\n 4: Black Hat'\ntitle_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "title_trackbar_operator_type",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "title_trackbar_operator_type = 'Operator:\\n 0: Opening - 1: Closing  \\n 2: Gradient - 3: Top Hat \\n 4: Black Hat'\ntitle_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "title_trackbar_element_type",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "title_trackbar_element_type = 'Element:\\n 0: Rect - 1: Cross - 2: Ellipse'\ntitle_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)\n    if val_type == 0:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "title_trackbar_kernel_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "title_trackbar_kernel_size = 'Kernel size:\\n 2n + 1'\ntitle_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)\n    if val_type == 0:\n        morph_elem = cv.MORPH_RECT",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "title_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "title_window = 'Morphology Transformations Demo'\nmorph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)\n    if val_type == 0:\n        morph_elem = cv.MORPH_RECT\n    elif val_type == 1:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "morph_op_dic",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "morph_op_dic = {0: cv.MORPH_OPEN, 1: cv.MORPH_CLOSE, 2: cv.MORPH_GRADIENT, 3: cv.MORPH_TOPHAT, 4: cv.MORPH_BLACKHAT}\ndef morphology_operations(val):\n    morph_operator = cv.getTrackbarPos(title_trackbar_operator_type, title_window)\n    morph_size = cv.getTrackbarPos(title_trackbar_kernel_size, title_window)\n    morph_elem = 0\n    val_type = cv.getTrackbarPos(title_trackbar_element_type, title_window)\n    if val_type == 0:\n        morph_elem = cv.MORPH_RECT\n    elif val_type == 1:\n        morph_elem = cv.MORPH_CROSS",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for More Morphology Transformations tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='LinuxLogo.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.namedWindow(title_window)\ncv.createTrackbar(title_trackbar_operator_type, title_window , 0, max_operator, morphology_operations)\ncv.createTrackbar(title_trackbar_element_type, title_window , 0, max_elem, morphology_operations)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.namedWindow(title_window)\ncv.createTrackbar(title_trackbar_operator_type, title_window , 0, max_operator, morphology_operations)\ncv.createTrackbar(title_trackbar_element_type, title_window , 0, max_elem, morphology_operations)\ncv.createTrackbar(title_trackbar_kernel_size, title_window , 0, max_kernel_size, morphology_operations)\nmorphology_operations(0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.namedWindow(title_window)\ncv.createTrackbar(title_trackbar_operator_type, title_window , 0, max_operator, morphology_operations)\ncv.createTrackbar(title_trackbar_element_type, title_window , 0, max_elem, morphology_operations)\ncv.createTrackbar(title_trackbar_kernel_size, title_window , 0, max_kernel_size, morphology_operations)\nmorphology_operations(0)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.opening_closing_hats.morphology_2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Pyramids.pyramids",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Pyramids.pyramids",
        "peekOfCode": "def main(argv):\n    print(\"\"\"\n    Zoom In-Out demo\n    ------------------\n    * [i] -> Zoom [i]n\n    * [o] -> Zoom [o]ut\n    * [ESC] -> Close program\n    \"\"\")\n    ## [load]\n    filename = argv[0] if len(argv) > 0 else 'chicky_512.png'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Pyramids.pyramids",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "def main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src\n    src = cv.imread(cv.samples.findFile(imageName))\n    if src is None:\n        print ('Error opening image')\n        print ('Usage: smoothing.py [image_name -- default ../data/lena.jpg] \\n')\n        return -1",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "display_caption",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "def display_caption(caption):\n    global dst\n    dst = np.zeros(src.shape, src.dtype)\n    rows, cols, _ch = src.shape\n    cv.putText(dst, caption,\n                (int(cols / 4), int(rows / 2)),\n                cv.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255))\n    return display_dst(DELAY_CAPTION)\ndef display_dst(delay):\n    cv.imshow(window_name, dst)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "display_dst",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "def display_dst(delay):\n    cv.imshow(window_name, dst)\n    c = cv.waitKey(delay)\n    if c >= 0 : return -1\n    return 0\nif __name__ == \"__main__\":\n    main(sys.argv[1:])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "DELAY_CAPTION",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "DELAY_CAPTION = 1500\nDELAY_BLUR = 100\nMAX_KERNEL_LENGTH = 31\nsrc = None\ndst = None\nwindow_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "DELAY_BLUR",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "DELAY_BLUR = 100\nMAX_KERNEL_LENGTH = 31\nsrc = None\ndst = None\nwindow_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "MAX_KERNEL_LENGTH",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "MAX_KERNEL_LENGTH = 31\nsrc = None\ndst = None\nwindow_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src\n    src = cv.imread(cv.samples.findFile(imageName))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "src = None\ndst = None\nwindow_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src\n    src = cv.imread(cv.samples.findFile(imageName))\n    if src is None:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "dst = None\nwindow_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src\n    src = cv.imread(cv.samples.findFile(imageName))\n    if src is None:\n        print ('Error opening image')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "window_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "peekOfCode": "window_name = 'Smoothing Demo'\ndef main(argv):\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    # Load the source image\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    global src\n    src = cv.imread(cv.samples.findFile(imageName))\n    if src is None:\n        print ('Error opening image')\n        print ('Usage: smoothing.py [image_name -- default ../data/lena.jpg] \\n')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.Smoothing.smoothing",
        "documentation": {}
    },
    {
        "label": "Threshold_Demo",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "def Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated\n    #3: Threshold to Zero\n    #4: Threshold to Zero Inverted\n    threshold_type = cv.getTrackbarPos(trackbar_type, window_name)\n    threshold_value = cv.getTrackbarPos(trackbar_value, window_name)\n    _, dst = cv.threshold(src_gray, threshold_value, max_binary_value, threshold_type )\n    cv.imshow(window_name, dst)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "max_value",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "max_value = 255\nmax_type = 4\nmax_binary_value = 255\ntrackbar_type = 'Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted'\ntrackbar_value = 'Value'\nwindow_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "max_type",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "max_type = 4\nmax_binary_value = 255\ntrackbar_type = 'Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted'\ntrackbar_value = 'Value'\nwindow_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "max_binary_value",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "max_binary_value = 255\ntrackbar_type = 'Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted'\ntrackbar_value = 'Value'\nwindow_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated\n    #3: Threshold to Zero",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "trackbar_type",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "trackbar_type = 'Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted'\ntrackbar_value = 'Value'\nwindow_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated\n    #3: Threshold to Zero\n    #4: Threshold to Zero Inverted",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "trackbar_value",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "trackbar_value = 'Value'\nwindow_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated\n    #3: Threshold to Zero\n    #4: Threshold to Zero Inverted\n    threshold_type = cv.getTrackbarPos(trackbar_type, window_name)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "window_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "window_name = 'Threshold Demo'\n## [Threshold_Demo]\ndef Threshold_Demo(val):\n    #0: Binary\n    #1: Binary Inverted\n    #2: Threshold Truncated\n    #3: Threshold to Zero\n    #4: Threshold to Zero Inverted\n    threshold_type = cv.getTrackbarPos(trackbar_type, window_name)\n    threshold_value = cv.getTrackbarPos(trackbar_value, window_name)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Basic Thresholding Operations tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='stuff.jpg')\nargs = parser.parse_args()\n## [load]\n# Load an image\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n# Convert the image to Gray",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "args = parser.parse_args()\n## [load]\n# Load an image\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n# Convert the image to Gray\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [load]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n# Convert the image to Gray\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [load]\n## [window]\n# Create a window to display results\ncv.namedWindow(window_name)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n## [load]\n## [window]\n# Create a window to display results\ncv.namedWindow(window_name)\n## [window]\n## [trackbar]\n# Create Trackbar to choose type of Threshold\ncv.createTrackbar(trackbar_type, window_name , 3, max_type, Threshold_Demo)\n# Create Trackbar to choose Threshold value",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold.threshold",
        "documentation": {}
    },
    {
        "label": "on_low_H_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    low_H = min(high_H-1, low_H)\n    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)\n## [low]\n## [high]\ndef on_high_H_thresh_trackbar(val):\n    global low_H",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "on_high_H_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_high_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    high_H = val\n    high_H = max(high_H, low_H+1)\n    cv.setTrackbarPos(high_H_name, window_detection_name, high_H)\n## [high]\ndef on_low_S_thresh_trackbar(val):\n    global low_S\n    global high_S",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "on_low_S_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_low_S_thresh_trackbar(val):\n    global low_S\n    global high_S\n    low_S = val\n    low_S = min(high_S-1, low_S)\n    cv.setTrackbarPos(low_S_name, window_detection_name, low_S)\ndef on_high_S_thresh_trackbar(val):\n    global low_S\n    global high_S\n    high_S = val",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "on_high_S_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_high_S_thresh_trackbar(val):\n    global low_S\n    global high_S\n    high_S = val\n    high_S = max(high_S, low_S+1)\n    cv.setTrackbarPos(high_S_name, window_detection_name, high_S)\ndef on_low_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    low_V = val",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "on_low_V_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_low_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    low_V = val\n    low_V = min(high_V-1, low_V)\n    cv.setTrackbarPos(low_V_name, window_detection_name, low_V)\ndef on_high_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    high_V = val",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "on_high_V_thresh_trackbar",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "def on_high_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    high_V = val\n    high_V = max(high_V, low_V+1)\n    cv.setTrackbarPos(high_V_name, window_detection_name, high_V)\nparser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')\nparser.add_argument('--camera', help='Camera divide number.', default=0, type=int)\nargs = parser.parse_args()\n## [cap]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "max_value",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "max_value = 255\nmax_value_H = 360//2\nlow_H = 0\nlow_S = 0\nlow_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "max_value_H",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "max_value_H = 360//2\nlow_H = 0\nlow_S = 0\nlow_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_H",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_H = 0\nlow_S = 0\nlow_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_S",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_S = 0\nlow_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_V",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_H",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_S",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_V",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "window_capture_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "window_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "window_detection_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "window_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_H_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_S_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "low_V_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "low_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    low_H = min(high_H-1, low_H)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_H_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    low_H = min(high_H-1, low_H)\n    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_S_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_S_name = 'High S'\nhigh_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    low_H = min(high_H-1, low_H)\n    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)\n## [low]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "high_V_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "high_V_name = 'High V'\n## [low]\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    low_H = min(high_H-1, low_H)\n    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)\n## [low]\n## [high]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')\nparser.add_argument('--camera', help='Camera divide number.', default=0, type=int)\nargs = parser.parse_args()\n## [cap]\ncap = cv.VideoCapture(args.camera)\n## [cap]\n## [window]\ncv.namedWindow(window_capture_name)\ncv.namedWindow(window_detection_name)\n## [window]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "args = parser.parse_args()\n## [cap]\ncap = cv.VideoCapture(args.camera)\n## [cap]\n## [window]\ncv.namedWindow(window_capture_name)\ncv.namedWindow(window_detection_name)\n## [window]\n## [trackbar]\ncv.createTrackbar(low_H_name, window_detection_name , low_H, max_value_H, on_low_H_thresh_trackbar)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "description": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "peekOfCode": "cap = cv.VideoCapture(args.camera)\n## [cap]\n## [window]\ncv.namedWindow(window_capture_name)\ncv.namedWindow(window_detection_name)\n## [window]\n## [trackbar]\ncv.createTrackbar(low_H_name, window_detection_name , low_H, max_value_H, on_low_H_thresh_trackbar)\ncv.createTrackbar(high_H_name, window_detection_name , high_H, max_value_H, on_high_H_thresh_trackbar)\ncv.createTrackbar(low_S_name, window_detection_name , low_S, max_value, on_low_S_thresh_trackbar)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.imgProc.threshold_inRange.threshold_inRange",
        "documentation": {}
    },
    {
        "label": "CannyThreshold",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "def CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))\n    cv.imshow(window_name, dst)\nparser = argparse.ArgumentParser(description='Code for Canny Edge Detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='fruits.jpg')\nargs = parser.parse_args()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "max_lowThreshold",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "max_lowThreshold = 100\nwindow_name = 'Edge Map'\ntitle_trackbar = 'Min Threshold:'\nratio = 3\nkernel_size = 3\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "window_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "window_name = 'Edge Map'\ntitle_trackbar = 'Min Threshold:'\nratio = 3\nkernel_size = 3\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "title_trackbar",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "title_trackbar = 'Min Threshold:'\nratio = 3\nkernel_size = 3\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))\n    cv.imshow(window_name, dst)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "ratio",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "ratio = 3\nkernel_size = 3\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))\n    cv.imshow(window_name, dst)\nparser = argparse.ArgumentParser(description='Code for Canny Edge Detector tutorial.')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "kernel_size",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "kernel_size = 3\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))\n    cv.imshow(window_name, dst)\nparser = argparse.ArgumentParser(description='Code for Canny Edge Detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='fruits.jpg')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Canny Edge Detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='fruits.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\ncv.namedWindow(window_name)\ncv.createTrackbar(title_trackbar, window_name , 0, max_lowThreshold, CannyThreshold)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\ncv.namedWindow(window_name)\ncv.createTrackbar(title_trackbar, window_name , 0, max_lowThreshold, CannyThreshold)\nCannyThreshold(0)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\ncv.namedWindow(window_name)\ncv.createTrackbar(title_trackbar, window_name , 0, max_lowThreshold, CannyThreshold)\nCannyThreshold(0)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\ncv.namedWindow(window_name)\ncv.createTrackbar(title_trackbar, window_name , 0, max_lowThreshold, CannyThreshold)\nCannyThreshold(0)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.canny_detector.CannyDetector_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Image Segmentation with Distance Transform and Watershed Algorithm.\\\n    Sample code showing how to segment overlapping objects using Laplacian filtering, \\\n    in addition to Watershed and Distance Transformation')\nparser.add_argument('--input', help='Path to input image.', default='cards.png')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Show source image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Show source image\ncv.imshow('Source Image', src)\n## [load_image]\n## [black_bg]\n# Change the background from white to black, since that will help later to extract",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Show source image\ncv.imshow('Source Image', src)\n## [load_image]\n## [black_bg]\n# Change the background from white to black, since that will help later to extract\n# better results during the use of Distance Transform",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "kernel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)\n# do the laplacian filtering as it is\n# well, we need to convert everything in something more deeper then CV_8U\n# because the kernel has some negative values,\n# and we can expect in general to have a Laplacian image with negative values\n# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255\n# so the possible negative number will be truncated\nimgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)\nsharp = np.float32(src)\nimgResult = sharp - imgLaplacian",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgLaplacian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)\nsharp = np.float32(src)\nimgResult = sharp - imgLaplacian\n# convert back to 8bits gray scale\nimgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "sharp",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "sharp = np.float32(src)\nimgResult = sharp - imgLaplacian\n# convert back to 8bits gray scale\nimgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgResult",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgResult = sharp - imgLaplacian\n# convert back to 8bits gray scale\nimgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]\n## [bin]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgResult",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]\n## [bin]\n# Create binary image from source image\nbw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgResult",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]\n## [bin]\n# Create binary image from source image\nbw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)\n_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgLaplacian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]\n## [bin]\n# Create binary image from source image\nbw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)\n_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\ncv.imshow('Binary Image', bw)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "imgLaplacian",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "imgLaplacian = np.uint8(imgLaplacian)\n#cv.imshow('Laplace Filtered Image', imgLaplacian)\ncv.imshow('New Sharped Image', imgResult)\n## [sharp]\n## [bin]\n# Create binary image from source image\nbw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)\n_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\ncv.imshow('Binary Image', bw)\n## [bin]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "bw",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)\n_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\ncv.imshow('Binary Image', bw)\n## [bin]\n## [dist]\n# Perform the distance transform algorithm\ndist = cv.distanceTransform(bw, cv.DIST_L2, 3)\n# Normalize the distance image for range = {0.0, 1.0}\n# so we can visualize and threshold it\ncv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "dist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "dist = cv.distanceTransform(bw, cv.DIST_L2, 3)\n# Normalize the distance image for range = {0.0, 1.0}\n# so we can visualize and threshold it\ncv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)\ncv.imshow('Distance Transform Image', dist)\n## [dist]\n## [peaks]\n# Threshold to obtain the peaks\n# This will be the markers for the foreground objects\n_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "kernel1",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "kernel1 = np.ones((3,3), dtype=np.uint8)\ndist = cv.dilate(dist, kernel1)\ncv.imshow('Peaks', dist)\n## [peaks]\n## [seeds]\n# Create the CV_8U version of the distance image\n# It is needed for findContours()\ndist_8u = dist.astype('uint8')\n# Find total markers\ncontours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "dist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "dist = cv.dilate(dist, kernel1)\ncv.imshow('Peaks', dist)\n## [peaks]\n## [seeds]\n# Create the CV_8U version of the distance image\n# It is needed for findContours()\ndist_8u = dist.astype('uint8')\n# Find total markers\ncontours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n# Create the marker image for the watershed algorithm",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "dist_8u",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "dist_8u = dist.astype('uint8')\n# Find total markers\ncontours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n# Create the marker image for the watershed algorithm\nmarkers = np.zeros(dist.shape, dtype=np.int32)\n# Draw the foreground markers\nfor i in range(len(contours)):\n    cv.drawContours(markers, contours, i, (i+1), -1)\n# Draw the background marker\ncv.circle(markers, (5,5), 3, (255,255,255), -1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "markers",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "markers = np.zeros(dist.shape, dtype=np.int32)\n# Draw the foreground markers\nfor i in range(len(contours)):\n    cv.drawContours(markers, contours, i, (i+1), -1)\n# Draw the background marker\ncv.circle(markers, (5,5), 3, (255,255,255), -1)\nmarkers_8u = (markers * 10).astype('uint8')\ncv.imshow('Markers', markers_8u)\n## [seeds]\n## [watershed]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "markers_8u",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "markers_8u = (markers * 10).astype('uint8')\ncv.imshow('Markers', markers_8u)\n## [seeds]\n## [watershed]\n# Perform the watershed algorithm\ncv.watershed(imgResult, markers)\n#mark = np.zeros(markers.shape, dtype=np.uint8)\nmark = markers.astype('uint8')\nmark = cv.bitwise_not(mark)\n# uncomment this if you want to see how the mark",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "#mark",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "#mark = np.zeros(markers.shape, dtype=np.uint8)\nmark = markers.astype('uint8')\nmark = cv.bitwise_not(mark)\n# uncomment this if you want to see how the mark\n# image looks like at that point\n#cv.imshow('Markers_v2', mark)\n# Generate random colors\ncolors = []\nfor contour in contours:\n    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "mark",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "mark = markers.astype('uint8')\nmark = cv.bitwise_not(mark)\n# uncomment this if you want to see how the mark\n# image looks like at that point\n#cv.imshow('Markers_v2', mark)\n# Generate random colors\ncolors = []\nfor contour in contours:\n    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))\n# Create the result image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "mark",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "mark = cv.bitwise_not(mark)\n# uncomment this if you want to see how the mark\n# image looks like at that point\n#cv.imshow('Markers_v2', mark)\n# Generate random colors\ncolors = []\nfor contour in contours:\n    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))\n# Create the result image\ndst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "colors = []\nfor contour in contours:\n    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))\n# Create the result image\ndst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\n# Fill labeled objects with random colors\nfor i in range(markers.shape[0]):\n    for j in range(markers.shape[1]):\n        index = markers[i,j]\n        if index > 0 and index <= len(contours):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "peekOfCode": "dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\n# Fill labeled objects with random colors\nfor i in range(markers.shape[0]):\n    for j in range(markers.shape[1]):\n        index = markers[i,j]\n        if index > 0 and index <= len(contours):\n            dst[i,j,:] = colors[index-1]\n# Visualize the final image\ncv.imshow('Final Result', dst)\n## [watershed]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.distance_transformation.imageSegmentation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.Filter2D.filter2D",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.Filter2D.filter2D",
        "peekOfCode": "def main(argv):\n    window_name = 'filter2D Demo'\n    ## [load]\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    # Loads an image\n    src = cv.imread(cv.samples.findFile(imageName), cv.IMREAD_COLOR)\n    # Check if image is loaded fine\n    if src is None:\n        print ('Error opening image!')\n        print ('Usage: filter2D.py [image_name -- default lena.jpg] \\n')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.Filter2D.filter2D",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughCircle.hough_circle",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughCircle.hough_circle",
        "peekOfCode": "def main(argv):\n    ## [load]\n    default_file = 'smarties.png'\n    filename = argv[0] if len(argv) > 0 else default_file\n    # Loads an image\n    src = cv.imread(cv.samples.findFile(filename), cv.IMREAD_COLOR)\n    # Check if image is loaded fine\n    if src is None:\n        print ('Error opening image!')\n        print ('Usage: hough_circle.py [image_name -- default ' + default_file + '] \\n')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughCircle.hough_circle",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughLine.hough_lines",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughLine.hough_lines",
        "peekOfCode": "def main(argv):\n    ## [load]\n    default_file = 'sudoku.png'\n    filename = argv[0] if len(argv) > 0 else default_file\n    # Loads an image\n    src = cv.imread(cv.samples.findFile(filename), cv.IMREAD_GRAYSCALE)\n    # Check if image is loaded fine\n    if src is None:\n        print ('Error opening image!')\n        print ('Usage: hough_lines.py [image_name -- default ' + default_file + '] \\n')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.HoughLine.hough_lines",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.LaPlace.laplace_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.LaPlace.laplace_demo",
        "peekOfCode": "def main(argv):\n    # [variables]\n    # Declare the variables we are going to use\n    ddepth = cv.CV_16S\n    kernel_size = 3\n    window_name = \"Laplace Demo\"\n    # [variables]\n    # [load]\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    src = cv.imread(cv.samples.findFile(imageName), cv.IMREAD_COLOR) # Load an image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.LaPlace.laplace_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.MakeBorder.copy_make_border",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.MakeBorder.copy_make_border",
        "peekOfCode": "def main(argv):\n    ## [variables]\n    # First we declare the variables we are going to use\n    borderType = cv.BORDER_CONSTANT\n    window_name = \"copyMakeBorder Demo\"\n    ## [variables]\n    ## [load]\n    imageName = argv[0] if len(argv) > 0 else 'lena.jpg'\n    # Loads an image\n    src = cv.imread(cv.samples.findFile(imageName), cv.IMREAD_COLOR)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.MakeBorder.copy_make_border",
        "documentation": {}
    },
    {
        "label": "update_map",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "def update_map(ind, map_x, map_y):\n    if ind == 0:\n        for i in range(map_x.shape[0]):\n            for j in range(map_x.shape[1]):\n                if j > map_x.shape[1]*0.25 and j < map_x.shape[1]*0.75 and i > map_x.shape[0]*0.25 and i < map_x.shape[0]*0.75:\n                    map_x[i,j] = 2 * (j-map_x.shape[1]*0.25) + 0.5\n                    map_y[i,j] = 2 * (i-map_y.shape[0]*0.25) + 0.5\n                else:\n                    map_x[i,j] = 0\n                    map_y[i,j] = 0",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Remapping tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='chicky_512.png')\nargs = parser.parse_args()\n## [Load]\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_COLOR)\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [Load]\n## [Create]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "args = parser.parse_args()\n## [Load]\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_COLOR)\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [Load]\n## [Create]\nmap_x = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nmap_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_COLOR)\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n## [Load]\n## [Create]\nmap_x = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nmap_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\n## [Create]\n## [Window]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "map_x",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "map_x = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nmap_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\n## [Create]\n## [Window]\nwindow_name = 'Remap demo'\ncv.namedWindow(window_name)\n## [Window]\n## [Loop]\nind = 0\nwhile True:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "map_y",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "map_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\n## [Create]\n## [Window]\nwindow_name = 'Remap demo'\ncv.namedWindow(window_name)\n## [Window]\n## [Loop]\nind = 0\nwhile True:\n    update_map(ind, map_x, map_y)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "window_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "window_name = 'Remap demo'\ncv.namedWindow(window_name)\n## [Window]\n## [Loop]\nind = 0\nwhile True:\n    update_map(ind, map_x, map_y)\n    ind = (ind + 1) % 4\n    dst = cv.remap(src, map_x, map_y, cv.INTER_LINEAR)\n    cv.imshow(window_name, dst)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "ind",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "peekOfCode": "ind = 0\nwhile True:\n    update_map(ind, map_x, map_y)\n    ind = (ind + 1) % 4\n    dst = cv.remap(src, map_x, map_y, cv.INTER_LINEAR)\n    cv.imshow(window_name, dst)\n    c = cv.waitKey(1000)\n    if c == 27:\n        break\n## [Loop]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.remap.Remap_Demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.SobelDemo.sobel_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.SobelDemo.sobel_demo",
        "peekOfCode": "def main(argv):\n    ## [variables]\n    # First we declare the variables we are going to use\n    window_name = ('Sobel Demo - Simple Edge Detector')\n    scale = 1\n    delta = 0\n    ddepth = cv.CV_16S\n    ## [variables]\n    ## [load]\n    # As usual we load our source image (src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.SobelDemo.sobel_demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Affine Transformations tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load the image]\n## [Set your 3 points to calculate the  Affine Transform]\nsrcTri = np.array( [[0, 0], [src.shape[1] - 1, 0], [0, src.shape[0] - 1]] ).astype(np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load the image]\n## [Set your 3 points to calculate the  Affine Transform]\nsrcTri = np.array( [[0, 0], [src.shape[1] - 1, 0], [0, src.shape[0] - 1]] ).astype(np.float32)\ndstTri = np.array( [[0, src.shape[1]*0.33], [src.shape[1]*0.85, src.shape[0]*0.25], [src.shape[1]*0.15, src.shape[0]*0.7]] ).astype(np.float32)\n## [Set your 3 points to calculate the  Affine Transform]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n## [Load the image]\n## [Set your 3 points to calculate the  Affine Transform]\nsrcTri = np.array( [[0, 0], [src.shape[1] - 1, 0], [0, src.shape[0] - 1]] ).astype(np.float32)\ndstTri = np.array( [[0, src.shape[1]*0.33], [src.shape[1]*0.85, src.shape[0]*0.25], [src.shape[1]*0.15, src.shape[0]*0.7]] ).astype(np.float32)\n## [Set your 3 points to calculate the  Affine Transform]\n## [Get the Affine Transform]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "srcTri",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "srcTri = np.array( [[0, 0], [src.shape[1] - 1, 0], [0, src.shape[0] - 1]] ).astype(np.float32)\ndstTri = np.array( [[0, src.shape[1]*0.33], [src.shape[1]*0.85, src.shape[0]*0.25], [src.shape[1]*0.15, src.shape[0]*0.7]] ).astype(np.float32)\n## [Set your 3 points to calculate the  Affine Transform]\n## [Get the Affine Transform]\nwarp_mat = cv.getAffineTransform(srcTri, dstTri)\n## [Get the Affine Transform]\n## [Apply the Affine Transform just found to the src image]\nwarp_dst = cv.warpAffine(src, warp_mat, (src.shape[1], src.shape[0]))\n## [Apply the Affine Transform just found to the src image]\n# Rotating the image after Warp",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "dstTri",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "dstTri = np.array( [[0, src.shape[1]*0.33], [src.shape[1]*0.85, src.shape[0]*0.25], [src.shape[1]*0.15, src.shape[0]*0.7]] ).astype(np.float32)\n## [Set your 3 points to calculate the  Affine Transform]\n## [Get the Affine Transform]\nwarp_mat = cv.getAffineTransform(srcTri, dstTri)\n## [Get the Affine Transform]\n## [Apply the Affine Transform just found to the src image]\nwarp_dst = cv.warpAffine(src, warp_mat, (src.shape[1], src.shape[0]))\n## [Apply the Affine Transform just found to the src image]\n# Rotating the image after Warp\n## [Compute a rotation matrix with respect to the center of the image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "warp_mat",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "warp_mat = cv.getAffineTransform(srcTri, dstTri)\n## [Get the Affine Transform]\n## [Apply the Affine Transform just found to the src image]\nwarp_dst = cv.warpAffine(src, warp_mat, (src.shape[1], src.shape[0]))\n## [Apply the Affine Transform just found to the src image]\n# Rotating the image after Warp\n## [Compute a rotation matrix with respect to the center of the image]\ncenter = (warp_dst.shape[1]//2, warp_dst.shape[0]//2)\nangle = -50\nscale = 0.6",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "warp_dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "warp_dst = cv.warpAffine(src, warp_mat, (src.shape[1], src.shape[0]))\n## [Apply the Affine Transform just found to the src image]\n# Rotating the image after Warp\n## [Compute a rotation matrix with respect to the center of the image]\ncenter = (warp_dst.shape[1]//2, warp_dst.shape[0]//2)\nangle = -50\nscale = 0.6\n## [Compute a rotation matrix with respect to the center of the image]\n## [Get the rotation matrix with the specifications above]\nrot_mat = cv.getRotationMatrix2D( center, angle, scale )",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "center",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "center = (warp_dst.shape[1]//2, warp_dst.shape[0]//2)\nangle = -50\nscale = 0.6\n## [Compute a rotation matrix with respect to the center of the image]\n## [Get the rotation matrix with the specifications above]\nrot_mat = cv.getRotationMatrix2D( center, angle, scale )\n## [Get the rotation matrix with the specifications above]\n## [Rotate the warped image]\nwarp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))\n## [Rotate the warped image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "angle",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "angle = -50\nscale = 0.6\n## [Compute a rotation matrix with respect to the center of the image]\n## [Get the rotation matrix with the specifications above]\nrot_mat = cv.getRotationMatrix2D( center, angle, scale )\n## [Get the rotation matrix with the specifications above]\n## [Rotate the warped image]\nwarp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))\n## [Rotate the warped image]\n## [Show what you got]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "scale = 0.6\n## [Compute a rotation matrix with respect to the center of the image]\n## [Get the rotation matrix with the specifications above]\nrot_mat = cv.getRotationMatrix2D( center, angle, scale )\n## [Get the rotation matrix with the specifications above]\n## [Rotate the warped image]\nwarp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))\n## [Rotate the warped image]\n## [Show what you got]\ncv.imshow('Source image', src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "rot_mat",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "rot_mat = cv.getRotationMatrix2D( center, angle, scale )\n## [Get the rotation matrix with the specifications above]\n## [Rotate the warped image]\nwarp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))\n## [Rotate the warped image]\n## [Show what you got]\ncv.imshow('Source image', src)\ncv.imshow('Warp', warp_dst)\ncv.imshow('Warp + Rotate', warp_rotate_dst)\n## [Show what you got]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "warp_rotate_dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "peekOfCode": "warp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))\n## [Rotate the warped image]\n## [Show what you got]\ncv.imshow('Source image', src)\ncv.imshow('Warp', warp_dst)\ncv.imshow('Warp + Rotate', warp_rotate_dst)\n## [Show what you got]\n## [Wait until user exits the program]\ncv.waitKey()\n## [Wait until user exits the program]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ImgTrans.warp_affine.Geometric_Transforms_Demo",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "peekOfCode": "img = cv.imread(cv.samples.findFile(\"starry_night.jpg\"))\n## [imread]\n## [empty]\nif img is None:\n    sys.exit(\"Could not read the image.\")\n## [empty]\n## [imshow]\ncv.imshow(\"Display window\", img)\nk = cv.waitKey(0)\n## [imshow]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "documentation": {}
    },
    {
        "label": "k",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "description": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "peekOfCode": "k = cv.waitKey(0)\n## [imshow]\n## [imsave]\nif k == ord(\"s\"):\n    cv.imwrite(\"starry_night.png\", img)\n## [imsave]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.introduction.display_image.display_image",
        "documentation": {}
    },
    {
        "label": "drawAxis",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "def drawAxis(img, p_, q_, colour, scale):\n    p = list(p_)\n    q = list(q_)\n    ## [visualization1]\n    angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians\n    hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))\n    # Here we lengthen the arrow by a factor of scale\n    q[0] = p[0] - scale * hypotenuse * cos(angle)\n    q[1] = p[1] - scale * hypotenuse * sin(angle)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "getOrientation",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "def getOrientation(pts, img):\n    ## [pca]\n    # Construct a buffer used by the pca analysis\n    sz = len(pts)\n    data_pts = np.empty((sz, 2), dtype=np.float64)\n    for i in range(data_pts.shape[0]):\n        data_pts[i,0] = pts[i,0,0]\n        data_pts[i,1] = pts[i,0,1]\n    # Perform PCA analysis\n    mean = np.empty((0))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\\\n                                              This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')\nparser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\n# Check if image is loaded successfully\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.imshow('src', src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\n# Check if image is loaded successfully\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.imshow('src', src)\n# Convert image to grayscale\ngray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Convert image to binary",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\n# Check if image is loaded successfully\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.imshow('src', src)\n# Convert image to grayscale\ngray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Convert image to binary\n_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "peekOfCode": "gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Convert image to binary\n_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\n## [pre-process]\n## [contours]\n# Find all the contours in the thresholded image\ncontours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\nfor i, c in enumerate(contours):\n    # Calculate the area of each contour\n    area = cv.contourArea(c)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_pca.introduction_to_pca",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "labels = np.array([1, -1, -1, -1])\ntrainingData = np.matrix([[501, 10], [255, 10], [501, 255], [10, 501]], dtype=np.float32)\n## [setup1]\n# Train the SVM\n## [init]\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n## [init]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "trainingData",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "trainingData = np.matrix([[501, 10], [255, 10], [501, 255], [10, 501]], dtype=np.float32)\n## [setup1]\n# Train the SVM\n## [init]\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n## [init]\n## [train]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "svm",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "svm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n## [init]\n## [train]\nsvm.train(trainingData, cv.ml.ROW_SAMPLE, labels)\n## [train]\n# Data for visual representation\nwidth = 512",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "width",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "width = 512\nheight = 512\nimage = np.zeros((height, width, 3), dtype=np.uint8)\n# Show the decision regions given by the SVM\n## [show]\ngreen = (0,255,0)\nblue = (255,0,0)\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "height",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "height = 512\nimage = np.zeros((height, width, 3), dtype=np.uint8)\n# Show the decision regions given by the SVM\n## [show]\ngreen = (0,255,0)\nblue = (255,0,0)\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "image = np.zeros((height, width, 3), dtype=np.uint8)\n# Show the decision regions given by the SVM\n## [show]\ngreen = (0,255,0)\nblue = (255,0,0)\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "green",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "green = (0,255,0)\nblue = (255,0,0)\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:\n            image[i,j] = green\n        elif response == -1:\n            image[i,j] = blue",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "blue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "blue = (255,0,0)\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:\n            image[i,j] = green\n        elif response == -1:\n            image[i,j] = blue\n## [show]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "thickness",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "thickness = -1\ncv.circle(image, (501,  10), 5, (  0,   0,   0), thickness)\ncv.circle(image, (255,  10), 5, (255, 255, 255), thickness)\ncv.circle(image, (501, 255), 5, (255, 255, 255), thickness)\ncv.circle(image, ( 10, 501), 5, (255, 255, 255), thickness)\n## [show_data]\n# Show support vectors\n## [show_vectors]\nthickness = 2\nsv = svm.getUncompressedSupportVectors()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "thickness",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "thickness = 2\nsv = svm.getUncompressedSupportVectors()\nfor i in range(sv.shape[0]):\n    cv.circle(image, (int(sv[i,0]), int(sv[i,1])), 6, (128, 128, 128), thickness)\n## [show_vectors]\ncv.imwrite('result.png', image) # save the image\ncv.imshow('SVM Simple Example', image) # show it to the user\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "sv",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "peekOfCode": "sv = svm.getUncompressedSupportVectors()\nfor i in range(sv.shape[0]):\n    cv.circle(image, (int(sv[i,0]), int(sv[i,1])), 6, (128, 128, 128), thickness)\n## [show_vectors]\ncv.imwrite('result.png', image) # save the image\ncv.imshow('SVM Simple Example', image) # show it to the user\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.introduction_to_svm.introduction_to_svm",
        "documentation": {}
    },
    {
        "label": "NTRAINING_SAMPLES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "NTRAINING_SAMPLES = 100 # Number of training samples per class\nFRAC_LINEAR_SEP = 0.9   # Fraction of samples which compose the linear separable part\n# Data for visual representation\nWIDTH = 512\nHEIGHT = 512\nI = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n# --------------------- 1. Set up training data randomly ---------------------------------------\ntrainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "FRAC_LINEAR_SEP",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "FRAC_LINEAR_SEP = 0.9   # Fraction of samples which compose the linear separable part\n# Data for visual representation\nWIDTH = 512\nHEIGHT = 512\nI = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n# --------------------- 1. Set up training data randomly ---------------------------------------\ntrainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "WIDTH",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "WIDTH = 512\nHEIGHT = 512\nI = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n# --------------------- 1. Set up training data randomly ---------------------------------------\ntrainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data\nnLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "HEIGHT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "HEIGHT = 512\nI = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n# --------------------- 1. Set up training data randomly ---------------------------------------\ntrainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data\nnLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]\n# Generate random points for the class 1",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "I",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "I = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n# --------------------- 1. Set up training data randomly ---------------------------------------\ntrainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data\nnLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]\n# Generate random points for the class 1\ntrainClass = trainData[0:nLinearSamples,:]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "trainData",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "trainData = np.empty((2*NTRAINING_SAMPLES, 2), dtype=np.float32)\nlabels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data\nnLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]\n# Generate random points for the class 1\ntrainClass = trainData[0:nLinearSamples,:]\n# The x coordinate of the points is in [0, 0.4)\nc = trainClass[:,0:1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "labels = np.empty((2*NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng.seed(100) # Random value generation class\n# Set up the linearly separable part of the training data\nnLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]\n# Generate random points for the class 1\ntrainClass = trainData[0:nLinearSamples,:]\n# The x coordinate of the points is in [0, 0.4)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.0, 0.4 * WIDTH, c.shape)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "nLinearSamples",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "nLinearSamples = int(FRAC_LINEAR_SEP * NTRAINING_SAMPLES)\n## [setup1]\n# Generate random points for the class 1\ntrainClass = trainData[0:nLinearSamples,:]\n# The x coordinate of the points is in [0, 0.4)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.0, 0.4 * WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "trainClass",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "trainClass = trainData[0:nLinearSamples,:]\n# The x coordinate of the points is in [0, 0.4)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.0, 0.4 * WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n# Generate random points for the class 2\ntrainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,0:1]\nc[:] = np.random.uniform(0.0, 0.4 * WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n# Generate random points for the class 2\ntrainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.0, 0.4 * WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n# Generate random points for the class 2\ntrainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n# Generate random points for the class 2\ntrainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n# Generate random points for the class 2\ntrainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "trainClass",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "trainClass = trainData[2*NTRAINING_SAMPLES-nLinearSamples:2*NTRAINING_SAMPLES,:]\n# The x coordinate of the points is in [0.6, 1]\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]\n#------------------ Set up the non-linearly separable part of the training data ---------------\n## [setup2]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,0:1]\nc[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]\n#------------------ Set up the non-linearly separable part of the training data ---------------\n## [setup2]\n# Generate random points for the classes 1 and 2\ntrainClass = trainData[nLinearSamples:2*NTRAINING_SAMPLES-nLinearSamples,:]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.6*WIDTH, WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]\n#------------------ Set up the non-linearly separable part of the training data ---------------\n## [setup2]\n# Generate random points for the classes 1 and 2\ntrainClass = trainData[nLinearSamples:2*NTRAINING_SAMPLES-nLinearSamples,:]\n# The x coordinate of the points is in [0.4, 0.6)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]\n#------------------ Set up the non-linearly separable part of the training data ---------------\n## [setup2]\n# Generate random points for the classes 1 and 2\ntrainClass = trainData[nLinearSamples:2*NTRAINING_SAMPLES-nLinearSamples,:]\n# The x coordinate of the points is in [0.4, 0.6)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.4*WIDTH, 0.6*WIDTH, c.shape)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup1]\n#------------------ Set up the non-linearly separable part of the training data ---------------\n## [setup2]\n# Generate random points for the classes 1 and 2\ntrainClass = trainData[nLinearSamples:2*NTRAINING_SAMPLES-nLinearSamples,:]\n# The x coordinate of the points is in [0.4, 0.6)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.4*WIDTH, 0.6*WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "trainClass",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "trainClass = trainData[nLinearSamples:2*NTRAINING_SAMPLES-nLinearSamples,:]\n# The x coordinate of the points is in [0.4, 0.6)\nc = trainClass[:,0:1]\nc[:] = np.random.uniform(0.4*WIDTH, 0.6*WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup2]\n#------------------------- Set up the labels for the classes ---------------------------------\nlabels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,0:1]\nc[:] = np.random.uniform(0.4*WIDTH, 0.6*WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup2]\n#------------------------- Set up the labels for the classes ---------------------------------\nlabels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1\nlabels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.4*WIDTH, 0.6*WIDTH, c.shape)\n# The y coordinate of the points is in [0, 1)\nc = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup2]\n#------------------------- Set up the labels for the classes ---------------------------------\nlabels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1\nlabels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------\nprint('Starting training process')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c = trainClass[:,1:2]\nc[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup2]\n#------------------------- Set up the labels for the classes ---------------------------------\nlabels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1\nlabels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------\nprint('Starting training process')\n## [init]\nsvm = cv.ml.SVM_create()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "c[:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "c[:] = np.random.uniform(0.0, HEIGHT, c.shape)\n## [setup2]\n#------------------------- Set up the labels for the classes ---------------------------------\nlabels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1\nlabels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------\nprint('Starting training process')\n## [init]\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "labels[0:NTRAINING_SAMPLES,:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "labels[0:NTRAINING_SAMPLES,:] = 1                   # Class 1\nlabels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------\nprint('Starting training process')\n## [init]\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(0.1)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, int(1e7), 1e-6))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "labels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "labels[NTRAINING_SAMPLES:2*NTRAINING_SAMPLES,:] = 2 # Class 2\n#------------------------ 2. Set up the support vector machines parameters --------------------\nprint('Starting training process')\n## [init]\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(0.1)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, int(1e7), 1e-6))\n## [init]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "svm",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "svm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(0.1)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, int(1e7), 1e-6))\n## [init]\n#------------------------ 3. Train the svm ----------------------------------------------------\n## [train]\nsvm.train(trainData, cv.ml.ROW_SAMPLE, labels)\n## [train]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "green",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "green = (0,100,0)\nblue = (100,0,0)\nfor i in range(I.shape[0]):\n    for j in range(I.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:\n            I[i,j] = green\n        elif response == 2:\n            I[i,j] = blue",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "blue",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "blue = (100,0,0)\nfor i in range(I.shape[0]):\n    for j in range(I.shape[1]):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:\n            I[i,j] = green\n        elif response == 2:\n            I[i,j] = blue\n## [show]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "thick",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "thick = -1\n# Class 1\nfor i in range(NTRAINING_SAMPLES):\n    px = trainData[i,0]\n    py = trainData[i,1]\n    cv.circle(I, (int(px), int(py)), 3, (0, 255, 0), thick)\n# Class 2\nfor i in range(NTRAINING_SAMPLES, 2*NTRAINING_SAMPLES):\n    px = trainData[i,0]\n    py = trainData[i,1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "thick",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "thick = 2\nsv = svm.getUncompressedSupportVectors()\nfor i in range(sv.shape[0]):\n    cv.circle(I, (int(sv[i,0]), int(sv[i,1])), 6, (128, 128, 128), thick)\n## [show_vectors]\ncv.imwrite('result.png', I)                      # save the Image\ncv.imshow('SVM for Non-Linear Training Data', I) # show it to the user\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "sv",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "peekOfCode": "sv = svm.getUncompressedSupportVectors()\nfor i in range(sv.shape[0]):\n    cv.circle(I, (int(sv[i,0]), int(sv[i,1])), 6, (128, 128, 128), thick)\n## [show_vectors]\ncv.imwrite('result.png', I)                      # save the Image\ncv.imshow('SVM for Non-Linear Training Data', I) # show it to the user\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.non_linear_svms.non_linear_svms",
        "documentation": {}
    },
    {
        "label": "deskew",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "def deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n    return img\n## [deskew]\n## [hog]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "hog",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "def hog(img):\n    gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n    gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n    mag, ang = cv.cartToPolar(gx, gy)\n    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n    hist = np.hstack(hists)     # hist is a 64 bit vector\n    return hist",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "bin_n",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "bin_n = 16 # Number of bins\naffine_flags = cv.WARP_INVERSE_MAP|cv.INTER_LINEAR\n## [deskew]\ndef deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "affine_flags",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "affine_flags = cv.WARP_INVERSE_MAP|cv.INTER_LINEAR\n## [deskew]\ndef deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n    return img",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "img = cv.imread(cv.samples.findFile('digits.png'),0)\nif img is None:\n    raise Exception(\"we need the digits.png image from samples/data here !\")\ncells = [np.hsplit(row,100) for row in np.vsplit(img,50)]\n# First half is trainData, remaining is testData\ntrain_cells = [ i[:50] for i in cells ]\ntest_cells = [ i[50:] for i in cells]\n######     Now training      ########################\ndeskewed = [list(map(deskew,row)) for row in train_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "cells",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]\n# First half is trainData, remaining is testData\ntrain_cells = [ i[:50] for i in cells ]\ntest_cells = [ i[50:] for i in cells]\n######     Now training      ########################\ndeskewed = [list(map(deskew,row)) for row in train_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]\ntrainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "train_cells",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "train_cells = [ i[:50] for i in cells ]\ntest_cells = [ i[50:] for i in cells]\n######     Now training      ########################\ndeskewed = [list(map(deskew,row)) for row in train_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]\ntrainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "test_cells",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "test_cells = [ i[50:] for i in cells]\n######     Now training      ########################\ndeskewed = [list(map(deskew,row)) for row in train_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]\ntrainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "deskewed",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "deskewed = [list(map(deskew,row)) for row in train_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]\ntrainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)\nsvm.setGamma(5.383)\nsvm.train(trainData, cv.ml.ROW_SAMPLE, responses)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "hogdata",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "hogdata = [list(map(hog,row)) for row in deskewed]\ntrainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)\nsvm.setGamma(5.383)\nsvm.train(trainData, cv.ml.ROW_SAMPLE, responses)\nsvm.save('svm_data.dat')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "trainData",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "trainData = np.float32(hogdata).reshape(-1,64)\nresponses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)\nsvm.setGamma(5.383)\nsvm.train(trainData, cv.ml.ROW_SAMPLE, responses)\nsvm.save('svm_data.dat')\n######     Now testing      ########################",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "responses",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "responses = np.repeat(np.arange(10),250)[:,np.newaxis]\nsvm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)\nsvm.setGamma(5.383)\nsvm.train(trainData, cv.ml.ROW_SAMPLE, responses)\nsvm.save('svm_data.dat')\n######     Now testing      ########################\ndeskewed = [list(map(deskew,row)) for row in test_cells]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "svm",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "svm = cv.ml.SVM_create()\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(2.67)\nsvm.setGamma(5.383)\nsvm.train(trainData, cv.ml.ROW_SAMPLE, responses)\nsvm.save('svm_data.dat')\n######     Now testing      ########################\ndeskewed = [list(map(deskew,row)) for row in test_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "deskewed",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "deskewed = [list(map(deskew,row)) for row in test_cells]\nhogdata = [list(map(hog,row)) for row in deskewed]\ntestData = np.float32(hogdata).reshape(-1,bin_n*4)\nresult = svm.predict(testData)[1]\n#######   Check Accuracy   ########################\nmask = result==responses\ncorrect = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "hogdata",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "hogdata = [list(map(hog,row)) for row in deskewed]\ntestData = np.float32(hogdata).reshape(-1,bin_n*4)\nresult = svm.predict(testData)[1]\n#######   Check Accuracy   ########################\nmask = result==responses\ncorrect = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "testData",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "testData = np.float32(hogdata).reshape(-1,bin_n*4)\nresult = svm.predict(testData)[1]\n#######   Check Accuracy   ########################\nmask = result==responses\ncorrect = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "result = svm.predict(testData)[1]\n#######   Check Accuracy   ########################\nmask = result==responses\ncorrect = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "mask = result==responses\ncorrect = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "correct",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "peekOfCode": "correct = np.count_nonzero(mask)\nprint(correct*100.0/result.size)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ml.py_svm_opencv.hogsvm",
        "documentation": {}
    },
    {
        "label": "detectAndDisplay",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "def detectAndDisplay(frame):\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    frame_gray = cv.equalizeHist(frame_gray)\n    #-- Detect faces\n    faces = face_cascade.detectMultiScale(frame_gray)\n    for (x,y,w,h) in faces:\n        center = (x + w//2, y + h//2)\n        frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)\n        faceROI = frame_gray[y:y+h,x:x+w]\n        #-- In each face, detect eyes",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Cascade Classifier tutorial.')\nparser.add_argument('--face_cascade', help='Path to face cascade.', default='data/haarcascades/haarcascade_frontalface_alt.xml')\nparser.add_argument('--eyes_cascade', help='Path to eyes cascade.', default='data/haarcascades/haarcascade_eye_tree_eyeglasses.xml')\nparser.add_argument('--camera', help='Camera divide number.', type=int, default=0)\nargs = parser.parse_args()\nface_cascade_name = args.face_cascade\neyes_cascade_name = args.eyes_cascade\nface_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "args = parser.parse_args()\nface_cascade_name = args.face_cascade\neyes_cascade_name = args.eyes_cascade\nface_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "face_cascade_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "face_cascade_name = args.face_cascade\neyes_cascade_name = args.eyes_cascade\nface_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n    print('--(!)Error loading eyes cascade')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "eyes_cascade_name",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "eyes_cascade_name = args.eyes_cascade\nface_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n    print('--(!)Error loading eyes cascade')\n    exit(0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "face_cascade",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "face_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n    print('--(!)Error loading eyes cascade')\n    exit(0)\ncamera_device = args.camera",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "eyes_cascade",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "eyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n    print('--(!)Error loading eyes cascade')\n    exit(0)\ncamera_device = args.camera\n#-- 2. Read the video stream",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "camera_device",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "camera_device = args.camera\n#-- 2. Read the video stream\ncap = cv.VideoCapture(camera_device)\nif not cap.isOpened:\n    print('--(!)Error opening video capture')\n    exit(0)\nwhile True:\n    ret, frame = cap.read()\n    if frame is None:\n        print('--(!) No captured frame -- Break!')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "description": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "peekOfCode": "cap = cv.VideoCapture(camera_device)\nif not cap.isOpened:\n    print('--(!)Error opening video capture')\n    exit(0)\nwhile True:\n    ret, frame = cap.read()\n    if frame is None:\n        print('--(!) No captured frame -- Break!')\n        break\n    detectAndDisplay(frame)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.objectDetection.cascade_classifier.objectDetection",
        "documentation": {}
    },
    {
        "label": "loadExposureSeq",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "def loadExposureSeq(path):\n    images = []\n    times = []\n    with open(os.path.join(path, 'list.txt')) as f:\n        content = f.readlines()\n    for line in content:\n        tokens = line.split()\n        images.append(cv.imread(os.path.join(path, tokens[0])))\n        times.append(1 / float(tokens[1]))\n    return images, np.asarray(times, dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for High Dynamic Range Imaging tutorial.')\nparser.add_argument('--input', type=str, help='Path to the directory that contains images and exposure times.')\nargs = parser.parse_args()\nif not args.input:\n    parser.print_help()\n    exit(0)\n## [Load images and exposure times]\nimages, times = loadExposureSeq(args.input)\n## [Load images and exposure times]\n## [Estimate camera response]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "args = parser.parse_args()\nif not args.input:\n    parser.print_help()\n    exit(0)\n## [Load images and exposure times]\nimages, times = loadExposureSeq(args.input)\n## [Load images and exposure times]\n## [Estimate camera response]\ncalibrate = cv.createCalibrateDebevec()\nresponse = calibrate.process(images, times)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "calibrate",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "calibrate = cv.createCalibrateDebevec()\nresponse = calibrate.process(images, times)\n## [Estimate camera response]\n## [Make HDR image]\nmerge_debevec = cv.createMergeDebevec()\nhdr = merge_debevec.process(images, times, response)\n## [Make HDR image]\n## [Tonemap HDR image]\ntonemap = cv.createTonemap(2.2)\nldr = tonemap.process(hdr)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "response = calibrate.process(images, times)\n## [Estimate camera response]\n## [Make HDR image]\nmerge_debevec = cv.createMergeDebevec()\nhdr = merge_debevec.process(images, times, response)\n## [Make HDR image]\n## [Tonemap HDR image]\ntonemap = cv.createTonemap(2.2)\nldr = tonemap.process(hdr)\n## [Tonemap HDR image]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "merge_debevec",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "merge_debevec = cv.createMergeDebevec()\nhdr = merge_debevec.process(images, times, response)\n## [Make HDR image]\n## [Tonemap HDR image]\ntonemap = cv.createTonemap(2.2)\nldr = tonemap.process(hdr)\n## [Tonemap HDR image]\n## [Perform exposure fusion]\nmerge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "hdr",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "hdr = merge_debevec.process(images, times, response)\n## [Make HDR image]\n## [Tonemap HDR image]\ntonemap = cv.createTonemap(2.2)\nldr = tonemap.process(hdr)\n## [Tonemap HDR image]\n## [Perform exposure fusion]\nmerge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)\n## [Perform exposure fusion]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "tonemap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "tonemap = cv.createTonemap(2.2)\nldr = tonemap.process(hdr)\n## [Tonemap HDR image]\n## [Perform exposure fusion]\nmerge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)\n## [Perform exposure fusion]\n## [Write results]\ncv.imwrite('fusion.png', fusion * 255)\ncv.imwrite('ldr.png', ldr * 255)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "ldr",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "ldr = tonemap.process(hdr)\n## [Tonemap HDR image]\n## [Perform exposure fusion]\nmerge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)\n## [Perform exposure fusion]\n## [Write results]\ncv.imwrite('fusion.png', fusion * 255)\ncv.imwrite('ldr.png', ldr * 255)\ncv.imwrite('hdr.hdr', hdr)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "merge_mertens",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "merge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)\n## [Perform exposure fusion]\n## [Write results]\ncv.imwrite('fusion.png', fusion * 255)\ncv.imwrite('ldr.png', ldr * 255)\ncv.imwrite('hdr.hdr', hdr)\n## [Write results]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "fusion",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "description": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "peekOfCode": "fusion = merge_mertens.process(images)\n## [Perform exposure fusion]\n## [Write results]\ncv.imwrite('fusion.png', fusion * 255)\ncv.imwrite('ldr.png', ldr * 255)\ncv.imwrite('hdr.hdr', hdr)\n## [Write results]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.photo.hdr_imaging.hdr_imaging",
        "documentation": {}
    },
    {
        "label": "thresh_callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "def thresh_callback(val):\n    threshold = val\n    ## [Canny]\n    # Detect edges using Canny\n    canny_output = cv.Canny(src_gray, threshold, threshold * 2)\n    ## [Canny]\n    ## [findContours]\n    # Find contours\n    contours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    ## [findContours]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Creating Bounding boxes and circles for contours tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='stuff.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "src_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "source_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "max_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "peekOfCode": "thresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rects_circles.generalContours_demo1",
        "documentation": {}
    },
    {
        "label": "thresh_callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "def thresh_callback(val):\n    threshold = val\n    ## [Canny]\n    # Detect edges using Canny\n    canny_output = cv.Canny(src_gray, threshold, threshold * 2)\n    ## [Canny]\n    ## [findContours]\n    # Find contours\n    contours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    ## [findContours]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Creating Bounding rotated boxes and ellipses for contours tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='stuff.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "src_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "source_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "max_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "peekOfCode": "thresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.bounding_rotated_ellipses.generalContours_demo2",
        "documentation": {}
    },
    {
        "label": "thresh_callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "def thresh_callback(val):\n    threshold = val\n    # Detect edges using Canny\n    canny_output = cv.Canny(src_gray, threshold, threshold * 2)\n    # Find contours\n    contours, hierarchy = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    # Draw contours\n    drawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n    for i in range(len(contours)):\n        color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Finding contours in your image tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='HappyFish.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "src_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "source_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "max_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "peekOfCode": "thresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.find_contours.findContours_demo",
        "documentation": {}
    },
    {
        "label": "thresh_callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "def thresh_callback(val):\n    threshold = val\n    # Detect edges using Canny\n    canny_output = cv.Canny(src_gray, threshold, threshold * 2)\n    # Find contours\n    contours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    # Find the convex hull object for each contour\n    hull_list = []\n    for i in range(len(contours)):\n        hull = cv.convexHull(contours[i])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Convex Hull tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='stuff.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "src_gray = cv.blur(src_gray, (3,3))\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "source_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "max_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "peekOfCode": "thresh = 100 # initial threshold\ncv.createTrackbar('Canny thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.hull.hull_demo",
        "documentation": {}
    },
    {
        "label": "thresh_callback",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "def thresh_callback(val):\n    threshold = val\n    ## [Canny]\n    # Detect edges using Canny\n    canny_output = cv.Canny(src_gray, threshold, threshold * 2)\n    ## [Canny]\n    ## [findContours]\n    # Find contours\n    contours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    ## [findContours]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Image Moments tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='stuff.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n# Convert image to gray and blur it\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nsrc_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "src_gray = cv.blur(src_gray, (3,3))\n## [setup]\n## [createWindow]\n# Create Window\nsource_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "source_window = 'Source'\ncv.namedWindow(source_window)\ncv.imshow(source_window, src)\n## [createWindow]\n## [trackbar]\nmax_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "max_thresh = 255\nthresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "peekOfCode": "thresh = 100 # initial threshold\ncv.createTrackbar('Canny Thresh:', source_window, thresh, max_thresh, thresh_callback)\nthresh_callback(thresh)\n## [trackbar]\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.moments.moments_demo",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "r = 100\nsrc = np.zeros((4*r, 4*r), dtype=np.uint8)\n# Create a sequence of points to make a contour\nvert = [None]*6\nvert[0] = (3*r//2, int(1.34*r))\nvert[1] = (1*r, 2*r)\nvert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "src = np.zeros((4*r, 4*r), dtype=np.uint8)\n# Create a sequence of points to make a contour\nvert = [None]*6\nvert[0] = (3*r//2, int(1.34*r))\nvert[1] = (1*r, 2*r)\nvert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert = [None]*6\nvert[0] = (3*r//2, int(1.34*r))\nvert[1] = (1*r, 2*r)\nvert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[0]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[0] = (3*r//2, int(1.34*r))\nvert[1] = (1*r, 2*r)\nvert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[1]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[1] = (1*r, 2*r)\nvert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[2]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[2] = (3*r//2, int(2.866*r))\nvert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Calculate the distances to the contour",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[3]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[3] = (5*r//2, int(2.866*r))\nvert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Calculate the distances to the contour\nraw_dist = np.empty(src.shape, dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[4]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[4] = (3*r, 2*r)\nvert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Calculate the distances to the contour\nraw_dist = np.empty(src.shape, dtype=np.float32)\nfor i in range(src.shape[0]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "vert[5]",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "vert[5] = (5*r//2, int(1.34*r))\n# Draw it in src\nfor i in range(6):\n    cv.line(src, vert[i],  vert[(i+1)%6], ( 255 ), 3)\n# Get the contours\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Calculate the distances to the contour\nraw_dist = np.empty(src.shape, dtype=np.float32)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "raw_dist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "raw_dist = np.empty(src.shape, dtype=np.float32)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        raw_dist[i,j] = cv.pointPolygonTest(contours[0], (j,i), True)\nminVal, maxVal, _, maxDistPt = cv.minMaxLoc(raw_dist)\nminVal = abs(minVal)\nmaxVal = abs(maxVal)\n# Depicting the  distances graphically\ndrawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)\nfor i in range(src.shape[0]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "minVal",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "minVal = abs(minVal)\nmaxVal = abs(maxVal)\n# Depicting the  distances graphically\ndrawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        if raw_dist[i,j] < 0:\n            drawing[i,j,0] = 255 - abs(raw_dist[i,j]) * 255 / minVal\n        elif raw_dist[i,j] > 0:\n            drawing[i,j,2] = 255 - raw_dist[i,j] * 255 / maxVal",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "maxVal",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "maxVal = abs(maxVal)\n# Depicting the  distances graphically\ndrawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        if raw_dist[i,j] < 0:\n            drawing[i,j,0] = 255 - abs(raw_dist[i,j]) * 255 / minVal\n        elif raw_dist[i,j] > 0:\n            drawing[i,j,2] = 255 - raw_dist[i,j] * 255 / maxVal\n        else:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "drawing",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "peekOfCode": "drawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        if raw_dist[i,j] < 0:\n            drawing[i,j,0] = 255 - abs(raw_dist[i,j]) * 255 / minVal\n        elif raw_dist[i,j] > 0:\n            drawing[i,j,2] = 255 - raw_dist[i,j] * 255 / maxVal\n        else:\n            drawing[i,j,0] = 255\n            drawing[i,j,1] = 255",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.ShapeDescriptors.point_polygon_test.pointPolygonTest_demo",
        "documentation": {}
    },
    {
        "label": "goodFeaturesToTrack_Demo",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "def goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3\n    useHarrisDetector = False\n    k = 0.04\n    # Copy the source image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "source_window = 'Image'\nmaxTrackbar = 25\nrng.seed(12345)\ndef goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "maxTrackbar",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "maxTrackbar = 25\nrng.seed(12345)\ndef goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3\n    useHarrisDetector = False",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Shi-Tomasi corner detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='pic3.png')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 10 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 10 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 10 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)\ngoodFeaturesToTrack_Demo(maxCorners)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "maxCorners",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "peekOfCode": "maxCorners = 10 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)\ngoodFeaturesToTrack_Demo(maxCorners)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.corner_subpixels.cornerSubPix_Demo",
        "documentation": {}
    },
    {
        "label": "myHarris_function",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "def myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):\n            if Mc[i,j] > myHarris_minVal + ( myHarris_maxVal - myHarris_minVal )*myHarris_qualityLevel/max_qualityLevel:\n                cv.circle(myHarris_copy, (j,i), 4, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)), cv.FILLED)\n    cv.imshow(myHarris_window, myHarris_copy)\ndef myShiTomasi_function(val):\n    myShiTomasi_copy = np.copy(src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myShiTomasi_function",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "def myShiTomasi_function(val):\n    myShiTomasi_copy = np.copy(src)\n    myShiTomasi_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):\n            if myShiTomasi_dst[i,j] > myShiTomasi_minVal + ( myShiTomasi_maxVal - myShiTomasi_minVal )*myShiTomasi_qualityLevel/max_qualityLevel:\n                cv.circle(myShiTomasi_copy, (j,i), 4, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)), cv.FILLED)\n    cv.imshow(myShiTomasi_window, myShiTomasi_copy)\n# Load source image and convert it to gray\nparser = argparse.ArgumentParser(description='Code for Creating your own corner detector tutorial.')",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myHarris_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myHarris_window = 'My Harris corner detector'\nmyShiTomasi_window = 'My Shi Tomasi corner detector'\nmyHarris_qualityLevel = 50\nmyShiTomasi_qualityLevel = 50\nmax_qualityLevel = 100\nrng.seed(12345)\ndef myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myShiTomasi_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myShiTomasi_window = 'My Shi Tomasi corner detector'\nmyHarris_qualityLevel = 50\nmyShiTomasi_qualityLevel = 50\nmax_qualityLevel = 100\nrng.seed(12345)\ndef myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myHarris_qualityLevel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myHarris_qualityLevel = 50\nmyShiTomasi_qualityLevel = 50\nmax_qualityLevel = 100\nrng.seed(12345)\ndef myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):\n            if Mc[i,j] > myHarris_minVal + ( myHarris_maxVal - myHarris_minVal )*myHarris_qualityLevel/max_qualityLevel:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myShiTomasi_qualityLevel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myShiTomasi_qualityLevel = 50\nmax_qualityLevel = 100\nrng.seed(12345)\ndef myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):\n            if Mc[i,j] > myHarris_minVal + ( myHarris_maxVal - myHarris_minVal )*myHarris_qualityLevel/max_qualityLevel:\n                cv.circle(myHarris_copy, (j,i), 4, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)), cv.FILLED)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "max_qualityLevel",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "max_qualityLevel = 100\nrng.seed(12345)\ndef myHarris_function(val):\n    myHarris_copy = np.copy(src)\n    myHarris_qualityLevel = max(val, 1)\n    for i in range(src_gray.shape[0]):\n        for j in range(src_gray.shape[1]):\n            if Mc[i,j] > myHarris_minVal + ( myHarris_maxVal - myHarris_minVal )*myHarris_qualityLevel/max_qualityLevel:\n                cv.circle(myHarris_copy, (j,i), 4, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)), cv.FILLED)\n    cv.imshow(myHarris_window, myHarris_copy)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Creating your own corner detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='building.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Set some parameters\nblockSize = 3",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Set some parameters\nblockSize = 3\napertureSize = 3\n# My Harris matrix -- Using cornerEigenValsAndVecs",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Set some parameters\nblockSize = 3\napertureSize = 3\n# My Harris matrix -- Using cornerEigenValsAndVecs\nmyHarris_dst = cv.cornerEigenValsAndVecs(src_gray, blockSize, apertureSize)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Set some parameters\nblockSize = 3\napertureSize = 3\n# My Harris matrix -- Using cornerEigenValsAndVecs\nmyHarris_dst = cv.cornerEigenValsAndVecs(src_gray, blockSize, apertureSize)\n# calculate Mc\nMc = np.empty(src_gray.shape, dtype=np.float32)\nfor i in range(src_gray.shape[0]):\n    for j in range(src_gray.shape[1]):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "blockSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "blockSize = 3\napertureSize = 3\n# My Harris matrix -- Using cornerEigenValsAndVecs\nmyHarris_dst = cv.cornerEigenValsAndVecs(src_gray, blockSize, apertureSize)\n# calculate Mc\nMc = np.empty(src_gray.shape, dtype=np.float32)\nfor i in range(src_gray.shape[0]):\n    for j in range(src_gray.shape[1]):\n        lambda_1 = myHarris_dst[i,j,0]\n        lambda_2 = myHarris_dst[i,j,1]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "apertureSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "apertureSize = 3\n# My Harris matrix -- Using cornerEigenValsAndVecs\nmyHarris_dst = cv.cornerEigenValsAndVecs(src_gray, blockSize, apertureSize)\n# calculate Mc\nMc = np.empty(src_gray.shape, dtype=np.float32)\nfor i in range(src_gray.shape[0]):\n    for j in range(src_gray.shape[1]):\n        lambda_1 = myHarris_dst[i,j,0]\n        lambda_2 = myHarris_dst[i,j,1]\n        Mc[i,j] = lambda_1*lambda_2 - 0.04*pow( ( lambda_1 + lambda_2 ), 2 )",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myHarris_dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myHarris_dst = cv.cornerEigenValsAndVecs(src_gray, blockSize, apertureSize)\n# calculate Mc\nMc = np.empty(src_gray.shape, dtype=np.float32)\nfor i in range(src_gray.shape[0]):\n    for j in range(src_gray.shape[1]):\n        lambda_1 = myHarris_dst[i,j,0]\n        lambda_2 = myHarris_dst[i,j,1]\n        Mc[i,j] = lambda_1*lambda_2 - 0.04*pow( ( lambda_1 + lambda_2 ), 2 )\nmyHarris_minVal, myHarris_maxVal, _, _ = cv.minMaxLoc(Mc)\n# Create Window and Trackbar",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "Mc",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "Mc = np.empty(src_gray.shape, dtype=np.float32)\nfor i in range(src_gray.shape[0]):\n    for j in range(src_gray.shape[1]):\n        lambda_1 = myHarris_dst[i,j,0]\n        lambda_2 = myHarris_dst[i,j,1]\n        Mc[i,j] = lambda_1*lambda_2 - 0.04*pow( ( lambda_1 + lambda_2 ), 2 )\nmyHarris_minVal, myHarris_maxVal, _, _ = cv.minMaxLoc(Mc)\n# Create Window and Trackbar\ncv.namedWindow(myHarris_window)\ncv.createTrackbar('Quality Level:', myHarris_window, myHarris_qualityLevel, max_qualityLevel, myHarris_function)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "myShiTomasi_dst",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "peekOfCode": "myShiTomasi_dst = cv.cornerMinEigenVal(src_gray, blockSize, apertureSize)\nmyShiTomasi_minVal, myShiTomasi_maxVal, _, _ = cv.minMaxLoc(myShiTomasi_dst)\n# Create Window and Trackbar\ncv.namedWindow(myShiTomasi_window)\ncv.createTrackbar('Quality Level:', myShiTomasi_window, myShiTomasi_qualityLevel, max_qualityLevel, myShiTomasi_function)\nmyShiTomasi_function(myShiTomasi_qualityLevel)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.generic_corner_detector.cornerDetector_Demo",
        "documentation": {}
    },
    {
        "label": "goodFeaturesToTrack_Demo",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "def goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3\n    useHarrisDetector = False\n    k = 0.04\n    # Copy the source image",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "source_window = 'Image'\nmaxTrackbar = 100\nrng.seed(12345)\ndef goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "maxTrackbar",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "maxTrackbar = 100\nrng.seed(12345)\ndef goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3\n    useHarrisDetector = False",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Shi-Tomasi corner detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='pic3.png')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 23 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 23 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmaxCorners = 23 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)\ngoodFeaturesToTrack_Demo(maxCorners)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "maxCorners",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "peekOfCode": "maxCorners = 23 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\ncv.imshow(source_window, src)\ngoodFeaturesToTrack_Demo(maxCorners)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.good_features_to_track.goodFeaturesToTrack_Demo",
        "documentation": {}
    },
    {
        "label": "cornerHarris_demo",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "def cornerHarris_demo(val):\n    thresh = val\n    # Detector parameters\n    blockSize = 2\n    apertureSize = 3\n    k = 0.04\n    # Detecting corners\n    dst = cv.cornerHarris(src_gray, blockSize, apertureSize, k)\n    # Normalizing\n    dst_norm = np.empty(dst.shape, dtype=np.float32)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "source_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "source_window = 'Source image'\ncorners_window = 'Corners detected'\nmax_thresh = 255\ndef cornerHarris_demo(val):\n    thresh = val\n    # Detector parameters\n    blockSize = 2\n    apertureSize = 3\n    k = 0.04\n    # Detecting corners",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "corners_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "corners_window = 'Corners detected'\nmax_thresh = 255\ndef cornerHarris_demo(val):\n    thresh = val\n    # Detector parameters\n    blockSize = 2\n    apertureSize = 3\n    k = 0.04\n    # Detecting corners\n    dst = cv.cornerHarris(src_gray, blockSize, apertureSize, k)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "max_thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "max_thresh = 255\ndef cornerHarris_demo(val):\n    thresh = val\n    # Detector parameters\n    blockSize = 2\n    apertureSize = 3\n    k = 0.04\n    # Detecting corners\n    dst = cv.cornerHarris(src_gray, blockSize, apertureSize, k)\n    # Normalizing",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Code for Harris corner detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='building.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "args = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nthresh = 200 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "src",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "src = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nthresh = 200 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\ncv.imshow(source_window, src)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "src_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nthresh = 200 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\ncv.imshow(source_window, src)\ncornerHarris_demo(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "thresh",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "description": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "peekOfCode": "thresh = 200 # initial threshold\ncv.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\ncv.imshow(source_window, src)\ncornerHarris_demo(thresh)\ncv.waitKey()",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.TrackingMotion.harris_detector.cornerHarris_Demo",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "peekOfCode": "parser = argparse.ArgumentParser(description='This program shows how to use background subtraction methods provided by \\\n                                              OpenCV. You can process both videos and images.')\nparser.add_argument('--input', type=str, help='Path to a video or a sequence of image.', default='vtest.avi')\nparser.add_argument('--algo', type=str, help='Background subtraction method (KNN, MOG2).', default='MOG2')\nargs = parser.parse_args()\n## [create]\n#create Background Subtractor objects\nif args.algo == 'MOG2':\n    backSub = cv.createBackgroundSubtractorMOG2()\nelse:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "peekOfCode": "args = parser.parse_args()\n## [create]\n#create Background Subtractor objects\nif args.algo == 'MOG2':\n    backSub = cv.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv.createBackgroundSubtractorKNN()\n## [create]\n## [capture]\ncapture = cv.VideoCapture(cv.samples.findFileOrKeep(args.input))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "documentation": {}
    },
    {
        "label": "capture",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "peekOfCode": "capture = cv.VideoCapture(cv.samples.findFileOrKeep(args.input))\nif not capture.isOpened():\n    print('Unable to open: ' + args.input)\n    exit(0)\n## [capture]\nwhile True:\n    ret, frame = capture.read()\n    if frame is None:\n        break\n    ## [apply]",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.background_subtraction.bg_sub",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "parser = argparse.ArgumentParser(description='This sample demonstrates the camshift algorithm. \\\n                                              The example file can be downloaded from: \\\n                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\nparser.add_argument('image', type=str, help='path to image file')\nargs = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "args = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "cap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "ret,frame",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "ret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "track_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "track_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "roi",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "roi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "hsv_roi",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "hsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "roi_hist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n        # apply camshift to get the new location",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "term_crit",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "peekOfCode": "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n        # apply camshift to get the new location\n        ret, track_window = cv.CamShift(dst, track_window, term_crit)\n        # Draw it on image\n        pts = cv.boxPoints(ret)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.camshift",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "parser = argparse.ArgumentParser(description='This sample demonstrates the meanshift algorithm. \\\n                                              The example file can be downloaded from: \\\n                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\nparser.add_argument('image', type=str, help='path to image file')\nargs = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "args = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "cap = cv.VideoCapture(args.image)\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "ret,frame",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "ret,frame = cap.read()\n# setup initial location of window\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\ntrack_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "track_window",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "track_window = (x, y, w, h)\n# set up the ROI for tracking\nroi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "roi",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "roi = frame[y:y+h, x:x+w]\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "hsv_roi",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "hsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "roi_hist",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n        # apply meanshift to get the new location",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "term_crit",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "peekOfCode": "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret, frame = cap.read()\n    if ret == True:\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n        # apply meanshift to get the new location\n        ret, track_window = cv.meanShift(dst, track_window, term_crit)\n        # Draw it on image\n        x,y,w,h = track_window",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.meanshift.meanshift",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "parser = argparse.ArgumentParser(description='This sample demonstrates Lucas-Kanade Optical Flow calculation. \\\n                                              The example file can be downloaded from: \\\n                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\nparser.add_argument('image', type=str, help='path to image file')\nargs = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# params for ShiTomasi corner detection\nfeature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "args = parser.parse_args()\ncap = cv.VideoCapture(args.image)\n# params for ShiTomasi corner detection\nfeature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\n# Parameters for lucas kanade optical flow\nlk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "cap = cv.VideoCapture(args.image)\n# params for ShiTomasi corner detection\nfeature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\n# Parameters for lucas kanade optical flow\nlk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "feature_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "feature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\n# Parameters for lucas kanade optical flow\nlk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n# Create some random colors\ncolor = np.random.randint(0, 255, (100, 3))",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "lk_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "lk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n# Create some random colors\ncolor = np.random.randint(0, 255, (100, 3))\n# Take first frame and find corners in it\nret, old_frame = cap.read()\nold_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\np0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n# Create a mask image for drawing purposes",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "color = np.random.randint(0, 255, (100, 3))\n# Take first frame and find corners in it\nret, old_frame = cap.read()\nold_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\np0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n# Create a mask image for drawing purposes\nmask = np.zeros_like(old_frame)\nwhile(1):\n    ret, frame = cap.read()\n    if not ret:",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "old_gray",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\np0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n# Create a mask image for drawing purposes\nmask = np.zeros_like(old_frame)\nwhile(1):\n    ret, frame = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "p0",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n# Create a mask image for drawing purposes\nmask = np.zeros_like(old_frame)\nwhile(1):\n    ret, frame = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    # calculate optical flow",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "peekOfCode": "mask = np.zeros_like(old_frame)\nwhile(1):\n    ret, frame = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    # calculate optical flow\n    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n    # Select good points",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "peekOfCode": "cap = cv.VideoCapture(cv.samples.findFile(\"vtest.avi\"))\nret, frame1 = cap.read()\nprvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\nhsv = np.zeros_like(frame1)\nhsv[..., 1] = 255\nwhile(1):\n    ret, frame2 = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "documentation": {}
    },
    {
        "label": "prvs",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "peekOfCode": "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\nhsv = np.zeros_like(frame1)\nhsv[..., 1] = 255\nwhile(1):\n    ret, frame2 = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "documentation": {}
    },
    {
        "label": "hsv",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "description": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "peekOfCode": "hsv = np.zeros_like(frame1)\nhsv[..., 1] = 255\nwhile(1):\n    ret, frame2 = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.video.optical_flow.optical_flow_dense",
        "documentation": {}
    },
    {
        "label": "getPSNR",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "description": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "peekOfCode": "def getPSNR(I1, I2):\n    s1 = cv.absdiff(I1, I2) #|I1 - I2|\n    s1 = np.float32(s1)     # cannot make a square on 8 bits\n    s1 = s1 * s1            # |I1 - I2|^2\n    sse = s1.sum()          # sum elements per channel\n    if sse <= 1e-10:        # sum channels\n        return 0            # for small values return zero\n    else:\n        shape = I1.shape\n        mse = 1.0 * sse / (shape[0] * shape[1] * shape[2])",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "documentation": {}
    },
    {
        "label": "getMSSISM",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "description": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "peekOfCode": "def getMSSISM(i1, i2):\n    C1 = 6.5025\n    C2 = 58.5225\n    # INITS\n    I1 = np.float32(i1) # cannot calculate on one byte large values\n    I2 = np.float32(i2)\n    I2_2 = I2 * I2 # I2^2\n    I1_2 = I1 * I1 # I1^2\n    I1_I2 = I1 * I2 # I1 * I2\n    # END INITS",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "description": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-d\", \"--delay\", type=int, default=30, help=\" Time delay\")\n    parser.add_argument(\"-v\", \"--psnrtriggervalue\", type=int, default=30, help=\"PSNR Trigger Value\")\n    parser.add_argument(\"-r\", \"--ref\", type=str, default=\"Megamind.avi\", help=\"Path to reference video\")\n    parser.add_argument(\"-t\", \"--undertest\", type=str, default=\"Megamind_bugy.avi\",\n                        help=\"Path to the video to be tested\")\n    args = parser.parse_args()\n    sourceReference = args.ref\n    sourceCompareWith = args.undertest",
        "detail": "Hw_2.opencv.samples.python.tutorial_code.videoio.video-input-psnr-ssim",
        "documentation": {}
    },
    {
        "label": "affine_skew",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.asift",
        "description": "Hw_2.opencv.samples.python.asift",
        "peekOfCode": "def affine_skew(tilt, phi, img, mask=None):\n    '''\n    affine_skew(tilt, phi, img, mask=None) -> skew_img, skew_mask, Ai\n    Ai - is an affine transform matrix from skew_img to img\n    '''\n    h, w = img.shape[:2]\n    if mask is None:\n        mask = np.zeros((h, w), np.uint8)\n        mask[:] = 255\n    A = np.float32([[1, 0, 0], [0, 1, 0]])",
        "detail": "Hw_2.opencv.samples.python.asift",
        "documentation": {}
    },
    {
        "label": "affine_detect",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.asift",
        "description": "Hw_2.opencv.samples.python.asift",
        "peekOfCode": "def affine_detect(detector, img, mask=None, pool=None):\n    '''\n    affine_detect(detector, img, mask=None, pool=None) -> keypoints, descrs\n    Apply a set of affine transformations to the image, detect keypoints and\n    reproject them into initial image coordinates.\n    See http://www.ipol.im/pub/algo/my_affine_sift/ for the details.\n    ThreadPool object may be passed to speedup the computation.\n    '''\n    params = [(1.0, 0.0)]\n    for t in 2**(0.5*np.arange(1,6)):",
        "detail": "Hw_2.opencv.samples.python.asift",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.asift",
        "description": "Hw_2.opencv.samples.python.asift",
        "peekOfCode": "def main():\n    import sys, getopt\n    opts, args = getopt.getopt(sys.argv[1:], '', ['feature='])\n    opts = dict(opts)\n    feature_name = opts.get('--feature', 'brisk-flann')\n    try:\n        fn1, fn2 = args\n    except:\n        fn1 = 'aero1.jpg'\n        fn2 = 'aero3.jpg'",
        "detail": "Hw_2.opencv.samples.python.asift",
        "documentation": {}
    },
    {
        "label": "AudioDrawing",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.audio_spectrogram",
        "description": "Hw_2.opencv.samples.python.audio_spectrogram",
        "peekOfCode": "class AudioDrawing:\n    '''\n        Used for drawing audio graphics\n    '''\n    def __init__(self, args):\n        self.inputType = args.inputType\n        self.draw = args.draw\n        self.graph = args.graph\n        self.audio = cv.samples.findFile(args.audio)\n        self.audioStream = args.audioStream",
        "detail": "Hw_2.opencv.samples.python.audio_spectrogram",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.browse",
        "description": "Hw_2.opencv.samples.python.browse",
        "peekOfCode": "def main():\n    if len(sys.argv) > 1:\n        fn = cv.samples.findFile(sys.argv[1])\n        print('loading %s ...' % fn)\n        img = cv.imread(fn)\n        if img is None:\n            print('Failed to load fn:', fn)\n            sys.exit(1)\n    else:\n        sz = 4096",
        "detail": "Hw_2.opencv.samples.python.browse",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.browse",
        "description": "Hw_2.opencv.samples.python.browse",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\n# built-in modules\nimport sys\ndef main():\n    if len(sys.argv) > 1:\n        fn = cv.samples.findFile(sys.argv[1])",
        "detail": "Hw_2.opencv.samples.python.browse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.calibrate",
        "description": "Hw_2.opencv.samples.python.calibrate",
        "peekOfCode": "def main():\n    import sys\n    import getopt\n    from glob import glob\n    args, img_mask = getopt.getopt(sys.argv[1:], '', ['debug=', 'square_size=', 'threads='])\n    args = dict(args)\n    args.setdefault('--debug', './output/')\n    args.setdefault('--square_size', 1.0)\n    args.setdefault('--threads', 4)\n    if not img_mask:",
        "detail": "Hw_2.opencv.samples.python.calibrate",
        "documentation": {}
    },
    {
        "label": "inverse_homogeneoux_matrix",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def inverse_homogeneoux_matrix(M):\n    R = M[0:3, 0:3]\n    T = M[0:3, 3]\n    M_inv = np.identity(4)\n    M_inv[0:3, 0:3] = R.T\n    M_inv[0:3, 3] = -(R.T).dot(T)\n    return M_inv\ndef transform_to_matplotlib_frame(cMo, X, inverse=False):\n    M = np.identity(4)\n    M[1,1] = 0",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "transform_to_matplotlib_frame",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def transform_to_matplotlib_frame(cMo, X, inverse=False):\n    M = np.identity(4)\n    M[1,1] = 0\n    M[1,2] = 1\n    M[2,1] = -1\n    M[2,2] = 0\n    if inverse:\n        return M.dot(inverse_homogeneoux_matrix(cMo).dot(X))\n    else:\n        return M.dot(cMo.dot(X))",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "create_camera_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def create_camera_model(camera_matrix, width, height, scale_focal, draw_frame_axis=False):\n    fx = camera_matrix[0,0]\n    fy = camera_matrix[1,1]\n    focal = 2 / (fx + fy)\n    f_scale = scale_focal * focal\n    # draw image plane\n    X_img_plane = np.ones((4,5))\n    X_img_plane[0:3,0] = [-width, height, f_scale]\n    X_img_plane[0:3,1] = [width, height, f_scale]\n    X_img_plane[0:3,2] = [width, -height, f_scale]",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "create_board_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def create_board_model(extrinsics, board_width, board_height, square_size, draw_frame_axis=False):\n    width = board_width*square_size\n    height = board_height*square_size\n    # draw calibration board\n    X_board = np.ones((4,5))\n    #X_board_cam = np.ones((extrinsics.shape[0],4,5))\n    X_board[0:3,0] = [0,0,0]\n    X_board[0:3,1] = [width,0,0]\n    X_board[0:3,2] = [width,height,0]\n    X_board[0:3,3] = [0,height,0]",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "draw_camera_boards",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def draw_camera_boards(ax, camera_matrix, cam_width, cam_height, scale_focal,\n                       extrinsics, board_width, board_height, square_size,\n                       patternCentric):\n    from matplotlib import cm\n    min_values = np.zeros((3,1))\n    min_values = np.inf\n    max_values = np.zeros((3,1))\n    max_values = -np.inf\n    if patternCentric:\n        X_moving = create_camera_model(camera_matrix, cam_width, cam_height, scale_focal)",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "description": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "peekOfCode": "def main():\n    import argparse\n    parser = argparse.ArgumentParser(description='Plot camera calibration extrinsics.',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--calibration', type=str, default='left_intrinsics.yml',\n                        help='YAML camera calibration file.')\n    parser.add_argument('--cam_width', type=float, default=0.064/2,\n                        help='Width/2 of the displayed camera.')\n    parser.add_argument('--cam_height', type=float, default=0.048/2,\n                        help='Height/2 of the displayed camera.')",
        "detail": "Hw_2.opencv.samples.python.camera_calibration_show_extrinsics",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.camshift",
        "description": "Hw_2.opencv.samples.python.camshift",
        "peekOfCode": "class App(object):\n    def __init__(self, video_src):\n        self.cam = video.create_capture(video_src, presets['cube'])\n        _ret, self.frame = self.cam.read()\n        cv.namedWindow('camshift')\n        cv.setMouseCallback('camshift', self.onmouse)\n        self.selection = None\n        self.drag_start = None\n        self.show_backproj = False\n        self.track_window = None",
        "detail": "Hw_2.opencv.samples.python.camshift",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.camshift",
        "description": "Hw_2.opencv.samples.python.camshift",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\n# local module\nimport video\nfrom video import presets\nclass App(object):\n    def __init__(self, video_src):",
        "detail": "Hw_2.opencv.samples.python.camshift",
        "documentation": {}
    },
    {
        "label": "coherence_filter",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.coherence",
        "description": "Hw_2.opencv.samples.python.coherence",
        "peekOfCode": "def coherence_filter(img, sigma = 11, str_sigma = 11, blend = 0.5, iter_n = 4):\n    h, w = img.shape[:2]\n    for i in xrange(iter_n):\n        print(i)\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        eigen = cv.cornerEigenValsAndVecs(gray, str_sigma, 3)\n        eigen = eigen.reshape(h, w, 3, 2)  # [[e1, e2], v1, v2]\n        x, y = eigen[:,:,1,0], eigen[:,:,1,1]\n        gxx = cv.Sobel(gray, cv.CV_32F, 2, 0, ksize=sigma)\n        gxy = cv.Sobel(gray, cv.CV_32F, 1, 1, ksize=sigma)",
        "detail": "Hw_2.opencv.samples.python.coherence",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.coherence",
        "description": "Hw_2.opencv.samples.python.coherence",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 'baboon.jpg'\n    src = cv.imread(cv.samples.findFile(fn))\n    def nothing(*argv):\n        pass\n    def update():",
        "detail": "Hw_2.opencv.samples.python.coherence",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.coherence",
        "description": "Hw_2.opencv.samples.python.coherence",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\ndef coherence_filter(img, sigma = 11, str_sigma = 11, blend = 0.5, iter_n = 4):\n    h, w = img.shape[:2]\n    for i in xrange(iter_n):\n        print(i)\n        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.coherence",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.color_histogram",
        "description": "Hw_2.opencv.samples.python.color_histogram",
        "peekOfCode": "class App():\n    def set_scale(self, val):\n        self.hist_scale = val\n    def run(self):\n        hsv_map = np.zeros((180, 256, 3), np.uint8)\n        h, s = np.indices(hsv_map.shape[:2])\n        hsv_map[:,:,0] = h\n        hsv_map[:,:,1] = s\n        hsv_map[:,:,2] = 255\n        hsv_map = cv.cvtColor(hsv_map, cv.COLOR_HSV2BGR)",
        "detail": "Hw_2.opencv.samples.python.color_histogram",
        "documentation": {}
    },
    {
        "label": "Bunch",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "class Bunch(object):\n    def __init__(self, **kw):\n        self.__dict__.update(kw)\n    def __str__(self):\n        return str(self.__dict__)\ndef splitfn(fn):\n    path, fn = os.path.split(fn)\n    name, ext = os.path.splitext(fn)\n    return path, name, ext\ndef anorm2(a):",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "Sketcher",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "class Sketcher:\n    def __init__(self, windowname, dests, colors_func):\n        self.prev_pt = None\n        self.windowname = windowname\n        self.dests = dests\n        self.colors_func = colors_func\n        self.dirty = False\n        self.show()\n        cv.setMouseCallback(self.windowname, self.on_mouse)\n    def show(self):",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "StatValue",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "class StatValue:\n    def __init__(self, smooth_coef = 0.5):\n        self.value = None\n        self.smooth_coef = smooth_coef\n    def update(self, v):\n        if self.value is None:\n            self.value = v\n        else:\n            c = self.smooth_coef\n            self.value = c * self.value + (1.0-c) * v",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "RectSelector",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "class RectSelector:\n    def __init__(self, win, callback):\n        self.win = win\n        self.callback = callback\n        cv.setMouseCallback(win, self.onmouse)\n        self.drag_start = None\n        self.drag_rect = None\n    def onmouse(self, event, x, y, flags, param):\n        x, y = np.int16([x, y]) # BUG\n        if event == cv.EVENT_LBUTTONDOWN:",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "splitfn",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def splitfn(fn):\n    path, fn = os.path.split(fn)\n    name, ext = os.path.splitext(fn)\n    return path, name, ext\ndef anorm2(a):\n    return (a*a).sum(-1)\ndef anorm(a):\n    return np.sqrt( anorm2(a) )\ndef homotrans(H, x, y):\n    xs = H[0, 0]*x + H[0, 1]*y + H[0, 2]",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "anorm2",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def anorm2(a):\n    return (a*a).sum(-1)\ndef anorm(a):\n    return np.sqrt( anorm2(a) )\ndef homotrans(H, x, y):\n    xs = H[0, 0]*x + H[0, 1]*y + H[0, 2]\n    ys = H[1, 0]*x + H[1, 1]*y + H[1, 2]\n    s  = H[2, 0]*x + H[2, 1]*y + H[2, 2]\n    return xs/s, ys/s\ndef to_rect(a):",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "anorm",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def anorm(a):\n    return np.sqrt( anorm2(a) )\ndef homotrans(H, x, y):\n    xs = H[0, 0]*x + H[0, 1]*y + H[0, 2]\n    ys = H[1, 0]*x + H[1, 1]*y + H[1, 2]\n    s  = H[2, 0]*x + H[2, 1]*y + H[2, 2]\n    return xs/s, ys/s\ndef to_rect(a):\n    a = np.ravel(a)\n    if len(a) == 2:",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "homotrans",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def homotrans(H, x, y):\n    xs = H[0, 0]*x + H[0, 1]*y + H[0, 2]\n    ys = H[1, 0]*x + H[1, 1]*y + H[1, 2]\n    s  = H[2, 0]*x + H[2, 1]*y + H[2, 2]\n    return xs/s, ys/s\ndef to_rect(a):\n    a = np.ravel(a)\n    if len(a) == 2:\n        a = (0, 0, a[0], a[1])\n    return np.array(a, np.float64).reshape(2, 2)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "to_rect",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def to_rect(a):\n    a = np.ravel(a)\n    if len(a) == 2:\n        a = (0, 0, a[0], a[1])\n    return np.array(a, np.float64).reshape(2, 2)\ndef rect2rect_mtx(src, dst):\n    src, dst = to_rect(src), to_rect(dst)\n    cx, cy = (dst[1] - dst[0]) / (src[1] - src[0])\n    tx, ty = dst[0] - src[0] * (cx, cy)\n    M = np.float64([[ cx,  0, tx],",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "rect2rect_mtx",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def rect2rect_mtx(src, dst):\n    src, dst = to_rect(src), to_rect(dst)\n    cx, cy = (dst[1] - dst[0]) / (src[1] - src[0])\n    tx, ty = dst[0] - src[0] * (cx, cy)\n    M = np.float64([[ cx,  0, tx],\n                    [  0, cy, ty],\n                    [  0,  0,  1]])\n    return M\ndef lookat(eye, target, up = (0, 0, 1)):\n    fwd = np.asarray(target, np.float64) - eye",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "lookat",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def lookat(eye, target, up = (0, 0, 1)):\n    fwd = np.asarray(target, np.float64) - eye\n    fwd /= anorm(fwd)\n    right = np.cross(fwd, up)\n    right /= anorm(right)\n    down = np.cross(fwd, right)\n    R = np.float64([right, down, fwd])\n    tvec = -np.dot(R, eye)\n    return R, tvec\ndef mtx2rvec(R):",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "mtx2rvec",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def mtx2rvec(R):\n    w, u, vt = cv.SVDecomp(R - np.eye(3))\n    p = vt[0] + u[:,0]*w[0]    # same as np.dot(R, vt[0])\n    c = np.dot(vt[0], p)\n    s = np.dot(vt[1], p)\n    axis = np.cross(vt[0], vt[1])\n    return axis * np.arctan2(s, c)\ndef draw_str(dst, target, s):\n    x, y = target\n    cv.putText(dst, s, (x+1, y+1), cv.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 0), thickness = 2, lineType=cv.LINE_AA)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "draw_str",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def draw_str(dst, target, s):\n    x, y = target\n    cv.putText(dst, s, (x+1, y+1), cv.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 0), thickness = 2, lineType=cv.LINE_AA)\n    cv.putText(dst, s, (x, y), cv.FONT_HERSHEY_PLAIN, 1.0, (255, 255, 255), lineType=cv.LINE_AA)\nclass Sketcher:\n    def __init__(self, windowname, dests, colors_func):\n        self.prev_pt = None\n        self.windowname = windowname\n        self.dests = dests\n        self.colors_func = colors_func",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "make_cmap",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def make_cmap(name, n=256):\n    data = cmap_data[name]\n    xs = np.linspace(0.0, 1.0, n)\n    channels = []\n    eps = 1e-6\n    for ch_name in ['blue', 'green', 'red']:\n        ch_data = data[ch_name]\n        xp, yp = [], []\n        for x, y1, y2 in ch_data:\n            xp += [x, x+eps]",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "nothing",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def nothing(*arg, **kw):\n    pass\ndef clock():\n    return cv.getTickCount() / cv.getTickFrequency()\n@contextmanager\ndef Timer(msg):\n    print(msg, '...',)\n    start = clock()\n    try:\n        yield",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "clock",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def clock():\n    return cv.getTickCount() / cv.getTickFrequency()\n@contextmanager\ndef Timer(msg):\n    print(msg, '...',)\n    start = clock()\n    try:\n        yield\n    finally:\n        print(\"%.2f ms\" % ((clock()-start)*1000))",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def Timer(msg):\n    print(msg, '...',)\n    start = clock()\n    try:\n        yield\n    finally:\n        print(\"%.2f ms\" % ((clock()-start)*1000))\nclass StatValue:\n    def __init__(self, smooth_coef = 0.5):\n        self.value = None",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "grouper",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def grouper(n, iterable, fillvalue=None):\n    '''grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx'''\n    args = [iter(iterable)] * n\n    if PY3:\n        output = it.zip_longest(fillvalue=fillvalue, *args)\n    else:\n        output = it.izip_longest(fillvalue=fillvalue, *args)\n    return output\ndef mosaic(w, imgs):\n    '''Make a grid from images.",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "mosaic",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def mosaic(w, imgs):\n    '''Make a grid from images.\n    w    -- number of grid columns\n    imgs -- images (must have same size and format)\n    '''\n    imgs = iter(imgs)\n    if PY3:\n        img0 = next(imgs)\n    else:\n        img0 = imgs.next()",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "getsize",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def getsize(img):\n    h, w = img.shape[:2]\n    return w, h\ndef mdot(*args):\n    return reduce(np.dot, args)\ndef draw_keypoints(vis, keypoints, color = (0, 255, 255)):\n    for kp in keypoints:\n        x, y = kp.pt\n        cv.circle(vis, (int(x), int(y)), 2, color)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "mdot",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def mdot(*args):\n    return reduce(np.dot, args)\ndef draw_keypoints(vis, keypoints, color = (0, 255, 255)):\n    for kp in keypoints:\n        x, y = kp.pt\n        cv.circle(vis, (int(x), int(y)), 2, color)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "draw_keypoints",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "def draw_keypoints(vis, keypoints, color = (0, 255, 255)):\n    for kp in keypoints:\n        x, y = kp.pt\n        cv.circle(vis, (int(x), int(y)), 2, color)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    from functools import reduce\nimport numpy as np\nimport cv2 as cv\n# built-in modules\nimport os\nimport itertools as it\nfrom contextlib import contextmanager\nimage_extensions = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.pbm', '.pgm', '.ppm']",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "image_extensions",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "image_extensions = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.pbm', '.pgm', '.ppm']\nclass Bunch(object):\n    def __init__(self, **kw):\n        self.__dict__.update(kw)\n    def __str__(self):\n        return str(self.__dict__)\ndef splitfn(fn):\n    path, fn = os.path.split(fn)\n    name, ext = os.path.splitext(fn)\n    return path, name, ext",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "_jet_data",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "_jet_data =   {'red':   ((0., 0, 0), (0.35, 0, 0), (0.66, 1, 1), (0.89,1, 1),\n                         (1, 0.5, 0.5)),\n               'green': ((0., 0, 0), (0.125,0, 0), (0.375,1, 1), (0.64,1, 1),\n                         (0.91,0,0), (1, 0, 0)),\n               'blue':  ((0., 0.5, 0.5), (0.11, 1, 1), (0.34, 1, 1), (0.65,0, 0),\n                         (1, 0, 0))}\ncmap_data = { 'jet' : _jet_data }\ndef make_cmap(name, n=256):\n    data = cmap_data[name]\n    xs = np.linspace(0.0, 1.0, n)",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "cmap_data",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.common",
        "description": "Hw_2.opencv.samples.python.common",
        "peekOfCode": "cmap_data = { 'jet' : _jet_data }\ndef make_cmap(name, n=256):\n    data = cmap_data[name]\n    xs = np.linspace(0.0, 1.0, n)\n    channels = []\n    eps = 1e-6\n    for ch_name in ['blue', 'green', 'red']:\n        ch_data = data[ch_name]\n        xp, yp = [], []\n        for x, y1, y2 in ch_data:",
        "detail": "Hw_2.opencv.samples.python.common",
        "documentation": {}
    },
    {
        "label": "make_image",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.contours",
        "description": "Hw_2.opencv.samples.python.contours",
        "peekOfCode": "def make_image():\n    img = np.zeros((500, 500), np.uint8)\n    black, white = 0, 255\n    for i in xrange(6):\n        dx = int((i%2)*250 - 30)\n        dy = int((i/2.)*150)\n        if i == 0:\n            for j in xrange(11):\n                angle = (j+5)*np.pi/21\n                c, s = np.cos(angle), np.sin(angle)",
        "detail": "Hw_2.opencv.samples.python.contours",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.contours",
        "description": "Hw_2.opencv.samples.python.contours",
        "peekOfCode": "def main():\n    img = make_image()\n    h, w = img.shape[:2]\n    contours0, hierarchy = cv.findContours( img.copy(), cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    contours = [cv.approxPolyDP(cnt, 3, True) for cnt in contours0]\n    def update(levels):\n        vis = np.zeros((h, w, 3), np.uint8)\n        levels = levels - 3\n        cv.drawContours( vis, contours, (-1, 2)[levels <= 0], (128,255,255),\n            3, cv.LINE_AA, hierarchy, abs(levels) )",
        "detail": "Hw_2.opencv.samples.python.contours",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.contours",
        "description": "Hw_2.opencv.samples.python.contours",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\ndef make_image():\n    img = np.zeros((500, 500), np.uint8)\n    black, white = 0, 255\n    for i in xrange(6):\n        dx = int((i%2)*250 - 30)",
        "detail": "Hw_2.opencv.samples.python.contours",
        "documentation": {}
    },
    {
        "label": "blur_edge",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.deconvolution",
        "description": "Hw_2.opencv.samples.python.deconvolution",
        "peekOfCode": "def blur_edge(img, d=31):\n    h, w  = img.shape[:2]\n    img_pad = cv.copyMakeBorder(img, d, d, d, d, cv.BORDER_WRAP)\n    img_blur = cv.GaussianBlur(img_pad, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n    y, x = np.indices((h, w))\n    dist = np.dstack([x, w-x-1, y, h-y-1]).min(-1)\n    w = np.minimum(np.float32(dist)/d, 1.0)\n    return img*w + img_blur*(1-w)\ndef motion_kernel(angle, d, sz=65):\n    kern = np.ones((1, d), np.float32)",
        "detail": "Hw_2.opencv.samples.python.deconvolution",
        "documentation": {}
    },
    {
        "label": "motion_kernel",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.deconvolution",
        "description": "Hw_2.opencv.samples.python.deconvolution",
        "peekOfCode": "def motion_kernel(angle, d, sz=65):\n    kern = np.ones((1, d), np.float32)\n    c, s = np.cos(angle), np.sin(angle)\n    A = np.float32([[c, -s, 0], [s, c, 0]])\n    sz2 = sz // 2\n    A[:,2] = (sz2, sz2) - np.dot(A[:,:2], ((d-1)*0.5, 0))\n    kern = cv.warpAffine(kern, A, (sz, sz), flags=cv.INTER_CUBIC)\n    return kern\ndef defocus_kernel(d, sz=65):\n    kern = np.zeros((sz, sz), np.uint8)",
        "detail": "Hw_2.opencv.samples.python.deconvolution",
        "documentation": {}
    },
    {
        "label": "defocus_kernel",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.deconvolution",
        "description": "Hw_2.opencv.samples.python.deconvolution",
        "peekOfCode": "def defocus_kernel(d, sz=65):\n    kern = np.zeros((sz, sz), np.uint8)\n    cv.circle(kern, (sz, sz), d, 255, -1, cv.LINE_AA, shift=1)\n    kern = np.float32(kern) / 255.0\n    return kern\ndef main():\n    import sys, getopt\n    opts, args = getopt.getopt(sys.argv[1:], '', ['circle', 'angle=', 'd=', 'snr='])\n    opts = dict(opts)\n    try:",
        "detail": "Hw_2.opencv.samples.python.deconvolution",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.deconvolution",
        "description": "Hw_2.opencv.samples.python.deconvolution",
        "peekOfCode": "def main():\n    import sys, getopt\n    opts, args = getopt.getopt(sys.argv[1:], '', ['circle', 'angle=', 'd=', 'snr='])\n    opts = dict(opts)\n    try:\n        fn = args[0]\n    except:\n        fn = 'licenseplate_motion.jpg'\n    win = 'deconvolution'\n    img = cv.imread(cv.samples.findFile(fn), cv.IMREAD_GRAYSCALE)",
        "detail": "Hw_2.opencv.samples.python.deconvolution",
        "documentation": {}
    },
    {
        "label": "LinkManager",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.demo",
        "description": "Hw_2.opencv.samples.python.demo",
        "peekOfCode": "class LinkManager:\n    def __init__(self, text, url_callback = None):\n        self.text = text\n        self.text.tag_config(\"link\", foreground=\"blue\", underline=1)\n        self.text.tag_bind(\"link\", \"<Enter>\", self._enter)\n        self.text.tag_bind(\"link\", \"<Leave>\", self._leave)\n        self.text.tag_bind(\"link\", \"<Button-1>\", self._click)\n        self.url_callback = url_callback\n        self.reset()\n    def reset(self):",
        "detail": "Hw_2.opencv.samples.python.demo",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.demo",
        "description": "Hw_2.opencv.samples.python.demo",
        "peekOfCode": "class App:\n    def __init__(self):\n        root = tk.Tk()\n        root.title('OpenCV Demo')\n        self.win = win = tk.PanedWindow(root, orient=tk.HORIZONTAL, sashrelief=tk.RAISED, sashwidth=4)\n        self.win.pack(fill=tk.BOTH, expand=1)\n        left = tk.Frame(win)\n        right = tk.Frame(win)\n        win.add(left)\n        win.add(right)",
        "detail": "Hw_2.opencv.samples.python.demo",
        "documentation": {}
    },
    {
        "label": "#ipshell",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.demo",
        "description": "Hw_2.opencv.samples.python.demo",
        "peekOfCode": "#ipshell = IPShellEmbed()\nexclude_list = ['demo', 'common']\nclass LinkManager:\n    def __init__(self, text, url_callback = None):\n        self.text = text\n        self.text.tag_config(\"link\", foreground=\"blue\", underline=1)\n        self.text.tag_bind(\"link\", \"<Enter>\", self._enter)\n        self.text.tag_bind(\"link\", \"<Leave>\", self._leave)\n        self.text.tag_bind(\"link\", \"<Button-1>\", self._click)\n        self.url_callback = url_callback",
        "detail": "Hw_2.opencv.samples.python.demo",
        "documentation": {}
    },
    {
        "label": "exclude_list",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.demo",
        "description": "Hw_2.opencv.samples.python.demo",
        "peekOfCode": "exclude_list = ['demo', 'common']\nclass LinkManager:\n    def __init__(self, text, url_callback = None):\n        self.text = text\n        self.text.tag_config(\"link\", foreground=\"blue\", underline=1)\n        self.text.tag_bind(\"link\", \"<Enter>\", self._enter)\n        self.text.tag_bind(\"link\", \"<Leave>\", self._leave)\n        self.text.tag_bind(\"link\", \"<Button-1>\", self._click)\n        self.url_callback = url_callback\n        self.reset()",
        "detail": "Hw_2.opencv.samples.python.demo",
        "documentation": {}
    },
    {
        "label": "shift_dft",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dft",
        "description": "Hw_2.opencv.samples.python.dft",
        "peekOfCode": "def shift_dft(src, dst=None):\n    '''\n        Rearrange the quadrants of Fourier image so that the origin is at\n        the image center. Swaps quadrant 1 with 3, and 2 with 4.\n        src and dst arrays must be equal size & type\n    '''\n    if dst is None:\n        dst = np.empty(src.shape, src.dtype)\n    elif src.shape != dst.shape:\n        raise ValueError(\"src and dst must have equal sizes\")",
        "detail": "Hw_2.opencv.samples.python.dft",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dft",
        "description": "Hw_2.opencv.samples.python.dft",
        "peekOfCode": "def main():\n    if len(sys.argv) > 1:\n        fname = sys.argv[1]\n    else:\n        fname = 'baboon.jpg'\n        print(\"usage : python dft.py <image_file>\")\n    im = cv.imread(cv.samples.findFile(fname))\n    # convert to grayscale\n    im = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\n    h, w = im.shape[:2]",
        "detail": "Hw_2.opencv.samples.python.dft",
        "documentation": {}
    },
    {
        "label": "KNearest",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "class KNearest(object):\n    def __init__(self, k = 3):\n        self.k = k\n        self.model = cv.ml.KNearest_create()\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, self.k)\n        return results.ravel()\n    def load(self, fn):",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "SVM",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "class SVM(object):\n    def __init__(self, C = 1, gamma = 0.5):\n        self.model = cv.ml.SVM_create()\n        self.model.setGamma(gamma)\n        self.model.setC(C)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setType(cv.ml.SVM_C_SVC)\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "split2d",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells\ndef load_digits(fn):\n    fn = cv.samples.findFile(fn)",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "load_digits",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def load_digits(fn):\n    fn = cv.samples.findFile(fn)\n    print('loading \"%s\" ...' % fn)\n    digits_img = cv.imread(fn, cv.IMREAD_GRAYSCALE)\n    digits = split2d(digits_img, (SZ, SZ))\n    labels = np.repeat(np.arange(CLASS_N), len(digits)/CLASS_N)\n    return digits, labels\ndef deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "deskew",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def deskew(img):\n    m = cv.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv.warpAffine(img, M, (SZ, SZ), flags=cv.WARP_INVERSE_MAP | cv.INTER_LINEAR)\n    return img\nclass KNearest(object):\n    def __init__(self, k = 3):",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def evaluate_model(model, digits, samples, labels):\n    resp = model.predict(samples)\n    err = (labels != resp).mean()\n    print('error: %.2f %%' % (err*100))\n    confusion = np.zeros((10, 10), np.int32)\n    for i, j in zip(labels, resp):\n        confusion[i, int(j)] += 1\n    print('confusion matrix:')\n    print(confusion)\n    print()",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "preprocess_simple",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def preprocess_simple(digits):\n    return np.float32(digits).reshape(-1, SZ*SZ) / 255.0\ndef preprocess_hog(digits):\n    samples = []\n    for img in digits:\n        gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n        gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n        mag, ang = cv.cartToPolar(gx, gy)\n        bin_n = 16\n        bin = np.int32(bin_n*ang/(2*np.pi))",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "preprocess_hog",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "def preprocess_hog(digits):\n    samples = []\n    for img in digits:\n        gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n        gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n        mag, ang = cv.cartToPolar(gx, gy)\n        bin_n = 16\n        bin = np.int32(bin_n*ang/(2*np.pi))\n        bin_cells = bin[:10,:10], bin[10:,:10], bin[:10,10:], bin[10:,10:]\n        mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "SZ",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "SZ = 20 # size of each digit is SZ x SZ\nCLASS_N = 10\nDIGITS_FN = 'digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "CLASS_N",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "CLASS_N = 10\nDIGITS_FN = 'digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "DIGITS_FN",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.digits",
        "description": "Hw_2.opencv.samples.python.digits",
        "peekOfCode": "DIGITS_FN = 'digits.png'\ndef split2d(img, cell_size, flatten=True):\n    h, w = img.shape[:2]\n    sx, sy = cell_size\n    cells = [np.hsplit(row, w//sx) for row in np.vsplit(img, h//sy)]\n    cells = np.array(cells)\n    if flatten:\n        cells = cells.reshape(-1, sy, sx)\n    return cells\ndef load_digits(fn):",
        "detail": "Hw_2.opencv.samples.python.digits",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.digits_adjust",
        "description": "Hw_2.opencv.samples.python.digits_adjust",
        "peekOfCode": "class App(object):\n    def __init__(self):\n        self._samples, self._labels = self.preprocess()\n    def preprocess(self):\n        digits, labels = load_digits(DIGITS_FN)\n        shuffle = np.random.permutation(len(digits))\n        digits, labels = digits[shuffle], labels[shuffle]\n        digits2 = list(map(deskew, digits))\n        samples = preprocess_hog(digits2)\n        return samples, labels",
        "detail": "Hw_2.opencv.samples.python.digits_adjust",
        "documentation": {}
    },
    {
        "label": "cross_validate",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits_adjust",
        "description": "Hw_2.opencv.samples.python.digits_adjust",
        "peekOfCode": "def cross_validate(model_class, params, samples, labels, kfold = 3, pool = None):\n    n = len(samples)\n    folds = np.array_split(np.arange(n), kfold)\n    def f(i):\n        model = model_class(**params)\n        test_idx = folds[i]\n        train_idx = list(folds)\n        train_idx.pop(i)\n        train_idx = np.hstack(train_idx)\n        train_samples, train_labels = samples[train_idx], labels[train_idx]",
        "detail": "Hw_2.opencv.samples.python.digits_adjust",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.digits_adjust",
        "description": "Hw_2.opencv.samples.python.digits_adjust",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nfrom multiprocessing.pool import ThreadPool\nfrom digits import *\ndef cross_validate(model_class, params, samples, labels, kfold = 3, pool = None):\n    n = len(samples)\n    folds = np.array_split(np.arange(n), kfold)",
        "detail": "Hw_2.opencv.samples.python.digits_adjust",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.digits_video",
        "description": "Hw_2.opencv.samples.python.digits_video",
        "peekOfCode": "def main():\n    try:\n        src = sys.argv[1]\n    except:\n        src = 0\n    cap = video.create_capture(src, fallback='synth:bg={}:noise=0.05'.format(cv.samples.findFile('sudoku.png')))\n    classifier_fn = 'digits_svm.dat'\n    if not os.path.exists(classifier_fn):\n        print('\"%s\" not found, run digits.py first' % classifier_fn)\n        return",
        "detail": "Hw_2.opencv.samples.python.digits_video",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.distrans",
        "description": "Hw_2.opencv.samples.python.distrans",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 'fruits.jpg'\n    fn = cv.samples.findFile(fn)\n    img = cv.imread(fn, cv.IMREAD_GRAYSCALE)\n    if img is None:\n        print('Failed to load fn:', fn)",
        "detail": "Hw_2.opencv.samples.python.distrans",
        "documentation": {}
    },
    {
        "label": "draw_flow",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dis_opt_flow",
        "description": "Hw_2.opencv.samples.python.dis_opt_flow",
        "peekOfCode": "def draw_flow(img, flow, step=16):\n    h, w = img.shape[:2]\n    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n    fx, fy = flow[y,x].T\n    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n    lines = np.int32(lines + 0.5)\n    vis = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n    cv.polylines(vis, lines, 0, (0, 255, 0))\n    for (x1, y1), (_x2, _y2) in lines:\n        cv.circle(vis, (x1, y1), 1, (0, 255, 0), -1)",
        "detail": "Hw_2.opencv.samples.python.dis_opt_flow",
        "documentation": {}
    },
    {
        "label": "draw_hsv",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dis_opt_flow",
        "description": "Hw_2.opencv.samples.python.dis_opt_flow",
        "peekOfCode": "def draw_hsv(flow):\n    h, w = flow.shape[:2]\n    fx, fy = flow[:,:,0], flow[:,:,1]\n    ang = np.arctan2(fy, fx) + np.pi\n    v = np.sqrt(fx*fx+fy*fy)\n    hsv = np.zeros((h, w, 3), np.uint8)\n    hsv[...,0] = ang*(180/np.pi/2)\n    hsv[...,1] = 255\n    hsv[...,2] = np.minimum(v*4, 255)\n    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)",
        "detail": "Hw_2.opencv.samples.python.dis_opt_flow",
        "documentation": {}
    },
    {
        "label": "warp_flow",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dis_opt_flow",
        "description": "Hw_2.opencv.samples.python.dis_opt_flow",
        "peekOfCode": "def warp_flow(img, flow):\n    h, w = flow.shape[:2]\n    flow = -flow\n    flow[:,:,0] += np.arange(w)\n    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n    res = cv.remap(img, flow, None, cv.INTER_LINEAR)\n    return res\ndef main():\n    import sys\n    print(__doc__)",
        "detail": "Hw_2.opencv.samples.python.dis_opt_flow",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.dis_opt_flow",
        "description": "Hw_2.opencv.samples.python.dis_opt_flow",
        "peekOfCode": "def main():\n    import sys\n    print(__doc__)\n    try:\n        fn = sys.argv[1]\n    except IndexError:\n        fn = 0\n    cam = video.create_capture(fn)\n    _ret, prev = cam.read()\n    prevgray = cv.cvtColor(prev, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.dis_opt_flow",
        "documentation": {}
    },
    {
        "label": "lines",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def lines():\n    for i in range(NUMBER*2):\n        pt1, pt2 = [], []\n        pt1.append(np.random.randint(x1, x2))\n        pt1.append(np.random.randint(y1, y2))\n        pt2.append(np.random.randint(x1, x2))\n        pt2.append(np.random.randint(y1, y2))\n        color = \"%06x\" % np.random.randint(0, 0xFFFFFF)\n        color = tuple(int(color[i:i+2], 16) for i in (0, 2 ,4))\n        arrowed =  np.random.randint(0, 6)",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "rectangle",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def rectangle():\n    for i in range(NUMBER*2):\n        pt1, pt2 = [], []\n        pt1.append(np.random.randint(x1, x2))\n        pt1.append(np.random.randint(y1, y2))\n        pt2.append(np.random.randint(x1, x2))\n        pt2.append(np.random.randint(y1, y2))\n        color = \"%06x\" % np.random.randint(0, 0xFFFFFF)\n        color = tuple(int(color[i:i+2], 16) for i in (0, 2 ,4))\n        thickness = np.random.randint(-3, 10)",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "ellipse",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def ellipse():\n    for i in range(NUMBER*2):\n        center = []\n        center.append(np.random.randint(x1, x2))\n        center.append(np.random.randint(x1, x2))\n        axes = []\n        axes.append(np.random.randint(0, 200))\n        axes.append(np.random.randint(0, 200))\n        angle = np.random.randint(0, 180)\n        color = \"%06x\" % np.random.randint(0, 0xFFFFFF)",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "polygonal",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def polygonal():\n    for i in range(NUMBER):\n        pt = [(0, 0)]*6\n        pt = np.resize(pt, (2, 3, 2))\n        pt[0][0][0] = np.random.randint(x1, x2)\n        pt[0][0][1] = np.random.randint(y1, y2)\n        pt[0][1][0] = np.random.randint(x1, x2)\n        pt[0][1][1] = np.random.randint(y1, y2)\n        pt[0][2][0] = np.random.randint(x1, x2)\n        pt[0][2][1] = np.random.randint(y1, y2)",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "fill",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def fill():\n    for i in range(NUMBER):\n        pt = [(0, 0)]*6\n        pt = np.resize(pt, (2, 3, 2))\n        pt[0][0][0] = np.random.randint(x1, x2)\n        pt[0][0][1] = np.random.randint(y1, y2)\n        pt[0][1][0] = np.random.randint(x1, x2)\n        pt[0][1][1] = np.random.randint(y1, y2)\n        pt[0][2][0] = np.random.randint(x1, x2)\n        pt[0][2][1] = np.random.randint(y1, y2)",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "circles",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def circles():\n    for i in range(NUMBER):\n        center = []\n        center.append(np.random.randint(x1, x2))\n        center.append(np.random.randint(x1, x2))\n        color = \"%06x\" % np.random.randint(0, 0xFFFFFF)\n        color = tuple(int(color[i:i+2], 16) for i in (0, 2 ,4))\n        cv.circle(image, tuple(center), np.random.randint(0, 300), color, np.random.randint(-1, 9), lineType)\n        cv.imshow(wndname, image)\n        if cv.waitKey(DELAY) >= 0:",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def string():\n    for i in range(NUMBER):\n        org = []\n        org.append(np.random.randint(x1, x2))\n        org.append(np.random.randint(x1, x2))\n        color = \"%06x\" % np.random.randint(0, 0xFFFFFF)\n        color = tuple(int(color[i:i+2], 16) for i in (0, 2 ,4))\n        cv.putText(image, \"Testing text rendering\", tuple(org), np.random.randint(0, 8), np.random.randint(0, 100)*0.05+0.1, color, np.random.randint(1, 10), lineType)\n        cv.imshow(wndname, image)\n        if cv.waitKey(DELAY) >= 0:",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "string1",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.drawing",
        "description": "Hw_2.opencv.samples.python.drawing",
        "peekOfCode": "def string1():\n    textsize = cv.getTextSize(\"OpenCV forever!\", cv.FONT_HERSHEY_COMPLEX, 3, 5)\n    org = (int((width - textsize[0][0])/2), int((height - textsize[0][1])/2))\n    for i in range(0, 255, 2):\n        image2 = np.array(image) - i\n        cv.putText(image2, \"OpenCV forever!\", org, cv.FONT_HERSHEY_COMPLEX, 3, (i, i, 255), 5, lineType)\n        cv.imshow(wndname, image2)\n        if cv.waitKey(DELAY) >= 0:\n            return\nif __name__ == '__main__':",
        "detail": "Hw_2.opencv.samples.python.drawing",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.edge",
        "description": "Hw_2.opencv.samples.python.edge",
        "peekOfCode": "def main():\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 0\n    def nothing(*arg):\n        pass\n    cv.namedWindow('edge')\n    cv.createTrackbar('thrs1', 'edge', 2000, 5000, nothing)\n    cv.createTrackbar('thrs2', 'edge', 4000, 5000, nothing)",
        "detail": "Hw_2.opencv.samples.python.edge",
        "documentation": {}
    },
    {
        "label": "getEpipolarError",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.essential_mat_reconstr",
        "description": "Hw_2.opencv.samples.python.essential_mat_reconstr",
        "peekOfCode": "def getEpipolarError(F, pts1_, pts2_, inliers):\n    pts1 = np.concatenate((pts1_.T, np.ones((1, pts1_.shape[0]))))[:,inliers]\n    pts2 = np.concatenate((pts2_.T, np.ones((1, pts2_.shape[0]))))[:,inliers]\n    lines2 = np.dot(F  , pts1)\n    lines1 = np.dot(F.T, pts2)\n    return np.median((np.abs(np.sum(pts1 * lines1, axis=0)) / np.sqrt(lines1[0,:]**2 + lines1[1,:]**2) +\n                      np.abs(np.sum(pts2 * lines2, axis=0)) / np.sqrt(lines2[0,:]**2 + lines2[1,:]**2))/2)\nif __name__ == '__main__':\n    if len(sys.argv) < 3:\n        print(\"Path to data file and directory to image files are missing!\\nData file must have\"",
        "detail": "Hw_2.opencv.samples.python.essential_mat_reconstr",
        "documentation": {}
    },
    {
        "label": "detect",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.facedetect",
        "description": "Hw_2.opencv.samples.python.facedetect",
        "peekOfCode": "def detect(img, cascade):\n    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30),\n                                     flags=cv.CASCADE_SCALE_IMAGE)\n    if len(rects) == 0:\n        return []\n    rects[:,2:] += rects[:,:2]\n    return rects\ndef draw_rects(img, rects, color):\n    for x1, y1, x2, y2 in rects:\n        cv.rectangle(img, (x1, y1), (x2, y2), color, 2)",
        "detail": "Hw_2.opencv.samples.python.facedetect",
        "documentation": {}
    },
    {
        "label": "draw_rects",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.facedetect",
        "description": "Hw_2.opencv.samples.python.facedetect",
        "peekOfCode": "def draw_rects(img, rects, color):\n    for x1, y1, x2, y2 in rects:\n        cv.rectangle(img, (x1, y1), (x2, y2), color, 2)\ndef main():\n    import sys, getopt\n    args, video_src = getopt.getopt(sys.argv[1:], '', ['cascade=', 'nested-cascade='])\n    try:\n        video_src = video_src[0]\n    except:\n        video_src = 0",
        "detail": "Hw_2.opencv.samples.python.facedetect",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.facedetect",
        "description": "Hw_2.opencv.samples.python.facedetect",
        "peekOfCode": "def main():\n    import sys, getopt\n    args, video_src = getopt.getopt(sys.argv[1:], '', ['cascade=', 'nested-cascade='])\n    try:\n        video_src = video_src[0]\n    except:\n        video_src = 0\n    args = dict(args)\n    cascade_fn = args.get('--cascade', \"haarcascades/haarcascade_frontalface_alt.xml\")\n    nested_fn  = args.get('--nested-cascade', \"haarcascades/haarcascade_eye.xml\")",
        "detail": "Hw_2.opencv.samples.python.facedetect",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.feature_homography",
        "description": "Hw_2.opencv.samples.python.feature_homography",
        "peekOfCode": "class App:\n    def __init__(self, src):\n        self.cap = video.create_capture(src, presets['book'])\n        self.frame = None\n        self.paused = False\n        self.tracker = PlaneTracker()\n        cv.namedWindow('plane')\n        self.rect_sel = common.RectSelector('plane', self.on_rect)\n    def on_rect(self, rect):\n        self.tracker.clear()",
        "detail": "Hw_2.opencv.samples.python.feature_homography",
        "documentation": {}
    },
    {
        "label": "init_feature",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.find_obj",
        "description": "Hw_2.opencv.samples.python.find_obj",
        "peekOfCode": "def init_feature(name):\n    chunks = name.split('-')\n    if chunks[0] == 'sift':\n        detector = cv.SIFT_create()\n        norm = cv.NORM_L2\n    elif chunks[0] == 'surf':\n        detector = cv.xfeatures2d.SURF_create(800)\n        norm = cv.NORM_L2\n    elif chunks[0] == 'orb':\n        detector = cv.ORB_create(400)",
        "detail": "Hw_2.opencv.samples.python.find_obj",
        "documentation": {}
    },
    {
        "label": "filter_matches",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.find_obj",
        "description": "Hw_2.opencv.samples.python.find_obj",
        "peekOfCode": "def filter_matches(kp1, kp2, matches, ratio = 0.75):\n    mkp1, mkp2 = [], []\n    for m in matches:\n        if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n            m = m[0]\n            mkp1.append( kp1[m.queryIdx] )\n            mkp2.append( kp2[m.trainIdx] )\n    p1 = np.float32([kp.pt for kp in mkp1])\n    p2 = np.float32([kp.pt for kp in mkp2])\n    kp_pairs = zip(mkp1, mkp2)",
        "detail": "Hw_2.opencv.samples.python.find_obj",
        "documentation": {}
    },
    {
        "label": "explore_match",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.find_obj",
        "description": "Hw_2.opencv.samples.python.find_obj",
        "peekOfCode": "def explore_match(win, img1, img2, kp_pairs, status = None, H = None):\n    h1, w1 = img1.shape[:2]\n    h2, w2 = img2.shape[:2]\n    vis = np.zeros((max(h1, h2), w1+w2), np.uint8)\n    vis[:h1, :w1] = img1\n    vis[:h2, w1:w1+w2] = img2\n    vis = cv.cvtColor(vis, cv.COLOR_GRAY2BGR)\n    if H is not None:\n        corners = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]])\n        corners = np.int32( cv.perspectiveTransform(corners.reshape(1, -1, 2), H).reshape(-1, 2) + (w1, 0) )",
        "detail": "Hw_2.opencv.samples.python.find_obj",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.find_obj",
        "description": "Hw_2.opencv.samples.python.find_obj",
        "peekOfCode": "def main():\n    import sys, getopt\n    opts, args = getopt.getopt(sys.argv[1:], '', ['feature='])\n    opts = dict(opts)\n    feature_name = opts.get('--feature', 'brisk')\n    try:\n        fn1, fn2 = args\n    except:\n        fn1 = 'box.png'\n        fn2 = 'box_in_scene.png'",
        "detail": "Hw_2.opencv.samples.python.find_obj",
        "documentation": {}
    },
    {
        "label": "FLANN_INDEX_KDTREE",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.find_obj",
        "description": "Hw_2.opencv.samples.python.find_obj",
        "peekOfCode": "FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing\nFLANN_INDEX_LSH    = 6\ndef init_feature(name):\n    chunks = name.split('-')\n    if chunks[0] == 'sift':\n        detector = cv.SIFT_create()\n        norm = cv.NORM_L2\n    elif chunks[0] == 'surf':\n        detector = cv.xfeatures2d.SURF_create(800)\n        norm = cv.NORM_L2",
        "detail": "Hw_2.opencv.samples.python.find_obj",
        "documentation": {}
    },
    {
        "label": "toint",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "def toint(p):\n    return tuple(map(int, p))\ndef sample_line(p1, p2, n, noise=0.0):\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise\ndist_func_names = it.cycle('DIST_L2 DIST_L1 DIST_L12 DIST_FAIR DIST_WELSCH DIST_HUBER'.split())\nif PY3:\n    cur_func_name = next(dist_func_names)\nelse:",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "sample_line",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "def sample_line(p1, p2, n, noise=0.0):\n    p1 = np.float32(p1)\n    t = np.random.rand(n,1)\n    return p1 + (p2-p1)*t + np.random.normal(size=(n, 2))*noise\ndist_func_names = it.cycle('DIST_L2 DIST_L1 DIST_L12 DIST_FAIR DIST_WELSCH DIST_HUBER'.split())\nif PY3:\n    cur_func_name = next(dist_func_names)\nelse:\n    cur_func_name = dist_func_names.next()\ndef update(_=None):",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "update",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "def update(_=None):\n    noise = cv.getTrackbarPos('noise', 'fit line')\n    n = cv.getTrackbarPos('point n', 'fit line')\n    r = cv.getTrackbarPos('outlier %', 'fit line') / 100.0\n    outn = int(n*r)\n    p0, p1 = (90, 80), (w-90, h-80)\n    img = np.zeros((h, w, 3), np.uint8)\n    cv.line(img, toint(p0), toint(p1), (0, 255, 0))\n    if n > 0:\n        line_points = sample_line(p0, p1, n-outn, noise)",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "def main():\n    cv.namedWindow('fit line')\n    cv.createTrackbar('noise', 'fit line', 3, 50, update)\n    cv.createTrackbar('point n', 'fit line', 100, 500, update)\n    cv.createTrackbar('outlier %', 'fit line', 30, 100, update)\n    while True:\n        update()\n        ch = cv.waitKey(0)\n        if ch == ord('f'):\n            global cur_func_name",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nimport numpy as np\nimport cv2 as cv\n# built-in modules\nimport itertools as it\n# local modules\nfrom common import draw_str\nw, h = 512, 256\ndef toint(p):\n    return tuple(map(int, p))",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "dist_func_names",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.fitline",
        "description": "Hw_2.opencv.samples.python.fitline",
        "peekOfCode": "dist_func_names = it.cycle('DIST_L2 DIST_L1 DIST_L12 DIST_FAIR DIST_WELSCH DIST_HUBER'.split())\nif PY3:\n    cur_func_name = next(dist_func_names)\nelse:\n    cur_func_name = dist_func_names.next()\ndef update(_=None):\n    noise = cv.getTrackbarPos('noise', 'fit line')\n    n = cv.getTrackbarPos('point n', 'fit line')\n    r = cv.getTrackbarPos('outlier %', 'fit line') / 100.0\n    outn = int(n*r)",
        "detail": "Hw_2.opencv.samples.python.fitline",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.floodfill",
        "description": "Hw_2.opencv.samples.python.floodfill",
        "peekOfCode": "class App():\n    def update(self, dummy=None):\n        if self.seed_pt is None:\n            cv.imshow('floodfill', self.img)\n            return\n        flooded = self.img.copy()\n        self.mask[:] = 0\n        lo = cv.getTrackbarPos('lo', 'floodfill')\n        hi = cv.getTrackbarPos('hi', 'floodfill')\n        flags = self.connectivity",
        "detail": "Hw_2.opencv.samples.python.floodfill",
        "documentation": {}
    },
    {
        "label": "build_filters",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gabor_threads",
        "description": "Hw_2.opencv.samples.python.gabor_threads",
        "peekOfCode": "def build_filters():\n    filters = []\n    ksize = 31\n    for theta in np.arange(0, np.pi, np.pi / 16):\n        kern = cv.getGaborKernel((ksize, ksize), 4.0, theta, 10.0, 0.5, 0, ktype=cv.CV_32F)\n        kern /= 1.5*kern.sum()\n        filters.append(kern)\n    return filters\ndef process(img, filters):\n    accum = np.zeros_like(img)",
        "detail": "Hw_2.opencv.samples.python.gabor_threads",
        "documentation": {}
    },
    {
        "label": "process",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gabor_threads",
        "description": "Hw_2.opencv.samples.python.gabor_threads",
        "peekOfCode": "def process(img, filters):\n    accum = np.zeros_like(img)\n    for kern in filters:\n        fimg = cv.filter2D(img, cv.CV_8UC3, kern)\n        np.maximum(accum, fimg, accum)\n    return accum\ndef process_threaded(img, filters, threadn = 8):\n    accum = np.zeros_like(img)\n    def f(kern):\n        return cv.filter2D(img, cv.CV_8UC3, kern)",
        "detail": "Hw_2.opencv.samples.python.gabor_threads",
        "documentation": {}
    },
    {
        "label": "process_threaded",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gabor_threads",
        "description": "Hw_2.opencv.samples.python.gabor_threads",
        "peekOfCode": "def process_threaded(img, filters, threadn = 8):\n    accum = np.zeros_like(img)\n    def f(kern):\n        return cv.filter2D(img, cv.CV_8UC3, kern)\n    pool = ThreadPool(processes=threadn)\n    for fimg in pool.imap_unordered(f, filters):\n        np.maximum(accum, fimg, accum)\n    return accum\ndef main():\n    import sys",
        "detail": "Hw_2.opencv.samples.python.gabor_threads",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gabor_threads",
        "description": "Hw_2.opencv.samples.python.gabor_threads",
        "peekOfCode": "def main():\n    import sys\n    from common import Timer\n    try:\n        img_fn = sys.argv[1]\n    except:\n        img_fn = 'baboon.jpg'\n    img = cv.imread(cv.samples.findFile(img_fn))\n    if img is None:\n        print('Failed to load image file:', img_fn)",
        "detail": "Hw_2.opencv.samples.python.gabor_threads",
        "documentation": {}
    },
    {
        "label": "make_gaussians",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gaussian_mix",
        "description": "Hw_2.opencv.samples.python.gaussian_mix",
        "peekOfCode": "def make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _i in xrange(cluster_n):\n        mean = (0.1 + 0.8*random.rand(2)) * img_size\n        a = (random.rand(2, 2)-0.5)*img_size*0.1\n        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)\n        n = 100 + random.randint(900)\n        pts = random.multivariate_normal(mean, cov, n)\n        points.append( pts )",
        "detail": "Hw_2.opencv.samples.python.gaussian_mix",
        "documentation": {}
    },
    {
        "label": "draw_gaussain",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gaussian_mix",
        "description": "Hw_2.opencv.samples.python.gaussian_mix",
        "peekOfCode": "def draw_gaussain(img, mean, cov, color):\n    x, y = mean\n    w, u, _vt = cv.SVDecomp(cov)\n    ang = np.arctan2(u[1, 0], u[0, 0])*(180/np.pi)\n    s1, s2 = np.sqrt(w)*3.0\n    cv.ellipse(img, (int(x), int(y)), (int(s1), int(s2)), ang, 0, 360, color, 1, cv.LINE_AA)\ndef main():\n    cluster_n = 5\n    img_size = 512\n    print('press any key to update distributions, ESC - exit\\n')",
        "detail": "Hw_2.opencv.samples.python.gaussian_mix",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.gaussian_mix",
        "description": "Hw_2.opencv.samples.python.gaussian_mix",
        "peekOfCode": "def main():\n    cluster_n = 5\n    img_size = 512\n    print('press any key to update distributions, ESC - exit\\n')\n    while True:\n        print('sampling distributions...')\n        points, ref_distrs = make_gaussians(cluster_n, img_size)\n        print('EM (opencv) ...')\n        em = cv.ml.EM_create()\n        em.setClustersNumber(cluster_n)",
        "detail": "Hw_2.opencv.samples.python.gaussian_mix",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.gaussian_mix",
        "description": "Hw_2.opencv.samples.python.gaussian_mix",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nfrom numpy import random\ndef make_gaussians(cluster_n, img_size):\n    points = []\n    ref_distrs = []\n    for _i in xrange(cluster_n):",
        "detail": "Hw_2.opencv.samples.python.gaussian_mix",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.grabcut",
        "description": "Hw_2.opencv.samples.python.grabcut",
        "peekOfCode": "class App():\n    BLUE = [255,0,0]        # rectangle color\n    RED = [0,0,255]         # PR BG\n    GREEN = [0,255,0]       # PR FG\n    BLACK = [0,0,0]         # sure BG\n    WHITE = [255,255,255]   # sure FG\n    DRAW_BG = {'color' : BLACK, 'val' : 0}\n    DRAW_FG = {'color' : WHITE, 'val' : 1}\n    DRAW_PR_BG = {'color' : RED, 'val' : 2}\n    DRAW_PR_FG = {'color' : GREEN, 'val' : 3}",
        "detail": "Hw_2.opencv.samples.python.grabcut",
        "documentation": {}
    },
    {
        "label": "hist_curve",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.hist",
        "description": "Hw_2.opencv.samples.python.hist",
        "peekOfCode": "def hist_curve(im):\n    h = np.zeros((300,256,3))\n    if len(im.shape) == 2:\n        color = [(255,255,255)]\n    elif im.shape[2] == 3:\n        color = [ (255,0,0),(0,255,0),(0,0,255) ]\n    for ch, col in enumerate(color):\n        hist_item = cv.calcHist([im],[ch],None,[256],[0,256])\n        cv.normalize(hist_item,hist_item,0,255,cv.NORM_MINMAX)\n        hist=np.int32(np.around(hist_item))",
        "detail": "Hw_2.opencv.samples.python.hist",
        "documentation": {}
    },
    {
        "label": "hist_lines",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.hist",
        "description": "Hw_2.opencv.samples.python.hist",
        "peekOfCode": "def hist_lines(im):\n    h = np.zeros((300,256,3))\n    if len(im.shape)!=2:\n        print(\"hist_lines applicable only for grayscale images\")\n        #print(\"so converting image to grayscale for representation\"\n        im = cv.cvtColor(im,cv.COLOR_BGR2GRAY)\n    hist_item = cv.calcHist([im],[0],None,[256],[0,256])\n    cv.normalize(hist_item,hist_item,0,255,cv.NORM_MINMAX)\n    hist = np.int32(np.around(hist_item))\n    for x,y in enumerate(hist):",
        "detail": "Hw_2.opencv.samples.python.hist",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.hist",
        "description": "Hw_2.opencv.samples.python.hist",
        "peekOfCode": "def main():\n    import sys\n    if len(sys.argv)>1:\n        fname = sys.argv[1]\n    else :\n        fname = 'lena.jpg'\n        print(\"usage : python hist.py <image_file>\")\n    im = cv.imread(cv.samples.findFile(fname))\n    if im is None:\n        print('Failed to load image file:', fname)",
        "detail": "Hw_2.opencv.samples.python.hist",
        "documentation": {}
    },
    {
        "label": "bins",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.hist",
        "description": "Hw_2.opencv.samples.python.hist",
        "peekOfCode": "bins = np.arange(256).reshape(256,1)\ndef hist_curve(im):\n    h = np.zeros((300,256,3))\n    if len(im.shape) == 2:\n        color = [(255,255,255)]\n    elif im.shape[2] == 3:\n        color = [ (255,0,0),(0,255,0),(0,0,255) ]\n    for ch, col in enumerate(color):\n        hist_item = cv.calcHist([im],[ch],None,[256],[0,256])\n        cv.normalize(hist_item,hist_item,0,255,cv.NORM_MINMAX)",
        "detail": "Hw_2.opencv.samples.python.hist",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.houghcircles",
        "description": "Hw_2.opencv.samples.python.houghcircles",
        "peekOfCode": "def main():\n    try:\n        fn = sys.argv[1]\n    except IndexError:\n        fn = 'board.jpg'\n    src = cv.imread(cv.samples.findFile(fn))\n    img = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n    img = cv.medianBlur(img, 5)\n    cimg = src.copy() # numpy function\n    circles = cv.HoughCircles(img, cv.HOUGH_GRADIENT, 1, 10, np.array([]), 100, 30, 1, 30)",
        "detail": "Hw_2.opencv.samples.python.houghcircles",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.houghlines",
        "description": "Hw_2.opencv.samples.python.houghlines",
        "peekOfCode": "def main():\n    try:\n        fn = sys.argv[1]\n    except IndexError:\n        fn = 'pic1.png'\n    src = cv.imread(cv.samples.findFile(fn))\n    dst = cv.Canny(src, 50, 200)\n    cdst = cv.cvtColor(dst, cv.COLOR_GRAY2BGR)\n    if True: # HoughLinesP\n        lines = cv.HoughLinesP(dst, 1, math.pi/180.0, 40, np.array([]), 50, 10)",
        "detail": "Hw_2.opencv.samples.python.houghlines",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.inpaint",
        "description": "Hw_2.opencv.samples.python.inpaint",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 'fruits.jpg'\n    img = cv.imread(cv.samples.findFile(fn))\n    if img is None:\n        print('Failed to load image file:', fn)\n        sys.exit(1)",
        "detail": "Hw_2.opencv.samples.python.inpaint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.kalman",
        "description": "Hw_2.opencv.samples.python.kalman",
        "peekOfCode": "def main():\n    img_height = 500\n    img_width = 500\n    kalman = cv.KalmanFilter(2, 1, 0)\n    code = long(-1)\n    num_circle_steps = 12\n    while True:\n        img = np.zeros((img_height, img_width, 3), np.uint8)\n        state = np.array([[0.0],[(2 * pi) / num_circle_steps]])   # start state\n        kalman.transitionMatrix = np.array([[1., 1.], [0., 1.]])  # F. input",
        "detail": "Hw_2.opencv.samples.python.kalman",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.kalman",
        "description": "Hw_2.opencv.samples.python.kalman",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    long = int\nimport numpy as np\nimport cv2 as cv\nfrom math import cos, sin, sqrt, pi\ndef main():\n    img_height = 500\n    img_width = 500\n    kalman = cv.KalmanFilter(2, 1, 0)",
        "detail": "Hw_2.opencv.samples.python.kalman",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.kmeans",
        "description": "Hw_2.opencv.samples.python.kmeans",
        "peekOfCode": "def main():\n    cluster_n = 5\n    img_size = 512\n    # generating bright palette\n    colors = np.zeros((1, cluster_n, 3), np.uint8)\n    colors[0,:] = 255\n    colors[0,:,0] = np.arange(0, 180, 180.0/cluster_n)\n    colors = cv.cvtColor(colors, cv.COLOR_HSV2BGR)[0]\n    while True:\n        print('sampling distributions...')",
        "detail": "Hw_2.opencv.samples.python.kmeans",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.laplace",
        "description": "Hw_2.opencv.samples.python.laplace",
        "peekOfCode": "def main():\n    # Declare the variables we are going to use\n    ddepth = cv.CV_16S\n    smoothType = \"MedianBlur\"\n    sigma = 3\n    if len(sys.argv)==4:\n        ddepth = sys.argv[1]\n        smoothType = sys.argv[2]\n        sigma = sys.argv[3]\n    # Taking input from the camera",
        "detail": "Hw_2.opencv.samples.python.laplace",
        "documentation": {}
    },
    {
        "label": "build_lappyr",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lappyr",
        "description": "Hw_2.opencv.samples.python.lappyr",
        "peekOfCode": "def build_lappyr(img, leveln=6, dtype=np.int16):\n    img = dtype(img)\n    levels = []\n    for _i in xrange(leveln-1):\n        next_img = cv.pyrDown(img)\n        img1 = cv.pyrUp(next_img, dstsize=getsize(img))\n        levels.append(img-img1)\n        img = next_img\n    levels.append(img)\n    return levels",
        "detail": "Hw_2.opencv.samples.python.lappyr",
        "documentation": {}
    },
    {
        "label": "merge_lappyr",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lappyr",
        "description": "Hw_2.opencv.samples.python.lappyr",
        "peekOfCode": "def merge_lappyr(levels):\n    img = levels[-1]\n    for lev_img in levels[-2::-1]:\n        img = cv.pyrUp(img, dstsize=getsize(lev_img))\n        img += lev_img\n    return np.uint8(np.clip(img, 0, 255))\ndef main():\n    import sys\n    try:\n        fn = sys.argv[1]",
        "detail": "Hw_2.opencv.samples.python.lappyr",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lappyr",
        "description": "Hw_2.opencv.samples.python.lappyr",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 0\n    cap = video.create_capture(fn)\n    leveln = 6\n    cv.namedWindow('level control')\n    for i in xrange(leveln):",
        "detail": "Hw_2.opencv.samples.python.lappyr",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lappyr",
        "description": "Hw_2.opencv.samples.python.lappyr",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nimport video\nfrom common import nothing, getsize\ndef build_lappyr(img, leveln=6, dtype=np.int16):\n    img = dtype(img)\n    levels = []",
        "detail": "Hw_2.opencv.samples.python.lappyr",
        "documentation": {}
    },
    {
        "label": "LetterStatModel",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class LetterStatModel(object):\n    class_n = 26\n    train_ratio = 0.5\n    def load(self, fn):\n        self.model = self.model.load(fn)\n    def save(self, fn):\n        self.model.save(fn)\n    def unroll_samples(self, samples):\n        sample_n, var_n = samples.shape\n        new_samples = np.zeros((sample_n * self.class_n, var_n+1), np.float32)",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "RTrees",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class RTrees(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.RTrees_create()\n    def train(self, samples, responses):\n        self.model.setMaxDepth(20)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))\n    def predict(self, samples):\n        _ret, resp = self.model.predict(samples)\n        return resp.ravel()\nclass KNearest(LetterStatModel):",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "KNearest",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class KNearest(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.KNearest_create()\n    def train(self, samples, responses):\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses)\n    def predict(self, samples):\n        _retval, results, _neigh_resp, _dists = self.model.findNearest(samples, k = 10)\n        return results.ravel()\nclass Boost(LetterStatModel):\n    def __init__(self):",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "Boost",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class Boost(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.Boost_create()\n    def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_samples = self.unroll_samples(samples)\n        new_responses = self.unroll_responses(responses)\n        var_types = np.array([cv.ml.VAR_NUMERICAL] * var_n + [cv.ml.VAR_CATEGORICAL, cv.ml.VAR_CATEGORICAL], np.uint8)\n        self.model.setWeakCount(15)\n        self.model.setMaxDepth(10)",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "SVM",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class SVM(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.SVM_create()\n    def train(self, samples, responses):\n        self.model.setType(cv.ml.SVM_C_SVC)\n        self.model.setC(1)\n        self.model.setKernel(cv.ml.SVM_RBF)\n        self.model.setGamma(.1)\n        self.model.train(samples, cv.ml.ROW_SAMPLE, responses.astype(int))\n    def predict(self, samples):",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "class MLP(LetterStatModel):\n    def __init__(self):\n        self.model = cv.ml.ANN_MLP_create()\n    def train(self, samples, responses):\n        _sample_n, var_n = samples.shape\n        new_responses = self.unroll_responses(responses).reshape(-1, self.class_n)\n        layer_sizes = np.int32([var_n, 100, 100, self.class_n])\n        self.model.setLayerSizes(layer_sizes)\n        self.model.setTrainMethod(cv.ml.ANN_MLP_BACKPROP)\n        self.model.setBackpropMomentumScale(0.0)",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "load_base",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "def load_base(fn):\n    a = np.loadtxt(fn, np.float32, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n    samples, responses = a[:,1:], a[:,0]\n    return samples, responses\nclass LetterStatModel(object):\n    class_n = 26\n    train_ratio = 0.5\n    def load(self, fn):\n        self.model = self.model.load(fn)\n    def save(self, fn):",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.letter_recog",
        "description": "Hw_2.opencv.samples.python.letter_recog",
        "peekOfCode": "def main():\n    import getopt\n    import sys\n    models = [RTrees, KNearest, Boost, SVM, MLP] # NBayes\n    models = dict( [(cls.__name__.lower(), cls) for cls in models] )\n    args, dummy = getopt.getopt(sys.argv[1:], '', ['model=', 'data=', 'load=', 'save='])\n    args = dict(args)\n    args.setdefault('--model', 'svm')\n    args.setdefault('--data', 'letter-recognition.data')\n    datafile = cv.samples.findFile(args['--data'])",
        "detail": "Hw_2.opencv.samples.python.letter_recog",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "class App:\n    def __init__(self, video_src):\n        self.cam = self.cam = video.create_capture(video_src, presets['book'])\n        self.p0 = None\n        self.use_ransac = True\n    def run(self):\n        while True:\n            _ret, frame = self.cam.read()\n            frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n            vis = frame.copy()",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "checkedTrace",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "def checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n    d = abs(p0-p0r).reshape(-1, 2).max(-1)\n    status = d < back_threshold\n    return p1, status\ngreen = (0, 255, 0)\nred = (0, 0, 255)\nclass App:\n    def __init__(self, video_src):",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "def main():\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        video_src = 0\n    App(video_src).run()\n    print('Done')\nif __name__ == '__main__':\n    print(__doc__)",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "lk_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "lk_params = dict( winSize  = (19, 19),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\nfeature_params = dict( maxCorners = 1000,\n                       qualityLevel = 0.01,\n                       minDistance = 8,\n                       blockSize = 19 )\ndef checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "feature_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "feature_params = dict( maxCorners = 1000,\n                       qualityLevel = 0.01,\n                       minDistance = 8,\n                       blockSize = 19 )\ndef checkedTrace(img0, img1, p0, back_threshold = 1.0):\n    p1, _st, _err = cv.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n    p0r, _st, _err = cv.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n    d = abs(p0-p0r).reshape(-1, 2).max(-1)\n    status = d < back_threshold\n    return p1, status",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "green",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "green = (0, 255, 0)\nred = (0, 0, 255)\nclass App:\n    def __init__(self, video_src):\n        self.cam = self.cam = video.create_capture(video_src, presets['book'])\n        self.p0 = None\n        self.use_ransac = True\n    def run(self):\n        while True:\n            _ret, frame = self.cam.read()",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "red",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_homography",
        "description": "Hw_2.opencv.samples.python.lk_homography",
        "peekOfCode": "red = (0, 0, 255)\nclass App:\n    def __init__(self, video_src):\n        self.cam = self.cam = video.create_capture(video_src, presets['book'])\n        self.p0 = None\n        self.use_ransac = True\n    def run(self):\n        while True:\n            _ret, frame = self.cam.read()\n            frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.lk_homography",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.lk_track",
        "description": "Hw_2.opencv.samples.python.lk_track",
        "peekOfCode": "class App:\n    def __init__(self, video_src):\n        self.track_len = 10\n        self.detect_interval = 5\n        self.tracks = []\n        self.cam = video.create_capture(video_src)\n        self.frame_idx = 0\n    def run(self):\n        while True:\n            _ret, frame = self.cam.read()",
        "detail": "Hw_2.opencv.samples.python.lk_track",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.lk_track",
        "description": "Hw_2.opencv.samples.python.lk_track",
        "peekOfCode": "def main():\n    import sys\n    try:\n        video_src = sys.argv[1]\n    except:\n        video_src = 0\n    App(video_src).run()\n    print('Done')\nif __name__ == '__main__':\n    print(__doc__)",
        "detail": "Hw_2.opencv.samples.python.lk_track",
        "documentation": {}
    },
    {
        "label": "lk_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_track",
        "description": "Hw_2.opencv.samples.python.lk_track",
        "peekOfCode": "lk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\nfeature_params = dict( maxCorners = 500,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\nclass App:\n    def __init__(self, video_src):\n        self.track_len = 10",
        "detail": "Hw_2.opencv.samples.python.lk_track",
        "documentation": {}
    },
    {
        "label": "feature_params",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.lk_track",
        "description": "Hw_2.opencv.samples.python.lk_track",
        "peekOfCode": "feature_params = dict( maxCorners = 500,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\nclass App:\n    def __init__(self, video_src):\n        self.track_len = 10\n        self.detect_interval = 5\n        self.tracks = []\n        self.cam = video.create_capture(video_src)",
        "detail": "Hw_2.opencv.samples.python.lk_track",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.logpolar",
        "description": "Hw_2.opencv.samples.python.logpolar",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except IndexError:\n        fn = 'fruits.jpg'\n    img = cv.imread(cv.samples.findFile(fn))\n    if img is None:\n        print('Failed to load image file:', fn)\n        sys.exit(1)",
        "detail": "Hw_2.opencv.samples.python.logpolar",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.morphology",
        "description": "Hw_2.opencv.samples.python.morphology",
        "peekOfCode": "def main():\n    import sys\n    from itertools import cycle\n    from common import draw_str\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 'baboon.jpg'\n    img = cv.imread(cv.samples.findFile(fn))\n    if img is None:",
        "detail": "Hw_2.opencv.samples.python.morphology",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.morphology",
        "description": "Hw_2.opencv.samples.python.morphology",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nimport numpy as np\nimport cv2 as cv\ndef main():\n    import sys\n    from itertools import cycle\n    from common import draw_str\n    try:\n        fn = sys.argv[1]\n    except:",
        "detail": "Hw_2.opencv.samples.python.morphology",
        "documentation": {}
    },
    {
        "label": "MOSSE",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "class MOSSE:\n    def __init__(self, frame, rect):\n        x1, y1, x2, y2 = rect\n        w, h = map(cv.getOptimalDFTSize, [x2-x1, y2-y1])\n        x1, y1 = (x1+x2-w)//2, (y1+y2-h)//2\n        self.pos = x, y = x1+0.5*(w-1), y1+0.5*(h-1)\n        self.size = w, h\n        img = cv.getRectSubPix(frame, (w, h), (x, y))\n        self.win = cv.createHanningWindow((w, h), cv.CV_32F)\n        g = np.zeros((h, w), np.float32)",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "class App:\n    def __init__(self, video_src, paused = False):\n        self.cap = video.create_capture(video_src)\n        _, self.frame = self.cap.read()\n        cv.imshow('frame', self.frame)\n        self.rect_sel = RectSelector('frame', self.onrect)\n        self.trackers = []\n        self.paused = paused\n    def onrect(self, rect):\n        frame_gray = cv.cvtColor(self.frame, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "rnd_warp",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "def rnd_warp(a):\n    h, w = a.shape[:2]\n    T = np.zeros((2, 3))\n    coef = 0.2\n    ang = (np.random.rand()-0.5)*coef\n    c, s = np.cos(ang), np.sin(ang)\n    T[:2, :2] = [[c,-s], [s, c]]\n    T[:2, :2] += (np.random.rand(2, 2) - 0.5)*coef\n    c = (w/2, h/2)\n    T[:,2] = c - np.dot(T[:2, :2], c)",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "divSpec",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "def divSpec(A, B):\n    Ar, Ai = A[...,0], A[...,1]\n    Br, Bi = B[...,0], B[...,1]\n    C = (Ar+1j*Ai)/(Br+1j*Bi)\n    C = np.dstack([np.real(C), np.imag(C)]).copy()\n    return C\neps = 1e-5\nclass MOSSE:\n    def __init__(self, frame, rect):\n        x1, y1, x2, y2 = rect",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nfrom common import draw_str, RectSelector\nimport video\ndef rnd_warp(a):\n    h, w = a.shape[:2]\n    T = np.zeros((2, 3))",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "eps",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.mosse",
        "description": "Hw_2.opencv.samples.python.mosse",
        "peekOfCode": "eps = 1e-5\nclass MOSSE:\n    def __init__(self, frame, rect):\n        x1, y1, x2, y2 = rect\n        w, h = map(cv.getOptimalDFTSize, [x2-x1, y2-y1])\n        x1, y1 = (x1+x2-w)//2, (y1+y2-h)//2\n        self.pos = x, y = x1+0.5*(w-1), y1+0.5*(h-1)\n        self.size = w, h\n        img = cv.getRectSubPix(frame, (w, h), (x, y))\n        self.win = cv.createHanningWindow((w, h), cv.CV_32F)",
        "detail": "Hw_2.opencv.samples.python.mosse",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.mouse_and_match",
        "description": "Hw_2.opencv.samples.python.mouse_and_match",
        "peekOfCode": "class App():\n    drag_start = None\n    sel = (0,0,0,0)\n    def onmouse(self, event, x, y, flags, param):\n        if event == cv.EVENT_LBUTTONDOWN:\n            self.drag_start = x, y\n            self.sel = (0,0,0,0)\n        elif event == cv.EVENT_LBUTTONUP:\n            if self.sel[2] > self.sel[0] and self.sel[3] > self.sel[1]:\n                patch = self.gray[self.sel[1]:self.sel[3], self.sel[0]:self.sel[2]]",
        "detail": "Hw_2.opencv.samples.python.mouse_and_match",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.mser",
        "description": "Hw_2.opencv.samples.python.mser",
        "peekOfCode": "def main():\n    try:\n        video_src = sys.argv[1]\n    except:\n        video_src = 0\n    cam = video.create_capture(video_src)\n    mser = cv.MSER_create()\n    while True:\n        ret, img = cam.read()\n        if ret == 0:",
        "detail": "Hw_2.opencv.samples.python.mser",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.opencv_version",
        "description": "Hw_2.opencv.samples.python.opencv_version",
        "peekOfCode": "def main():\n    import sys\n    try:\n        param = sys.argv[1]\n    except IndexError:\n        param = \"\"\n    if \"--build\" == param:\n        print(cv.getBuildInformation())\n    elif \"--help\" == param:\n        print(\"\\t--build\\n\\t\\tprint complete build info\")",
        "detail": "Hw_2.opencv.samples.python.opencv_version",
        "documentation": {}
    },
    {
        "label": "draw_flow",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.opt_flow",
        "description": "Hw_2.opencv.samples.python.opt_flow",
        "peekOfCode": "def draw_flow(img, flow, step=16):\n    h, w = img.shape[:2]\n    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n    fx, fy = flow[y,x].T\n    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n    lines = np.int32(lines + 0.5)\n    vis = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n    cv.polylines(vis, lines, 0, (0, 255, 0))\n    for (x1, y1), (_x2, _y2) in lines:\n        cv.circle(vis, (x1, y1), 1, (0, 255, 0), -1)",
        "detail": "Hw_2.opencv.samples.python.opt_flow",
        "documentation": {}
    },
    {
        "label": "draw_hsv",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.opt_flow",
        "description": "Hw_2.opencv.samples.python.opt_flow",
        "peekOfCode": "def draw_hsv(flow):\n    h, w = flow.shape[:2]\n    fx, fy = flow[:,:,0], flow[:,:,1]\n    ang = np.arctan2(fy, fx) + np.pi\n    v = np.sqrt(fx*fx+fy*fy)\n    hsv = np.zeros((h, w, 3), np.uint8)\n    hsv[...,0] = ang*(180/np.pi/2)\n    hsv[...,1] = 255\n    hsv[...,2] = np.minimum(v*4, 255)\n    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)",
        "detail": "Hw_2.opencv.samples.python.opt_flow",
        "documentation": {}
    },
    {
        "label": "warp_flow",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.opt_flow",
        "description": "Hw_2.opencv.samples.python.opt_flow",
        "peekOfCode": "def warp_flow(img, flow):\n    h, w = flow.shape[:2]\n    flow = -flow\n    flow[:,:,0] += np.arange(w)\n    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n    res = cv.remap(img, flow, None, cv.INTER_LINEAR)\n    return res\ndef main():\n    import sys\n    try:",
        "detail": "Hw_2.opencv.samples.python.opt_flow",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.opt_flow",
        "description": "Hw_2.opencv.samples.python.opt_flow",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except IndexError:\n        fn = 0\n    cam = video.create_capture(fn)\n    _ret, prev = cam.read()\n    prevgray = cv.cvtColor(prev, cv.COLOR_BGR2GRAY)\n    show_hsv = False",
        "detail": "Hw_2.opencv.samples.python.opt_flow",
        "documentation": {}
    },
    {
        "label": "inside",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.peopledetect",
        "description": "Hw_2.opencv.samples.python.peopledetect",
        "peekOfCode": "def inside(r, q):\n    rx, ry, rw, rh = r\n    qx, qy, qw, qh = q\n    return rx > qx and ry > qy and rx + rw < qx + qw and ry + rh < qy + qh\ndef draw_detections(img, rects, thickness = 1):\n    for x, y, w, h in rects:\n        # the HOG detector returns slightly larger rectangles than the real objects.\n        # so we slightly shrink the rectangles to get a nicer output.\n        pad_w, pad_h = int(0.15*w), int(0.05*h)\n        cv.rectangle(img, (x+pad_w, y+pad_h), (x+w-pad_w, y+h-pad_h), (0, 255, 0), thickness)",
        "detail": "Hw_2.opencv.samples.python.peopledetect",
        "documentation": {}
    },
    {
        "label": "draw_detections",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.peopledetect",
        "description": "Hw_2.opencv.samples.python.peopledetect",
        "peekOfCode": "def draw_detections(img, rects, thickness = 1):\n    for x, y, w, h in rects:\n        # the HOG detector returns slightly larger rectangles than the real objects.\n        # so we slightly shrink the rectangles to get a nicer output.\n        pad_w, pad_h = int(0.15*w), int(0.05*h)\n        cv.rectangle(img, (x+pad_w, y+pad_h), (x+w-pad_w, y+h-pad_h), (0, 255, 0), thickness)\ndef main():\n    import sys\n    from glob import glob\n    import itertools as it",
        "detail": "Hw_2.opencv.samples.python.peopledetect",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.peopledetect",
        "description": "Hw_2.opencv.samples.python.peopledetect",
        "peekOfCode": "def main():\n    import sys\n    from glob import glob\n    import itertools as it\n    hog = cv.HOGDescriptor()\n    hog.setSVMDetector( cv.HOGDescriptor_getDefaultPeopleDetector() )\n    default = [cv.samples.findFile('basketball2.png')] if len(sys.argv[1:]) == 0 else []\n    for fn in it.chain(*map(glob, default + sys.argv[1:])):\n        print(fn, ' - ',)\n        try:",
        "detail": "Hw_2.opencv.samples.python.peopledetect",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.plane_ar",
        "description": "Hw_2.opencv.samples.python.plane_ar",
        "peekOfCode": "class App:\n    def __init__(self, src):\n        self.cap = video.create_capture(src, presets['book'])\n        self.frame = None\n        self.paused = False\n        self.tracker = PlaneTracker()\n        cv.namedWindow('plane')\n        cv.createTrackbar('focal', 'plane', 25, 50, common.nothing)\n        self.rect_sel = common.RectSelector('plane', self.on_rect)\n    def on_rect(self, rect):",
        "detail": "Hw_2.opencv.samples.python.plane_ar",
        "documentation": {}
    },
    {
        "label": "ar_verts",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_ar",
        "description": "Hw_2.opencv.samples.python.plane_ar",
        "peekOfCode": "ar_verts = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0],\n                       [0, 0, 1], [0, 1, 1], [1, 1, 1], [1, 0, 1],\n                       [0, 0.5, 2], [1, 0.5, 2]])\nar_edges = [(0, 1), (1, 2), (2, 3), (3, 0),\n            (4, 5), (5, 6), (6, 7), (7, 4),\n            (0, 4), (1, 5), (2, 6), (3, 7),\n            (4, 8), (5, 8), (6, 9), (7, 9), (8, 9)]\nclass App:\n    def __init__(self, src):\n        self.cap = video.create_capture(src, presets['book'])",
        "detail": "Hw_2.opencv.samples.python.plane_ar",
        "documentation": {}
    },
    {
        "label": "ar_edges",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_ar",
        "description": "Hw_2.opencv.samples.python.plane_ar",
        "peekOfCode": "ar_edges = [(0, 1), (1, 2), (2, 3), (3, 0),\n            (4, 5), (5, 6), (6, 7), (7, 4),\n            (0, 4), (1, 5), (2, 6), (3, 7),\n            (4, 8), (5, 8), (6, 9), (7, 9), (8, 9)]\nclass App:\n    def __init__(self, src):\n        self.cap = video.create_capture(src, presets['book'])\n        self.frame = None\n        self.paused = False\n        self.tracker = PlaneTracker()",
        "detail": "Hw_2.opencv.samples.python.plane_ar",
        "documentation": {}
    },
    {
        "label": "PlaneTracker",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "class PlaneTracker:\n    def __init__(self):\n        self.detector = cv.ORB_create( nfeatures = 1000 )\n        self.matcher = cv.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n        self.targets = []\n        self.frame_points = []\n    def add_target(self, image, rect, data=None):\n        '''Add a new tracking target.'''\n        x0, y0, x1, y1 = rect\n        raw_points, raw_descrs = self.detect_features(image)",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "class App:\n    def __init__(self, src):\n        self.cap = video.create_capture(src, presets['book'])\n        self.frame = None\n        self.paused = False\n        self.tracker = PlaneTracker()\n        cv.namedWindow('plane')\n        self.rect_sel = common.RectSelector('plane', self.on_rect)\n    def on_rect(self, rect):\n        self.tracker.add_target(self.frame, rect)",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\n# built-in modules\nfrom collections import namedtuple\n# local modules\nimport video\nimport common",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "FLANN_INDEX_KDTREE",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "FLANN_INDEX_KDTREE = 1\nFLANN_INDEX_LSH    = 6\nflann_params= dict(algorithm = FLANN_INDEX_LSH,\n                   table_number = 6, # 12\n                   key_size = 12,     # 20\n                   multi_probe_level = 1) #2\nMIN_MATCH_COUNT = 10\n'''\n  image     - image to track\n  rect      - tracked rectangle (x1, y1, x2, y2)",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "MIN_MATCH_COUNT",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "MIN_MATCH_COUNT = 10\n'''\n  image     - image to track\n  rect      - tracked rectangle (x1, y1, x2, y2)\n  keypoints - keypoints detected inside rect\n  descrs    - their descriptors\n  data      - some user-provided data\n'''\nPlanarTarget = namedtuple('PlaneTarget', 'image, rect, keypoints, descrs, data')\n'''",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "PlanarTarget",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "PlanarTarget = namedtuple('PlaneTarget', 'image, rect, keypoints, descrs, data')\n'''\n  target - reference to PlanarTarget\n  p0     - matched points coords in target image\n  p1     - matched points coords in input frame\n  H      - homography matrix from p0 to p1\n  quad   - target boundary quad in input frame\n'''\nTrackedTarget = namedtuple('TrackedTarget', 'target, p0, p1, H, quad')\nclass PlaneTracker:",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "TrackedTarget",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.plane_tracker",
        "description": "Hw_2.opencv.samples.python.plane_tracker",
        "peekOfCode": "TrackedTarget = namedtuple('TrackedTarget', 'target, p0, p1, H, quad')\nclass PlaneTracker:\n    def __init__(self):\n        self.detector = cv.ORB_create( nfeatures = 1000 )\n        self.matcher = cv.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n        self.targets = []\n        self.frame_points = []\n    def add_target(self, image, rect, data=None):\n        '''Add a new tracking target.'''\n        x0, y0, x1, y1 = rect",
        "detail": "Hw_2.opencv.samples.python.plane_tracker",
        "documentation": {}
    },
    {
        "label": "QrSample",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.qrcode",
        "description": "Hw_2.opencv.samples.python.qrcode",
        "peekOfCode": "class QrSample:\n    def __init__(self, args):\n        self.fname = ''\n        self.fext = ''\n        self.fsaveid = 0\n        self.input = args.input\n        self.detect = args.detect\n        self.out = args.out\n        self.multi = args.multi\n        self.saveDetections = args.save_detections",
        "detail": "Hw_2.opencv.samples.python.qrcode",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.qrcode",
        "description": "Hw_2.opencv.samples.python.qrcode",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        description='This program detects the QR-codes input images using OpenCV Library.')\n    parser.add_argument(\n        '-i',\n        '--input',\n        help=\"input image path (for example, 'opencv_extra/testdata/cv/qrcode/multiple/*_qrcodes.png)\",\n        default=\"\",\n        metavar=\"\")\n    parser.add_argument(",
        "detail": "Hw_2.opencv.samples.python.qrcode",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.qrcode",
        "description": "Hw_2.opencv.samples.python.qrcode",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nclass QrSample:\n    def __init__(self, args):\n        self.fname = ''\n        self.fext = ''\n        self.fsaveid = 0\n        self.input = args.input\n        self.detect = args.detect",
        "detail": "Hw_2.opencv.samples.python.qrcode",
        "documentation": {}
    },
    {
        "label": "angle_cos",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.squares",
        "description": "Hw_2.opencv.samples.python.squares",
        "peekOfCode": "def angle_cos(p0, p1, p2):\n    d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')\n    return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )\ndef find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)\n    squares = []\n    for gray in cv.split(img):\n        for thrs in xrange(0, 255, 26):\n            if thrs == 0:\n                bin = cv.Canny(gray, 0, 50, apertureSize=5)",
        "detail": "Hw_2.opencv.samples.python.squares",
        "documentation": {}
    },
    {
        "label": "find_squares",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.squares",
        "description": "Hw_2.opencv.samples.python.squares",
        "peekOfCode": "def find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)\n    squares = []\n    for gray in cv.split(img):\n        for thrs in xrange(0, 255, 26):\n            if thrs == 0:\n                bin = cv.Canny(gray, 0, 50, apertureSize=5)\n                bin = cv.dilate(bin, None)\n            else:\n                _retval, bin = cv.threshold(gray, thrs, 255, cv.THRESH_BINARY)",
        "detail": "Hw_2.opencv.samples.python.squares",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.squares",
        "description": "Hw_2.opencv.samples.python.squares",
        "peekOfCode": "def main():\n    from glob import glob\n    for fn in glob('../data/pic*.png'):\n        img = cv.imread(fn)\n        squares = find_squares(img)\n        cv.drawContours( img, squares, -1, (0, 255, 0), 3 )\n        cv.imshow('squares', img)\n        ch = cv.waitKey()\n        if ch == 27:\n            break",
        "detail": "Hw_2.opencv.samples.python.squares",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.squares",
        "description": "Hw_2.opencv.samples.python.squares",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\ndef angle_cos(p0, p1, p2):\n    d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')\n    return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )\ndef find_squares(img):\n    img = cv.GaussianBlur(img, (5, 5), 0)",
        "detail": "Hw_2.opencv.samples.python.squares",
        "documentation": {}
    },
    {
        "label": "write_ply",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stereo_match",
        "description": "Hw_2.opencv.samples.python.stereo_match",
        "peekOfCode": "def write_ply(fn, verts, colors):\n    verts = verts.reshape(-1, 3)\n    colors = colors.reshape(-1, 3)\n    verts = np.hstack([verts, colors])\n    with open(fn, 'wb') as f:\n        f.write((ply_header % dict(vert_num=len(verts))).encode('utf-8'))\n        np.savetxt(f, verts, fmt='%f %f %f %d %d %d ')\ndef main():\n    print('loading images...')\n    imgL = cv.pyrDown(cv.imread(cv.samples.findFile('aloeL.jpg')))  # downscale images for faster processing",
        "detail": "Hw_2.opencv.samples.python.stereo_match",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stereo_match",
        "description": "Hw_2.opencv.samples.python.stereo_match",
        "peekOfCode": "def main():\n    print('loading images...')\n    imgL = cv.pyrDown(cv.imread(cv.samples.findFile('aloeL.jpg')))  # downscale images for faster processing\n    imgR = cv.pyrDown(cv.imread(cv.samples.findFile('aloeR.jpg')))\n    # disparity range is tuned for 'aloe' image pair\n    window_size = 3\n    min_disp = 16\n    num_disp = 112-min_disp\n    stereo = cv.StereoSGBM_create(minDisparity = min_disp,\n        numDisparities = num_disp,",
        "detail": "Hw_2.opencv.samples.python.stereo_match",
        "documentation": {}
    },
    {
        "label": "ply_header",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stereo_match",
        "description": "Hw_2.opencv.samples.python.stereo_match",
        "peekOfCode": "ply_header = '''ply\nformat ascii 1.0\nelement vertex %(vert_num)d\nproperty float x\nproperty float y\nproperty float z\nproperty uchar red\nproperty uchar green\nproperty uchar blue\nend_header",
        "detail": "Hw_2.opencv.samples.python.stereo_match",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stitching",
        "description": "Hw_2.opencv.samples.python.stitching",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # read input images\n    imgs = []\n    for img_name in args.img:\n        img = cv.imread(cv.samples.findFile(img_name))\n        if img is None:\n            print(\"can't read image \" + img_name)\n            sys.exit(-1)\n        imgs.append(img)",
        "detail": "Hw_2.opencv.samples.python.stitching",
        "documentation": {}
    },
    {
        "label": "modes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching",
        "description": "Hw_2.opencv.samples.python.stitching",
        "peekOfCode": "modes = (cv.Stitcher_PANORAMA, cv.Stitcher_SCANS)\nparser = argparse.ArgumentParser(prog='stitching.py', description='Stitching sample.')\nparser.add_argument('--mode',\n    type = int, choices = modes, default = cv.Stitcher_PANORAMA,\n    help = 'Determines configuration of stitcher. The default is `PANORAMA` (%d), '\n         'mode suitable for creating photo panoramas. Option `SCANS` (%d) is suitable '\n         'for stitching materials under affine transformation, such as scans.' % modes)\nparser.add_argument('--output', default = 'result.jpg',\n    help = 'Resulting image. The default is `result.jpg`.')\nparser.add_argument('img', nargs='+', help = 'input images')",
        "detail": "Hw_2.opencv.samples.python.stitching",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching",
        "description": "Hw_2.opencv.samples.python.stitching",
        "peekOfCode": "parser = argparse.ArgumentParser(prog='stitching.py', description='Stitching sample.')\nparser.add_argument('--mode',\n    type = int, choices = modes, default = cv.Stitcher_PANORAMA,\n    help = 'Determines configuration of stitcher. The default is `PANORAMA` (%d), '\n         'mode suitable for creating photo panoramas. Option `SCANS` (%d) is suitable '\n         'for stitching materials under affine transformation, such as scans.' % modes)\nparser.add_argument('--output', default = 'result.jpg',\n    help = 'Resulting image. The default is `result.jpg`.')\nparser.add_argument('img', nargs='+', help = 'input images')\n__doc__ += '\\n' + parser.format_help()",
        "detail": "Hw_2.opencv.samples.python.stitching",
        "documentation": {}
    },
    {
        "label": "get_matcher",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "def get_matcher(args):\n    try_cuda = args.try_cuda\n    matcher_type = args.matcher\n    if args.match_conf is None:\n        if args.features == 'orb':\n            match_conf = 0.3\n        else:\n            match_conf = 0.65\n    else:\n        match_conf = args.match_conf",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "get_compensator",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "def get_compensator(args):\n    expos_comp_type = EXPOS_COMP_CHOICES[args.expos_comp]\n    expos_comp_nr_feeds = args.expos_comp_nr_feeds\n    expos_comp_block_size = args.expos_comp_block_size\n    # expos_comp_nr_filtering = args.expos_comp_nr_filtering\n    if expos_comp_type == cv.detail.ExposureCompensator_CHANNELS:\n        compensator = cv.detail_ChannelsCompensator(expos_comp_nr_feeds)\n        # compensator.setNrGainsFilteringIterations(expos_comp_nr_filtering)\n    elif expos_comp_type == cv.detail.ExposureCompensator_CHANNELS_BLOCKS:\n        compensator = cv.detail_BlocksChannelsCompensator(",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    img_names = args.img_names\n    print(img_names)\n    work_megapix = args.work_megapix\n    seam_megapix = args.seam_megapix\n    compose_megapix = args.compose_megapix\n    conf_thresh = args.conf_thresh\n    ba_refine_mask = args.ba_refine_mask\n    wave_correct = WAVE_CORRECT_CHOICES[args.wave_correct]",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES = OrderedDict()\nEXPOS_COMP_CHOICES['gain_blocks'] = cv.detail.ExposureCompensator_GAIN_BLOCKS\nEXPOS_COMP_CHOICES['gain'] = cv.detail.ExposureCompensator_GAIN\nEXPOS_COMP_CHOICES['channel'] = cv.detail.ExposureCompensator_CHANNELS\nEXPOS_COMP_CHOICES['channel_blocks'] = cv.detail.ExposureCompensator_CHANNELS_BLOCKS\nEXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES['gain_blocks']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES['gain_blocks'] = cv.detail.ExposureCompensator_GAIN_BLOCKS\nEXPOS_COMP_CHOICES['gain'] = cv.detail.ExposureCompensator_GAIN\nEXPOS_COMP_CHOICES['channel'] = cv.detail.ExposureCompensator_CHANNELS\nEXPOS_COMP_CHOICES['channel_blocks'] = cv.detail.ExposureCompensator_CHANNELS_BLOCKS\nEXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES['gain']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES['gain'] = cv.detail.ExposureCompensator_GAIN\nEXPOS_COMP_CHOICES['channel'] = cv.detail.ExposureCompensator_CHANNELS\nEXPOS_COMP_CHOICES['channel_blocks'] = cv.detail.ExposureCompensator_CHANNELS_BLOCKS\nEXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES['channel']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES['channel'] = cv.detail.ExposureCompensator_CHANNELS\nEXPOS_COMP_CHOICES['channel_blocks'] = cv.detail.ExposureCompensator_CHANNELS_BLOCKS\nEXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES['channel_blocks']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES['channel_blocks'] = cv.detail.ExposureCompensator_CHANNELS_BLOCKS\nEXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "EXPOS_COMP_CHOICES['no']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "EXPOS_COMP_CHOICES['no'] = cv.detail.ExposureCompensator_NO\nBA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BA_COST_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BA_COST_CHOICES = OrderedDict()\nBA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BA_COST_CHOICES['ray']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BA_COST_CHOICES['ray'] = cv.detail_BundleAdjusterRay\nBA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:\n    print(\"SURF not available\")",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BA_COST_CHOICES['reproj']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BA_COST_CHOICES['reproj'] = cv.detail_BundleAdjusterReproj\nBA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:\n    print(\"SURF not available\")\n# if SURF not available, ORB is default",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BA_COST_CHOICES['affine']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BA_COST_CHOICES['affine'] = cv.detail_BundleAdjusterAffinePartial\nBA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:\n    print(\"SURF not available\")\n# if SURF not available, ORB is default\nFEATURES_FIND_CHOICES['orb'] = cv.ORB.create",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BA_COST_CHOICES['no']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BA_COST_CHOICES['no'] = cv.detail_NoBundleAdjuster\nFEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:\n    print(\"SURF not available\")\n# if SURF not available, ORB is default\nFEATURES_FIND_CHOICES['orb'] = cv.ORB.create\ntry:",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "FEATURES_FIND_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "FEATURES_FIND_CHOICES = OrderedDict()\ntry:\n    cv.xfeatures2d_SURF.create() # check if the function can be called\n    FEATURES_FIND_CHOICES['surf'] = cv.xfeatures2d_SURF.create\nexcept (AttributeError, cv.error) as e:\n    print(\"SURF not available\")\n# if SURF not available, ORB is default\nFEATURES_FIND_CHOICES['orb'] = cv.ORB.create\ntry:\n    FEATURES_FIND_CHOICES['sift'] = cv.SIFT_create",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "FEATURES_FIND_CHOICES['orb']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "FEATURES_FIND_CHOICES['orb'] = cv.ORB.create\ntry:\n    FEATURES_FIND_CHOICES['sift'] = cv.SIFT_create\nexcept AttributeError:\n    print(\"SIFT not available\")\ntry:\n    FEATURES_FIND_CHOICES['brisk'] = cv.BRISK_create\nexcept AttributeError:\n    print(\"BRISK not available\")\ntry:",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES = OrderedDict()\nSEAM_FIND_CHOICES['gc_color'] = cv.detail_GraphCutSeamFinder('COST_COLOR')\nSEAM_FIND_CHOICES['gc_colorgrad'] = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\nSEAM_FIND_CHOICES['dp_color'] = cv.detail_DpSeamFinder('COLOR')\nSEAM_FIND_CHOICES['dp_colorgrad'] = cv.detail_DpSeamFinder('COLOR_GRAD')\nSEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['gc_color']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['gc_color'] = cv.detail_GraphCutSeamFinder('COST_COLOR')\nSEAM_FIND_CHOICES['gc_colorgrad'] = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\nSEAM_FIND_CHOICES['dp_color'] = cv.detail_DpSeamFinder('COLOR')\nSEAM_FIND_CHOICES['dp_colorgrad'] = cv.detail_DpSeamFinder('COLOR_GRAD')\nSEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['gc_colorgrad']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['gc_colorgrad'] = cv.detail_GraphCutSeamFinder('COST_COLOR_GRAD')\nSEAM_FIND_CHOICES['dp_color'] = cv.detail_DpSeamFinder('COLOR')\nSEAM_FIND_CHOICES['dp_colorgrad'] = cv.detail_DpSeamFinder('COLOR_GRAD')\nSEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['dp_color']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['dp_color'] = cv.detail_DpSeamFinder('COLOR')\nSEAM_FIND_CHOICES['dp_colorgrad'] = cv.detail_DpSeamFinder('COLOR_GRAD')\nSEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['dp_colorgrad']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['dp_colorgrad'] = cv.detail_DpSeamFinder('COLOR_GRAD')\nSEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['voronoi']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['voronoi'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_VORONOI_SEAM)\nSEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "SEAM_FIND_CHOICES['no']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "SEAM_FIND_CHOICES['no'] = cv.detail.SeamFinder_createDefault(cv.detail.SeamFinder_NO)\nESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',\n    'fisheye',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "ESTIMATOR_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "ESTIMATOR_CHOICES = OrderedDict()\nESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',\n    'fisheye',\n    'stereographic',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "ESTIMATOR_CHOICES['homography']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "ESTIMATOR_CHOICES['homography'] = cv.detail_HomographyBasedEstimator\nESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',\n    'fisheye',\n    'stereographic',\n    'compressedPlaneA2B1',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "ESTIMATOR_CHOICES['affine']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "ESTIMATOR_CHOICES['affine'] = cv.detail_AffineBasedEstimator\nWARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',\n    'fisheye',\n    'stereographic',\n    'compressedPlaneA2B1',\n    'compressedPlaneA1.5B1',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "WARP_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "WARP_CHOICES = (\n    'spherical',\n    'plane',\n    'affine',\n    'cylindrical',\n    'fisheye',\n    'stereographic',\n    'compressedPlaneA2B1',\n    'compressedPlaneA1.5B1',\n    'compressedPlanePortraitA2B1',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "WAVE_CORRECT_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "WAVE_CORRECT_CHOICES = OrderedDict()\nWAVE_CORRECT_CHOICES['horiz'] = cv.detail.WAVE_CORRECT_HORIZ\nWAVE_CORRECT_CHOICES['no'] = None\nWAVE_CORRECT_CHOICES['vert'] = cv.detail.WAVE_CORRECT_VERT\nBLEND_CHOICES = ('multiband', 'feather', 'no',)\nparser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "WAVE_CORRECT_CHOICES['horiz']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "WAVE_CORRECT_CHOICES['horiz'] = cv.detail.WAVE_CORRECT_HORIZ\nWAVE_CORRECT_CHOICES['no'] = None\nWAVE_CORRECT_CHOICES['vert'] = cv.detail.WAVE_CORRECT_VERT\nBLEND_CHOICES = ('multiband', 'feather', 'no',)\nparser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',\n    help=\"Files to stitch\", type=str",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "WAVE_CORRECT_CHOICES['no']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "WAVE_CORRECT_CHOICES['no'] = None\nWAVE_CORRECT_CHOICES['vert'] = cv.detail.WAVE_CORRECT_VERT\nBLEND_CHOICES = ('multiband', 'feather', 'no',)\nparser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',\n    help=\"Files to stitch\", type=str\n)",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "WAVE_CORRECT_CHOICES['vert']",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "WAVE_CORRECT_CHOICES['vert'] = cv.detail.WAVE_CORRECT_VERT\nBLEND_CHOICES = ('multiband', 'feather', 'no',)\nparser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',\n    help=\"Files to stitch\", type=str\n)\nparser.add_argument(",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "BLEND_CHOICES",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "BLEND_CHOICES = ('multiband', 'feather', 'no',)\nparser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',\n    help=\"Files to stitch\", type=str\n)\nparser.add_argument(\n    '--try_cuda',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.stitching_detailed",
        "description": "Hw_2.opencv.samples.python.stitching_detailed",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    prog=\"stitching_detailed.py\", description=\"Rotation model images stitcher\"\n)\nparser.add_argument(\n    'img_names', nargs='+',\n    help=\"Files to stitch\", type=str\n)\nparser.add_argument(\n    '--try_cuda',\n    action='store',",
        "detail": "Hw_2.opencv.samples.python.stitching_detailed",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.texture_flow",
        "description": "Hw_2.opencv.samples.python.texture_flow",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 'starry_night.jpg'\n    img = cv.imread(cv.samples.findFile(fn))\n    if img is None:\n        print('Failed to load image file:', fn)\n        sys.exit(1)",
        "detail": "Hw_2.opencv.samples.python.texture_flow",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.text_skewness_correction",
        "description": "Hw_2.opencv.samples.python.text_skewness_correction",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--image\", default=\"imageTextR.png\", help=\"path to input image file\")\n    args = vars(parser.parse_args())\n    # load the image from disk\n    image = cv.imread(cv.samples.findFile(args[\"image\"]))\n    if image is None:\n        print(\"can't read image \" + args[\"image\"])\n        sys.exit(-1)\n    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)",
        "detail": "Hw_2.opencv.samples.python.text_skewness_correction",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.tracker",
        "description": "Hw_2.opencv.samples.python.tracker",
        "peekOfCode": "class App(object):\n    def __init__(self, args):\n        self.args = args\n        self.trackerAlgorithm = args.tracker_algo\n        self.tracker = self.createTracker()\n    def createTracker(self):\n        if self.trackerAlgorithm == 'mil':\n            tracker = cv.TrackerMIL_create()\n        elif self.trackerAlgorithm == 'goturn':\n            params = cv.TrackerGOTURN_Params()",
        "detail": "Hw_2.opencv.samples.python.tracker",
        "documentation": {}
    },
    {
        "label": "TestSceneRender",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.tst_scene_render",
        "description": "Hw_2.opencv.samples.python.tst_scene_render",
        "peekOfCode": "class TestSceneRender():\n    def __init__(self, bgImg = None, fgImg = None,\n        deformation = False, speed = 0.25, **params):\n        self.time = 0.0\n        self.timeStep = 1.0 / 30.0\n        self.foreground = fgImg\n        self.deformation = deformation\n        self.speed = speed\n        if bgImg is not None:\n            self.sceneBg = bgImg.copy()",
        "detail": "Hw_2.opencv.samples.python.tst_scene_render",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.tst_scene_render",
        "description": "Hw_2.opencv.samples.python.tst_scene_render",
        "peekOfCode": "def main():\n    backGr = cv.imread(cv.samples.findFile('graf1.png'))\n    fgr = cv.imread(cv.samples.findFile('box.png'))\n    render = TestSceneRender(backGr, fgr)\n    while True:\n        img = render.getNextFrame()\n        cv.imshow('img', img)\n        ch = cv.waitKey(3)\n        if  ch == 27:\n            break",
        "detail": "Hw_2.opencv.samples.python.tst_scene_render",
        "documentation": {}
    },
    {
        "label": "defaultSize",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.tst_scene_render",
        "description": "Hw_2.opencv.samples.python.tst_scene_render",
        "peekOfCode": "defaultSize = 512\nclass TestSceneRender():\n    def __init__(self, bgImg = None, fgImg = None,\n        deformation = False, speed = 0.25, **params):\n        self.time = 0.0\n        self.timeStep = 1.0 / 30.0\n        self.foreground = fgImg\n        self.deformation = deformation\n        self.speed = speed\n        if bgImg is not None:",
        "detail": "Hw_2.opencv.samples.python.tst_scene_render",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.turing",
        "description": "Hw_2.opencv.samples.python.turing",
        "peekOfCode": "def main():\n    print(help_message)\n    w, h = 512, 512\n    args, _args_list = getopt.getopt(sys.argv[1:], 'o:', [])\n    args = dict(args)\n    out = None\n    if '-o' in args:\n        fn = args['-o']\n        out = cv.VideoWriter(args['-o'], cv.VideoWriter_fourcc(*'DIB '), 30.0, (w, h), False)\n        print('writing %s ...' % fn)",
        "detail": "Hw_2.opencv.samples.python.turing",
        "documentation": {}
    },
    {
        "label": "PY3",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.turing",
        "description": "Hw_2.opencv.samples.python.turing",
        "peekOfCode": "PY3 = sys.version_info[0] == 3\nif PY3:\n    xrange = range\nimport numpy as np\nimport cv2 as cv\nfrom common import draw_str\nimport getopt, sys\nfrom itertools import count\nhelp_message = '''\nUSAGE: turing.py [-o <output.avi>]",
        "detail": "Hw_2.opencv.samples.python.turing",
        "documentation": {}
    },
    {
        "label": "help_message",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.turing",
        "description": "Hw_2.opencv.samples.python.turing",
        "peekOfCode": "help_message = '''\nUSAGE: turing.py [-o <output.avi>]\nPress ESC to stop.\n'''\ndef main():\n    print(help_message)\n    w, h = 512, 512\n    args, _args_list = getopt.getopt(sys.argv[1:], 'o:', [])\n    args = dict(args)\n    out = None",
        "detail": "Hw_2.opencv.samples.python.turing",
        "documentation": {}
    },
    {
        "label": "VideoSynthBase",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "class VideoSynthBase(object):\n    def __init__(self, size=None, noise=0.0, bg = None, **params):\n        self.bg = None\n        self.frame_size = (640, 480)\n        if bg is not None:\n            self.bg = cv.imread(cv.samples.findFile(bg))\n            h, w = self.bg.shape[:2]\n            self.frame_size = (w, h)\n        if size is not None:\n            w, h = map(int, size.split('x'))",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "Book",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "class Book(VideoSynthBase):\n    def __init__(self, **kw):\n        super(Book, self).__init__(**kw)\n        backGr = cv.imread(cv.samples.findFile('graf1.png'))\n        fgr = cv.imread(cv.samples.findFile('box.png'))\n        self.render = TestSceneRender(backGr, fgr, speed = 1)\n    def read(self, dst=None):\n        noise = np.zeros(self.render.sceneBg.shape, np.int8)\n        cv.randn(noise, np.zeros(3), np.ones(3)*255*self.noise)\n        return True, cv.add(self.render.getNextFrame(), noise, dtype=cv.CV_8UC3)",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "Cube",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "class Cube(VideoSynthBase):\n    def __init__(self, **kw):\n        super(Cube, self).__init__(**kw)\n        self.render = TestSceneRender(cv.imread(cv.samples.findFile('pca_test1.jpg')), deformation = True,  speed = 1)\n    def read(self, dst=None):\n        noise = np.zeros(self.render.sceneBg.shape, np.int8)\n        cv.randn(noise, np.zeros(3), np.ones(3)*255*self.noise)\n        return True, cv.add(self.render.getNextFrame(), noise, dtype=cv.CV_8UC3)\nclass Chess(VideoSynthBase):\n    def __init__(self, **kw):",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "Chess",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "class Chess(VideoSynthBase):\n    def __init__(self, **kw):\n        super(Chess, self).__init__(**kw)\n        w, h = self.frame_size\n        self.grid_size = sx, sy = 10, 7\n        white_quads = []\n        black_quads = []\n        for i, j in np.ndindex(sy, sx):\n            q = [[j, i, 0], [j+1, i, 0], [j+1, i+1, 0], [j, i+1, 0]]\n            [white_quads, black_quads][(i + j) % 2].append(q)",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "create_capture",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "def create_capture(source = 0, fallback = presets['chess']):\n    '''source: <int> or '<int>|<filename>|synth [:<param_name>=<value> [:...]]'\n    '''\n    source = str(source).strip()\n    # Win32: handle drive letter ('c:', ...)\n    source = re.sub(r'(^|=)([a-zA-Z]):([/\\\\a-zA-Z0-9])', r'\\1?disk\\2?\\3', source)\n    chunks = source.split(':')\n    chunks = [re.sub(r'\\?disk([a-zA-Z])\\?', r'\\1:', s) for s in chunks]\n    source = chunks[0]\n    try: source = int(source)",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "classes = dict(chess=Chess, book=Book, cube=Cube)\npresets = dict(\n    empty = 'synth:',\n    lena = 'synth:bg=lena.jpg:noise=0.1',\n    chess = 'synth:class=chess:bg=lena.jpg:noise=0.1:size=640x480',\n    book = 'synth:class=book:bg=graf1.png:noise=0.1:size=640x480',\n    cube = 'synth:class=cube:bg=pca_test1.jpg:noise=0.0:size=640x480'\n)\ndef create_capture(source = 0, fallback = presets['chess']):\n    '''source: <int> or '<int>|<filename>|synth [:<param_name>=<value> [:...]]'",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "presets",
        "kind": 5,
        "importPath": "Hw_2.opencv.samples.python.video",
        "description": "Hw_2.opencv.samples.python.video",
        "peekOfCode": "presets = dict(\n    empty = 'synth:',\n    lena = 'synth:bg=lena.jpg:noise=0.1',\n    chess = 'synth:class=chess:bg=lena.jpg:noise=0.1:size=640x480',\n    book = 'synth:class=book:bg=graf1.png:noise=0.1:size=640x480',\n    cube = 'synth:class=cube:bg=pca_test1.jpg:noise=0.0:size=640x480'\n)\ndef create_capture(source = 0, fallback = presets['chess']):\n    '''source: <int> or '<int>|<filename>|synth [:<param_name>=<value> [:...]]'\n    '''",
        "detail": "Hw_2.opencv.samples.python.video",
        "documentation": {}
    },
    {
        "label": "DummyTask",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.video_threaded",
        "description": "Hw_2.opencv.samples.python.video_threaded",
        "peekOfCode": "class DummyTask:\n    def __init__(self, data):\n        self.data = data\n    def ready(self):\n        return True\n    def get(self):\n        return self.data\ndef main():\n    import sys\n    try:",
        "detail": "Hw_2.opencv.samples.python.video_threaded",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.video_threaded",
        "description": "Hw_2.opencv.samples.python.video_threaded",
        "peekOfCode": "def main():\n    import sys\n    try:\n        fn = sys.argv[1]\n    except:\n        fn = 0\n    cap = video.create_capture(fn)\n    def process_frame(frame, t0):\n        # some intensive computation...\n        frame = cv.medianBlur(frame, 19)",
        "detail": "Hw_2.opencv.samples.python.video_threaded",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Hw_2.opencv.samples.python.video_v4l2",
        "description": "Hw_2.opencv.samples.python.video_v4l2",
        "peekOfCode": "def main():\n    def decode_fourcc(v):\n        v = int(v)\n        return \"\".join([chr((v >> 8 * i) & 0xFF) for i in range(4)])\n    font = cv.FONT_HERSHEY_SIMPLEX\n    color = (0, 255, 0)\n    cap = cv.VideoCapture(0)\n    cap.set(cv.CAP_PROP_AUTOFOCUS, 0)  # Known bug: https://github.com/opencv/opencv/pull/5474\n    cv.namedWindow(\"Video\")\n    convert_rgb = True",
        "detail": "Hw_2.opencv.samples.python.video_v4l2",
        "documentation": {}
    },
    {
        "label": "App",
        "kind": 6,
        "importPath": "Hw_2.opencv.samples.python.watershed",
        "description": "Hw_2.opencv.samples.python.watershed",
        "peekOfCode": "class App:\n    def __init__(self, fn):\n        self.img = cv.imread(fn)\n        if self.img is None:\n            raise Exception('Failed to load image file: %s' % fn)\n        h, w = self.img.shape[:2]\n        self.markers = np.zeros((h, w), np.int32)\n        self.markers_vis = self.img.copy()\n        self.cur_marker = 1\n        self.colors = np.int32( list(np.ndindex(2, 2, 2)) ) * 255",
        "detail": "Hw_2.opencv.samples.python.watershed",
        "documentation": {}
    },
    {
        "label": "faceCascade",
        "kind": 5,
        "importPath": "Hw_2.try",
        "description": "Hw_2.try",
        "peekOfCode": "faceCascade = cv2.CascadeClassifier('data/haarcascades/haarcascade_frontalface_default.xml')\n#video caputure setting\ncapture = cv2.VideoCapture(0) # initialize, #  0,  1.      ,    0\ncapture.set(cv2.CAP_PROP_FRAME_WIDTH,1280) #CAP_PROP_FRAME_WIDTH == 3\ncapture.set(cv2.CAP_PROP_FRAME_HEIGHT,720) #CAP_PROP_FRAME_HEIGHT == 4\n#console message\nface_id = input('\\n enter user id end press <return> ==> ')\nprint(\"\\n [INFO] Initializing face capture. Look the camera and wait\")\ncount = 0 # # of caputre face images\n#   ",
        "detail": "Hw_2.try",
        "documentation": {}
    },
    {
        "label": "capture",
        "kind": 5,
        "importPath": "Hw_2.try",
        "description": "Hw_2.try",
        "peekOfCode": "capture = cv2.VideoCapture(0) # initialize, #  0,  1.      ,    0\ncapture.set(cv2.CAP_PROP_FRAME_WIDTH,1280) #CAP_PROP_FRAME_WIDTH == 3\ncapture.set(cv2.CAP_PROP_FRAME_HEIGHT,720) #CAP_PROP_FRAME_HEIGHT == 4\n#console message\nface_id = input('\\n enter user id end press <return> ==> ')\nprint(\"\\n [INFO] Initializing face capture. Look the camera and wait\")\ncount = 0 # # of caputre face images\n#   \nwhile True:                                                               #    \n    ret, frame = capture.read() #   ",
        "detail": "Hw_2.try",
        "documentation": {}
    },
    {
        "label": "face_id",
        "kind": 5,
        "importPath": "Hw_2.try",
        "description": "Hw_2.try",
        "peekOfCode": "face_id = input('\\n enter user id end press <return> ==> ')\nprint(\"\\n [INFO] Initializing face capture. Look the camera and wait\")\ncount = 0 # # of caputre face images\n#   \nwhile True:                                                               #    \n    ret, frame = capture.read() #   \n    # cf. frame = cv2.flip(frame, -1) \n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #\n    faces = faceCascade.detectMultiScale(\n        gray,#  ",
        "detail": "Hw_2.try",
        "documentation": {}
    },
    {
        "label": "count",
        "kind": 5,
        "importPath": "Hw_2.try",
        "description": "Hw_2.try",
        "peekOfCode": "count = 0 # # of caputre face images\n#   \nwhile True:                                                               #    \n    ret, frame = capture.read() #   \n    # cf. frame = cv2.flip(frame, -1) \n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #\n    faces = faceCascade.detectMultiScale(\n        gray,#  \n        scaleFactor = 1.2, #   , 1  \n        minNeighbors = 6, #   ()",
        "detail": "Hw_2.try",
        "documentation": {}
    }
]